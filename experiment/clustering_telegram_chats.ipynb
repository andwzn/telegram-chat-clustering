{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import calplot\n",
    "import validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection for Clustering of Telegram Chats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data and drop the duplicated index column \"Unnamed: 0\". \n",
    "\n",
    "**Due to the size of the dataset, this might take some time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), '../data/csv/freiesth_scrape_2.csv')\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "df.drop(labels=\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start exploring the data by...\n",
    "\n",
    "1. Inspecting column types.\n",
    "\n",
    "2. Inspecting column values.\n",
    "\n",
    "3. Searching the dataframe for missing datapoints.\n",
    "\n",
    "4. Check for duplicate rows.\n",
    "\n",
    "5. Verify dates.\n",
    "\n",
    "6. Verify webpages.\n",
    "\n",
    "7. Checking the distribution of messages across chats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspect column types\n",
    "\n",
    "First, we inspect the column types to see if they match the data contained in them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns that don't match the type of the data contained in them:\n",
    "- `post_author`: Is `float64`, should be `object`.\n",
    "- `sender_display_name`: Is `float64`, should be `object`.\n",
    "- `collection_time`: Is `object`, should be `datetime`.\n",
    "- `message_date`: Is `object`, should be `datetime`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect column values\n",
    "\n",
    "To get a feeling for the data and to weed out obvious inconsistencies, we can inspect the unqiue values for each column. \n",
    "\n",
    "1. First, we'll take a look at the number of unique values per column. This way we can get a feeling for which columns we can reasonably inspect manually.\n",
    "\n",
    "2. Afterwards, we'll display the  values for each column with less or equal to 20 unique values and inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_per_column = df.nunique()\n",
    "pd.DataFrame(unique_values_per_column, columns=[\"Unique values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_20 = unique_values_per_column[unique_values_per_column<=20]\n",
    "\n",
    "for column in under_20.index:\n",
    "    print(f\"{df[column].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "\n",
    "- As we can see, many columns have a high degree of unique values. This high cardinality is expected due to the inherently variable nature of Telegram messages.\n",
    "\n",
    "\n",
    "- For some columns with high cardinality, the variability might be influenced by how the data is stored or measured rather than by the content itself. In these cases, we may need to transform or aggregate these columns to make them more suitable for analysis. \n",
    "   - For example, datetime columns could be aggregated into broader time periods such as hours, days, or weeks.\n",
    "\n",
    "\n",
    "- For columns with few unique values, no faulty or obviously inconsistent values have been found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check for missing data\n",
    "\n",
    "To understand the extent of missing data, we will examine the percentage of missing values for each column in the dataset. This helps us identify which columns have significant amounts of missing data and might require imputation or other handling strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.isnull().sum().apply(lambda x: x/df.shape[0]).sort_values(ascending=False), columns = [\"Missing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these results are to be expected due to idiosyncrasies of the data collection process, Telegram's API, and the messenger's policy.\n",
    "\n",
    "For columns with more than 10% values missing, I will provide a brief overview of the reasons why this is the case.\n",
    "\n",
    "- `sender_display_name`: This value is usually not set because many users do not have a display name.\n",
    "\n",
    "\n",
    "- `post_author`: This value is only set in certain cases, for example, if an admin of the group sends a message.\n",
    "\n",
    "- `fwd_from_user_name`: This value is only set if a message was forwarded from a user, which seems to be rare for this dataset.\n",
    "\n",
    "- `sender_last_name`: This value is only set if a user provided a surname, which seems to be rare for this dataset.\n",
    "\n",
    "- `reply_to_top_message_id`: This value is only set if a message is a reply in a thread, which seems to be rare for this dataset.\n",
    "\n",
    "- `sender_first_name`: This value is only set if a user provided a first name, which seems to be rare for this dataset.\n",
    "\n",
    "- `reply_to_message_id`: This value is only set if a message is a reply to another message, which seems to be rare for this dataset.\n",
    "\n",
    "- `webpage_author/description/title`: These values are only set for messages that link to a webpage, which provides a preview to Telegram. As both not all messages contain links and not all links provide previews, missing values are to be expected.\n",
    "\n",
    "- `message_group_id`: This value is only set for messages that are part of a group (for example, photos in an album). As this is not the case for all messages, missing values are to be expected.\n",
    "\n",
    "- `fwd_from_chat_handle`: This value is only set for messages that were forwarded from another chat. As this is not the case for all messages, missing values are to be expected.\n",
    "\n",
    "- `message_reactions_count`: This value is only set for messages from chats that allow reactions. As this is not the case for all chat types, missing values are to be expected.\n",
    "\n",
    "- `message_reactions`: This value is only set for messages from chats that allow reactions. As this is not the case for all chat types, missing values are to be expected.\n",
    "\n",
    "- `message_text`: Some message types, for example, photos in an album or media files, don't contain texts. Missing values are to be expected.\n",
    "\n",
    "- `message_fwd_count`: This value is only set for messages from chats that provide information on the forwarding counts through the API. As this is not the case for all chat types, missing values are to be expected.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "- As we can see, most of the missing values actually point towards certain attributes of a message and should be considered in their analysis.\n",
    "- As they don't contain any relevant information and don't point to relevant information regarding a message, these columns with missing data can be dropped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check for duplicates\n",
    "\n",
    "Next, we'll check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates  = df[df.duplicated()]\n",
    "print(f\"Duplicates found: {len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these duplicates might be referenced in other rows, it might be beneficial to keep them in order to maintain the integrity of these connections.\n",
    "\n",
    "Let's check if they are referenced in another row. Messages could be both referenced in `reply_to_message_id` or `reply_to_top_message_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_msg_ids = duplicates[\"telegram_message_id\"]\n",
    "print(f\"Duplicates referenced in `reply_to_messages`: {df[\"reply_to_message_id\"].isin(duplicate_msg_ids).value_counts().iloc[1]}\")\n",
    "print(f\"Duplicates referenced in `reply_to_top_message_id`: {df[\"reply_to_top_message_id\"].isin(duplicate_msg_ids).value_counts().iloc[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, removing duplicates might lead to the loss of important information regarding reply-chains in our dataset.\n",
    "\n",
    "We now have two options:\n",
    "\n",
    "1. If we determine that this information is not needed, we can proceed with dropping the duplicates.\n",
    "\n",
    "2. Otherwise, we need to be mindful of their potential influence and handle them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Verify dates\n",
    "\n",
    "Now, let's verify that our dataframe does not contain any major inconsistencies.\n",
    "\n",
    "While it's impractical to check every single data point, we can make reasonable claims about certain columns, particularly those containing dates and webpage information.\n",
    "\n",
    "We'll begin by checking if the message_date falls within the expected timeframe. Note that there might be outliers, as messages could have been forwarded to the scraped chat within the timeframe but were originally created outside of it.\n",
    "\n",
    "To start, we'll visually inspect the times messages were sent using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(df[\"message_date\"])\n",
    "\n",
    "# Create a series with a date index and the message count for each date\n",
    "messages_dates = dates.dt.date.value_counts().apply(lambda x: x/df.shape[0])\n",
    "messages_dates.index = pd.to_datetime(messages_dates.index)\n",
    "\n",
    "# visualize a data\n",
    "calplot.calplot(messages_dates, cmap='YlGn', colorbar=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- As we can see, most messages were actually sent during the data-collection timeframe (July 2023 - July 2024).\n",
    "- As expected, some messages were sent before the data collection timeframe. \n",
    "- The messages sent after the data collection period  will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Verify Webpages\n",
    "\n",
    "The column \"urls\" contains the urls of webpages referenced in a messages. To prepare them for further analysis down the line, we need to check if they adhere to valid url-formats.\n",
    "\n",
    "To to so, we'll isolate invalid URLs and evaluate them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if urls are valid and save results in a list. If a message has no url, we consider it valid.\n",
    "valid_url = df[\"webpage_url\"].apply(lambda x: validators.url(x) if pd.notnull(x) else True) \n",
    "\n",
    "# As validator returns specific error messages, if a message is invalide, we need to replace them with False to use the filter in boolean indexing\n",
    "invalid_url_filter = [False if elem == True else True for elem in valid_url]\n",
    "\n",
    "df.loc[invalid_url_filter, [\"webpage_url\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "- Some of the urls contain backslashes, that should be removed.\n",
    "- Some urls contain double dashes, which causes the url to be falsely flagged as invalid. These cases can be ignored.\n",
    "- `http://3.US-Militär/\t` is actually invalid and should be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compare the Number of Messages per Chat\n",
    "\n",
    "Last but not least, let us take a look at some descriptive statistics on how many messages were collected for each chat in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats_message_counts = df.groupby(\"chat_name\").size()\n",
    "chats_message_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min & Distribution:** As we can see, the distribution is highly skewed with the majority of chats containing 1 message, while a small number of chats make up a disproportionately large number of messages. This result is to be expected, as a quirk in the data collection process creates a lot of chats with only one message. \n",
    "\n",
    "**Max:** The maximum is 100002. This, again, is to be expected, as the data collection process was limited to collect a maximum of 10.000 messages per chat. The two additional messages are presumably messages originating from this chat, that were found as forwarded messages in another chat. In this case, the data-collection software creates an entry for both chats. \n",
    "\n",
    "**Conclusion**: As we need a certain amount of content and messages for the vectorisation of a chat, we should drop chats containing only few messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the initial exploration points we found the following tasks we need to adress before moving on to engineering the features for clustering:\n",
    "\n",
    "- **Fix the faulty types.**\n",
    "\n",
    "- **Drop `sender_display_name` and `post_author` columns**\n",
    "\n",
    "- **Drop messages sent after the data collection period**\n",
    "\n",
    "- **Clean urls (remove trailing backslashes and invalid links)**\n",
    "\n",
    "- **Remove chats containing only few messages**\n",
    "\n",
    "\n",
    "Optional tasks include:\n",
    "\n",
    "- **(Optional) Delete duplicate rows**\n",
    "\n",
    "- **(Optional) Aggregate datetime columns into broader time periods such as minutes, hours, days, or weeks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean Columns\n",
    "\n",
    "First, we'll drop unnecessary columns and correct any faulty data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df.drop(labels=[\"sender_display_name\",\"post_author\"], axis=1, inplace=True)\n",
    "\n",
    "# convert columns to datetime\n",
    "df[\"collection_time\"] = pd.to_datetime(df[\"collection_time\"], errors='coerce')\n",
    "df[\"message_date\"] = pd.to_datetime(df[\"message_date\"], errors='coerce')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Drop Messages sent after the data collection period\n",
    "\n",
    "Now we drop all messages that were sent after June 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_data_collection = pd.Timestamp(year=2024, month=6, day=30)\n",
    "filter_outliers_date = df[\"message_date\"].dt.date > end_data_collection.date()\n",
    "df = df[~filter_outliers_date]\n",
    "print(f\"Messages sent after end of data collection left: {(df[\"message_date\"].dt.date > end_data_collection.date()).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean URLs\n",
    "\n",
    "Now we can clean up the urls by removing backslashes and invalid urls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove trailing backspaces\n",
    "df[\"webpage_url\"] = df[\"webpage_url\"].str.replace(\"\\\\\\\\$\", '', regex=True)\n",
    "\n",
    "# remove the urls manually evaluated as invalid\n",
    "invalid_urls = [\"http://3.US-Militär/\"]\n",
    "df.loc[df[\"webpage_url\"].isin(invalid_urls) , \"webpage_url\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove Chats containing only few messages\n",
    "\n",
    "As we need a certain amount of content for the vectorisation and clustering of a chat, we'll drop those with fewer than 1000 available messages. \n",
    "\n",
    "Once we're done, we can check the message count statistics for improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats_message_counts = df.groupby(\"chat_name\").size()\n",
    "over_1000 = chats_message_counts[chats_message_counts > 1000]\n",
    "over_1000_chat_names = list(over_1000.index)\n",
    "df_over_1000 = df[df[\"chat_name\"].isin(over_1000_chat_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(over_1000.describe())\n",
    "print(f\"Messages removed: {abs(df.shape[0] - df_over_1000.shape[0])}\")\n",
    "print(f\"Chats removed: {abs(len(chats_message_counts)-len(over_1000))}\")\n",
    "print(\"\")\n",
    "print(f\"Messages remaining: {df_over_1000.shape[0]}\")\n",
    "print(f\"Chats remaining: {len(over_1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing 3523 chats containing 146811 messages, we're left with 361 chats with an average message count of 5018 and no fewer than 1013 messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-chat-clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
