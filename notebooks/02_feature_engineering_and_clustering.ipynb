{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/telegram_chat_clustering_2/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import sys\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory of the current notebook directory and add it to the python path to import custom modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-Import the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the cleaned dataset and check, if we retained the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581498, 38)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"../data/csv/cleaned_data.csv\")\n",
    "dtypes_path = os.path.join(os.getcwd(), '../data/auxiliary/cleaned_data_dtypes.json')\n",
    "\n",
    "# load datatypes\n",
    "with open(dtypes_path, 'r') as f:\n",
    "    dtypes_dict = json.load(f)\n",
    "    \n",
    "# isolate datetime and non-datetime columns\n",
    "datetime_cols = [col for col, dtype in dtypes_dict.items() if dtype == 'datetime64[ns]']\n",
    "dtype_dict_nodate = {col: dtype for col, dtype in dtypes_dict.items() if dtype != 'datetime64[ns]'}\n",
    "\n",
    "# load cleaned dataset using the types defined above\n",
    "df = pd.read_csv(path, low_memory=False, parse_dates=datetime_cols, dtype=dtype_dict_nodate)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Saved Data Types</th>\n",
       "      <th>Current Data Types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chat_handle</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_type</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection_time</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_from_chat_handle</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_from_chat_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_from_user_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_fwd</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_group_elem</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_reply</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_date</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_fwd_count</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_group_id</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_media_type</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_reactions</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_reactions_count</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_text</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_view_count</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reply_to_message_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reply_to_top_message_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_first_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_last_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_username</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telegram_chat_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telegram_message_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telegram_sender_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television_message_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television_original_message_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_author</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_description</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_title</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_url</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_webpage</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_image</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_video</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_document</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_text_lang</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_description_lang</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Saved Data Types Current Data Types\n",
       "chat_handle                              object             object\n",
       "chat_name                                object             object\n",
       "chat_type                                object             object\n",
       "collection_time                  datetime64[ns]     datetime64[ns]\n",
       "fwd_from_chat_handle                     object             object\n",
       "fwd_from_chat_id                          Int64              Int64\n",
       "fwd_from_user_name                       object             object\n",
       "is_fwd                                     bool               bool\n",
       "is_group_elem                              bool               bool\n",
       "is_reply                                   bool               bool\n",
       "message_date                     datetime64[ns]     datetime64[ns]\n",
       "message_fwd_count                       float64            float64\n",
       "message_group_id                        float64            float64\n",
       "message_media_type                       object             object\n",
       "message_reactions                        object             object\n",
       "message_reactions_count                 float64            float64\n",
       "message_text                             object             object\n",
       "message_view_count                      float64            float64\n",
       "reply_to_message_id                       Int64              Int64\n",
       "reply_to_top_message_id                   Int64              Int64\n",
       "sender_first_name                        object             object\n",
       "sender_last_name                         object             object\n",
       "sender_username                          object             object\n",
       "telegram_chat_id                          int64              int64\n",
       "telegram_message_id                       int64              int64\n",
       "telegram_sender_id                        int64              int64\n",
       "television_message_id                     int64              int64\n",
       "television_original_message_id            Int64              Int64\n",
       "webpage_author                           object             object\n",
       "webpage_description                      object             object\n",
       "webpage_title                            object             object\n",
       "webpage_url                              object             object\n",
       "is_webpage                                 bool               bool\n",
       "is_image                                   bool               bool\n",
       "is_video                                   bool               bool\n",
       "is_document                                bool               bool\n",
       "message_text_lang                        object             object\n",
       "webpage_description_lang                 object             object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mismatched data-types: 0\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe to compare the original datatypes and the datatypes of the imported dataframe\n",
    "dtypes_df = pd.DataFrame({\n",
    "    'Saved Data Types': dtypes_dict,\n",
    "    'Current Data Types': df.dtypes\n",
    "})\n",
    "display(dtypes_df)\n",
    "differences_df = dtypes_df[dtypes_df['Saved Data Types'] != dtypes_df['Current Data Types']]\n",
    "print(f\"Number of mismatched data-types: {differences_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before we create the features, we preproess the columns that contain text we want to embed using SBERT. \n",
    "\n",
    "SBERT is a bidirectional encoder, meaning it considers both preceding and following words, thereby accounting for a sentence's structure and context. To preserve as much of this structure and context as possible, we will retain stopwords and regular punctuation. The preprocessing steps will include:\n",
    "\n",
    "1. **Combining Text Fields**: We will concatenate the webpage title and description into a single text field for embedding later on.\n",
    "\n",
    "2. **Handling Missing Values**: Any NaN values in the text columns will be replaced with empty strings to ensure seamless processing later on.\n",
    "\n",
    "3. **Removing Markdown Artifacts**: Since Telegram supports markdown, we will clean the text by removing any artifacts related to Telegram's markdown formatting.\n",
    "\n",
    "4. **Removing URLs**: We will remove any URLs present in the text.\n",
    "\n",
    "5. **Removing Emojis**: We remove the emojis, but retain a version of the text including them, as they might be usefull later on.\n",
    "\n",
    "6. **Normalize styled text**: Some Telegram Users use styled text (for example: ùíæùíπùëúùìáùìä). We normalize them to include them in the embedding. \n",
    "\n",
    "7. **Removing multiple Whitespaces**\n",
    "\n",
    "\n",
    "If we have already performed the preprocessing in earlier runs of the notebook, we'll re-load the preprocessed dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed: True\n"
     ]
    }
   ],
   "source": [
    "# check if we already preprocessed the data in earlier runs. If so, load the preprocessed data.\n",
    "preprocessed_path = os.path.join(os.getcwd(), \"../data/preprocessed/df_preprocessed.pkl\")\n",
    "preprocessed = os.path.isfile(preprocessed_path)\n",
    "if preprocessed:\n",
    "    print(f\"Already preprocessed: {preprocessed}\")\n",
    "    df = pd.read_pickle(preprocessed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add webpage title to its description\n",
    "if not preprocessed:\n",
    "    df[\"webpage_texts\"] = df[\"webpage_title\"] + df[\"webpage_description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan values with empty strings\n",
    "if not preprocessed:\n",
    "    df[\"message_text\"] = df[\"message_text\"].fillna('')\n",
    "    df[\"webpage_texts\"] = df[\"webpage_texts\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls\n",
    "if not preprocessed:\n",
    "    url_pattern = r\"\\(?\\bhttps?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?|\\(?\\bwww\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?\"\n",
    "    df[\"message_text\"] = df[\"message_text\"].str.replace(url_pattern, '', regex=True)\n",
    "    df[\"webpage_texts\"] = df[\"webpage_texts\"].str.replace(url_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove markdown-artifacts\n",
    "if not preprocessed:\n",
    "    bold_pattern = r\"\\*\\*|__\"\n",
    "    italic_pattern = r\"\\*|_\"\n",
    "    strikethrough_pattern = r\"~~\"\n",
    "    link_brackets_pattern = r\"[()\\[\\]]\"\n",
    "\n",
    "    df[\"message_text\"] = (\n",
    "        df[\"message_text\"]\n",
    "        .str.replace(bold_pattern, '', regex=True)\n",
    "        .str.replace(italic_pattern, '', regex=True)\n",
    "        .str.replace(strikethrough_pattern, '', regex=True)\n",
    "        .str.replace(link_brackets_pattern, '', regex=True)\n",
    "    )\n",
    "\n",
    "    df[\"webpage_texts\"] = (\n",
    "        df[\"webpage_texts\"]\n",
    "        .str.replace(bold_pattern, '', regex=True)\n",
    "        .str.replace(italic_pattern, '', regex=True)\n",
    "        .str.replace(strikethrough_pattern, '', regex=True)\n",
    "        .str.replace(link_brackets_pattern, '', regex=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls again, as some were surrounded by brackets before\n",
    "if not preprocessed:\n",
    "    url_pattern = r\"\\(?\\bhttps?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?|\\(?\\bwww\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?\"\n",
    "    df[\"message_text\"] = df[\"message_text\"].str.replace(url_pattern, '', regex=True)\n",
    "    df[\"webpage_texts\"] = df[\"webpage_texts\"].str.replace(url_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of emoji-unicodes using data from \"https://unicode.org/Public/emoji/15.1/\"\n",
    "if not preprocessed:\n",
    "    \n",
    "    def load_emoji_list(file_paths: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Load a list of all emoji from the given file paths.\n",
    "        Args:\n",
    "            file_paths (list): A list of file paths to load emoji sequences from.\n",
    "        Returns:\n",
    "            list: A list of unicode sequences representing the loaded emoji sequences.\n",
    "        \"\"\"\n",
    "        \n",
    "        unicode_list = []\n",
    "\n",
    "        # match lines with unicode, including ranges like 231A..231B \n",
    "        range_pattern = re.compile(r\"([0-9A-Fa-f]{4,6})\\.\\.([0-9A-Fa-f]{4,6})\\s*;\\s*\")\n",
    "        code_point_pattern = re.compile(r\"([0-9A-Fa-f]{4,6}(?:\\s[0-9A-Fa-f]{4,6})*)\\s*;\\s*\")\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                range_match = range_pattern.match(line)\n",
    "                \n",
    "                # add elements of ranges as individual codes to list\n",
    "                if range_match:\n",
    "                    start_code, end_code = range_match.groups()\n",
    "                    start_int = int(start_code, 16)\n",
    "                    end_int = int(end_code, 16)\n",
    "                    unicode_list.extend([chr(code) for code in range(start_int, end_int + 1)])\n",
    "                else:\n",
    "                    code_match = code_point_pattern.match(line)\n",
    "                    if code_match:\n",
    "                        code_points = code_match.group(1)       \n",
    "                        code_point_list = code_points.split()\n",
    "                        # create zwj sequences by combining all code points\n",
    "                        unicode_list.append(''.join([chr(int(code, 16)) for code in code_point_list]))\n",
    "\n",
    "        return unicode_list\n",
    "\n",
    "    # list the paths to the unicode-files\n",
    "    path_1 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-sequences.txt\")\n",
    "    path_2 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-test.txt\")\n",
    "    path_3 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-zwj-sequences.txt\")\n",
    "    file_paths = [path_1, path_2, path_3]\n",
    "\n",
    "    # load all emojis from the unicode-files\n",
    "    emoji_sequences = load_emoji_list(file_paths)\n",
    "\n",
    "    # create a regex pattern from the emoji sequence\n",
    "    emoji_pattern = '|'.join(re.escape(emoji) for emoji in emoji_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis (this will take a while, so we parallelize the process)\n",
    "\"\"\"\n",
    "if not preprocessed:\n",
    "    df[\"message_text_emoji\"] = df[\"message_text\"]\n",
    "    df[\"message_text\"] = df[\"message_text\"].str.replace(emoji_pattern, \" \", regex=True)\n",
    "\"\"\"\n",
    "\n",
    "def demojize_chunk(chunk, emoji_pattern):\n",
    "    # keep text including emojis in a seperate column\n",
    "    chunk[\"message_text_emoji\"] = chunk[\"message_text\"]\n",
    "    # remove emojis\n",
    "    chunk[\"message_text\"] = chunk[\"message_text\"].str.replace(emoji_pattern, \" \", regex=True)\n",
    "    return chunk\n",
    "\n",
    "n_jobs = 3  # Use three cores (seems to be fastest?)\n",
    "\n",
    "# apply the preprocessing in parallel to each chunk\n",
    "if not preprocessed:\n",
    "    chunks = np.array_split(df, n_jobs)\n",
    "    df_chunks = Parallel(n_jobs=n_jobs)(delayed(demojize_chunk)(chunk, emoji_pattern) for chunk in chunks)\n",
    "    df = pd.concat(df_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize styled scripts\n",
    "\n",
    "def normalize_style(text):\n",
    "    # remove characters that dont normalize\n",
    "    text = re.sub(r'[ùîÑ-ùî∑ùíú-ùìèùóî-ùó≠ùóÆ-ùóØ]', '', text)\n",
    "    # normalize Unicode\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    return text\n",
    "\n",
    "if not preprocessed:\n",
    "    df[\"message_text\"] = df[\"message_text\"].apply(lambda x: normalize_style(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated whitespaces\n",
    "if not preprocessed:\n",
    "    df['message_text'] = df['message_text'].str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed dataframe\n",
    "if not preprocessed:\n",
    "    path = os.path.join(os.getcwd(), \"../data/preprocessed/df_preprocessed.pkl\")\n",
    "    df.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create document embeddings based on a chats text. \n",
    "\n",
    "This is the most frequently used approach to vectorizing Telegram-Chats and will serve as a baseline for comparison in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already downloaded. Loading...\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Message-Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the baseline chat-embeddings, we first define the functions we'll use to create the embeddings for each message. To speed the process up, we'll also provide a function to do so in parallel. \n",
    "\n",
    "\n",
    "-> REALLY???\n",
    "Afterwards, we'll take the mean of the message-embeddings of each chat to create document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, default_embedding, model):\n",
    "    \"\"\"\n",
    "    Get embeddings for the given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    default_embedding: The default embedding to return if the input text is empty or NaN.\n",
    "    model: The model used for encoding the text.\n",
    "    Returns:\n",
    "    numpy.ndarray: The embeddings of the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return default_embedding\n",
    "    \n",
    "    return model.encode(text, convert_to_tensor=False)\n",
    "\n",
    "\n",
    "def process_chunk_with_saving(chunk, chunk_index, default_embedding, model, tmp_dir):\n",
    "    \"\"\"\n",
    "    Process a chunk of text data by computing embeddings for each text and saving the embeddings for future use.\n",
    "    Args:\n",
    "        chunk (list): A list of text data.\n",
    "        chunk_index (int): The index of the chunk.\n",
    "        default_embedding: The default embedding to use if a text is empty or NaN.\n",
    "        model: The embedding model to use for computing embeddings.\n",
    "        tmp_dir (str): The directory to temporarily store the embeddings of the chunk.\n",
    "    Returns:\n",
    "        numpy.ndarray: The computed embeddings for the chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # create path to temporarily store this chunk\n",
    "    chunk_path = os.path.join(tmp_dir, f\"chunk_{chunk_index}.npy\")\n",
    "    \n",
    "    if os.path.isfile(chunk_path):\n",
    "        # load the precomputed embeddings if they were already created\n",
    "        print(f\"Chunk #{chunk_index} already embedded. Loading...\")\n",
    "        return np.load(chunk_path)\n",
    "    \n",
    "    print(f\"Processing chunk #{chunk_index}...\")\n",
    "    embeddings = []\n",
    "\n",
    "    for text in chunk:\n",
    "        embedding = get_embeddings(text, default_embedding, model)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Save the embeddings for this chunk\n",
    "    embeddings = np.array(embeddings)\n",
    "    np.save(chunk_path, embeddings)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the embeddings. \n",
    "To avoid redundant calculations, we'll only calculate the embeddings if we have not saved them yet. If they are already in our project, we'll simply load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created: True\n"
     ]
    }
   ],
   "source": [
    "# check if the embeddings were already saved.\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/message_embeddings.npy')\n",
    "message_embeddings_df_path = os.path.join(os.getcwd(), '../features/message_embeddings.csv')\n",
    "already_vectorized = (os.path.isfile(message_embeddings_path) and os.path.isfile(message_embeddings_df_path))\n",
    "print(f\"Already created: {already_vectorized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already created. Loading Embeddings...\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings if they have not already been created\n",
    "\n",
    "if not already_vectorized:\n",
    "    \n",
    "    print(\"Embeddings not yet created. Vectorizing...\")\n",
    "\n",
    "    df_embeddings = df.copy()\n",
    "\n",
    "    # set environment variable to control tokenizers parallelism\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "    # define default embedding\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "\n",
    "    # split DataFrame into chunks for parallel processing\n",
    "    num_chunks = 3  # three seems to work fastest\n",
    "    df_chunks = np.array_split(df_embeddings[\"message_text\"], num_chunks)\n",
    "    \n",
    "    # set up the directory to save intermediate results\n",
    "    output_dir = os.path.join(os.getcwd(), \"../tmp\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # process each chunk in parallel and save intermediate results to limit the impact of crashes\n",
    "    results = Parallel(n_jobs=num_chunks)(\n",
    "        delayed(process_chunk_with_saving)(chunk, chunk_index, default_embedding, model, output_dir) \n",
    "        for chunk_index, chunk in enumerate(df_chunks)\n",
    "    )\n",
    "\n",
    "    # combine results into a single df\n",
    "    df_embeddings[\"message_text_embeddings\"] = np.concatenate(results).tolist()\n",
    "    \n",
    "    # save the final df with embeddings as a csv\n",
    "    df_embeddings.to_csv(message_embeddings_df_path)\n",
    "    print(df_embeddings.shape)\n",
    "    \n",
    "    # save the embeddings separately as a np array\n",
    "    embeddings_array = np.array(df_embeddings[\"message_text_embeddings\"].tolist())\n",
    "    np.save(message_embeddings_path, embeddings_array)\n",
    "\n",
    "else:\n",
    "    # loading the whole dataframe takes about 45min. Instead we'll load only the embedding column and add it to the dataframe we loaded earlier\n",
    "    print(\"Embeddings already created. Loading Embeddings...\")\n",
    "    #feature = ['message_text_embeddings']\n",
    "    #embeddings = pd.read_csv(message_embeddings_df_path, skipinitialspace=True, usecols=feature)\n",
    "    embeddings = np.load(message_embeddings_path)\n",
    "    df_embeddings = df.copy()\n",
    "    df_embeddings[\"message_text_embeddings\"] = embeddings.tolist() # converting to list to save one array per row\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Webpage Preview Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created: True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausf√ºhren von Code in der aktuellen Zelle oder einer vorherigen Zelle abgest√ºrzt. \n",
      "\u001b[1;31mBitte √ºberpr√ºfen Sie den Code in der/den Zelle(n), um eine m√∂gliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# check if the embeddings were already saved.\n",
    "webpage_embeddings_path = os.path.join(os.getcwd(), '../features/webpage_embeddings.npy')\n",
    "webpage_embeddings_df_path = os.path.join(os.getcwd(), '../features/webpage_embeddings.csv')\n",
    "wp_already_vectorized = (os.path.isfile(webpage_embeddings_path) and os.path.isfile(webpage_embeddings_df_path))\n",
    "print(f\"Already created: {wp_already_vectorized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage embeddings already created. Loading Embeddings...\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings if they have not already been created\n",
    "\n",
    "if not wp_already_vectorized:\n",
    "    \n",
    "    print(\"Embeddings not yet created. Vectorizing...\")\n",
    "    \n",
    "    # set environment variable to control tokenizers parallelism\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "    # define default embedding\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "\n",
    "    # split df into chunks for parallel processing\n",
    "    num_chunks = 3 # Three seems to be fastest on the machine the code was written on.\n",
    "    df_chunks = np.array_split(df_embeddings[\"webpage_description\"], num_chunks)\n",
    "    \n",
    "    # set up the directory to save intermediate results\n",
    "    output_dir = os.path.join(os.getcwd(), \"../tmp\")\n",
    "    os.makedirs(output_dir, exist_ok=True)    \n",
    "\n",
    "    # process each chunk in parallel and save intermediate results to limit the impact of crashes\n",
    "    results = Parallel(n_jobs=num_chunks)(\n",
    "        delayed(process_chunk_with_saving)(chunk, chunk_index, default_embedding, model, output_dir) \n",
    "        for chunk_index, chunk in enumerate(df_chunks)\n",
    "    )\n",
    "\n",
    "    # combine results into a single df\n",
    "    df_embeddings[\"webpage_description_embeddings\"] = np.concatenate(results).tolist()\n",
    "    \n",
    "    # save the results as a csv-file\n",
    "    df_embeddings.to_csv(webpage_embeddings_df_path)\n",
    "    print(df_embeddings.shape)\n",
    "    \n",
    "    # save the embeddings seperatly as a numpy array\n",
    "    embeddings_array = np.array(df_embeddings[\"webpage_description_embeddings\"].tolist())\n",
    "    np.save(webpage_embeddings_path, embeddings_array)\n",
    "\n",
    "    \n",
    "else:\n",
    "    # loading the whole dataframe takes about 45min. Instead we'll load only the embedding column and add it to the dataframe we loaded earlier\n",
    "    print(\"Webpage embeddings already created. Loading Embeddings...\") \n",
    "    #feature = ['webpage_description_embeddings']\n",
    "    #embeddings = pd.read_csv(webpage_embeddings_df_path, skipinitialspace=True, usecols=feature)\n",
    "    embeddings = np.load(webpage_embeddings_path)\n",
    "    df_embeddings[\"webpage_description_embeddings\"] = embeddings.tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embeddings[\"webpage_description_embeddings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: (Controll for) Implicit References in Text-Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Better Name: Non-Repeated messages???\n",
    "\n",
    "-- > # also remove handles of other telegram channels?? @....\n",
    "\n",
    "To filter out implicit connections between chats containing forwarded messages and chats that contain their original versions, we simply remove all instances where both a forwarded message and its source are present in our dataset.\n",
    "\n",
    "To avoid repeating the operation each time we run the notebook, we'll save the indices of the remaining messages. These indices will be used in subsequent runs to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we already filtered the dataset\n",
    "feature_1_path = os.path.join(os.getcwd(), \"../features/implicit_ref_filtered.csv\")\n",
    "indices_path = os.path.join(os.getcwd(), \"../data/auxiliary/references_filtered_indices.npy\")\n",
    "\n",
    "filtered = os.path.isfile(indices_path) and os.path.isfile(feature_1_path)\n",
    "print(f\"Data already filtered: {filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filtered: \n",
    "    \n",
    "    print(\"Data not yet filtered. Filtering...\")\n",
    "    \n",
    "    df_references_filtered = df_embeddings.copy()\n",
    "\n",
    "    # group messages according to their message text\n",
    "    grouped = df_references_filtered.groupby(\"message_text\")\n",
    "\n",
    "    # Filter the dataset. True indicates that either the source chat and original message of a duplicates are in our dataset, \n",
    "    # or that one of the duplicates chat id is the source chat of one of the duplicates.\n",
    "    fwds_with_source = grouped.apply(\n",
    "        lambda group: group['fwd_from_chat_id'].isin(group['telegram_chat_id']) | group['telegram_chat_id'].isin(group['fwd_from_chat_id']),\n",
    "        include_groups=False\n",
    "    )\n",
    "\n",
    "    # filter the dataframe to remove messages of which we have either the original or a forwarded instance of in the dataset \n",
    "    msg_to_keep = fwds_with_source[fwds_with_source == False]\n",
    "    msg_to_keep = msg_to_keep.reset_index()\n",
    "    msg_to_keep_indices = msg_to_keep[\"level_1\"].to_list() #level 1 contains the original row indices \n",
    "    df_references_filtered = df_references_filtered.loc[msg_to_keep_indices]\n",
    "\n",
    "    print(f\"Removed {abs(df.shape[0] - df_references_filtered.shape[0])} messages.\")\n",
    "\n",
    "    print(\"Saving filtered data...\")\n",
    "    df_references_filtered.to_csv(feature_1_path)    \n",
    "\n",
    "    print(\"Saving indices...\")\n",
    "    indices_array = np.array(df_references_filtered.index)\n",
    "    np.save(indices_path, indices_array)\n",
    "    \n",
    "else:\n",
    "    print(\"Data already filtered. Using saved indices to (re)filter the data...\")\n",
    "    filtered_rows_indices = np.load(indices_path)\n",
    "    df_references_filtered = df_embeddings.loc[filtered_rows_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Strucutral Equivalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We construct an adjacency matrix representing the frequency of forwarded messages from one chat to another in our dataset.\n",
    "\n",
    "2. We correlate the rows of the matrix, ignoring diagonals to produce a correlation matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create the Chat/Feature Matrix\n",
    "\n",
    "An adjacency matrix is a standard representation of a graph where each cell indicates the number of connections between nodes. In our case, the columns represent the source chats of messages, and the row indices represent the chats in our dataset.\n",
    "\n",
    "Due to limitations in the data collection process, the current adjacency matrix does not capture all forward-based relationships between chats. Instead, it only reflects the incoming connections observed within the data collection timeframe.\n",
    "\n",
    "To create a comprehensive adjacency matrix, we will:\n",
    "\n",
    "1. Isolate Rows with forwarded messages\n",
    "\n",
    "\n",
    "2. Construct the Adjacency Matrix\n",
    "\n",
    "\n",
    "3. Add Chats with no forwarded messages\n",
    "\n",
    "We'll start with isolating rows with forwarded messages and create the initial adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate rows that contain messages forwarded from a public chat\n",
    "fwd_messages = df[~(pd.isna(df[\"fwd_from_chat_id\"]))]\n",
    "\n",
    "# Create the adjacency matrix \n",
    "adj_matrix = fwd_messages.pivot_table(\n",
    "                            index='telegram_chat_id', \n",
    "                            columns='fwd_from_chat_id', \n",
    "                            aggfunc='size', # count the number of occurrences of each combination of telegram_chat_id and fwd_from_chat_id\n",
    "                            fill_value=0) # fills all cells with no co-occurances of chat and source-chat with 0\n",
    "adj_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check, if we have chats without any forwarded messages in our dataset and if they are already in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group messages by chat they were sent in\n",
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "\n",
    "# get the chat ids of all chats, that have 0 messages forwarded from public chats\n",
    "def all_nans(series):\n",
    "    return series.isna().all()\n",
    "no_fwd_chats = grouped[\"fwd_from_chat_id\"].apply(all_nans)\n",
    "no_fwd_chats = no_fwd_chats[no_fwd_chats==True].index\n",
    "\n",
    "# check if the Adjacency Matrix already contains the chats we identified\n",
    "index_adj_matrix = adj_matrix.index\n",
    "in_matrix = []\n",
    "not_in_matrix = []\n",
    "\n",
    "for index in no_fwd_chats:\n",
    "    if index in (index_adj_matrix):\n",
    "        in_matrix.append(index)\n",
    "    else:\n",
    "        not_in_matrix.append(index)\n",
    "        \n",
    "print(f\"{len(in_matrix)}/{len(no_fwd_chats)} chats without forwarded messages are already in the matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll add rows for chats containing no forwarded messages to the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the adjacency matrix for non-forward chats\n",
    "non_fwd_adj_matrix = pd.DataFrame(index=not_in_matrix, columns=adj_matrix.columns)\n",
    "non_fwd_adj_matrix.fillna(0,inplace=True) # set all values to 0, as these chats have no connections.\n",
    "\n",
    "# add them to the initial matrix\n",
    "adj_matrix_combined = pd.concat([adj_matrix, non_fwd_adj_matrix], axis=0)\n",
    "adj_matrix_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we need to check if each chat is represented in our adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_chat_count = df[\"telegram_chat_id\"].nunique()\n",
    "adj_matrix_chat_count = adj_matrix_combined.shape[0]\n",
    "print(f\"Number of chats in dataset: {initial_chat_count}\")\n",
    "print(f\"Number of chats in Adjacency Matrix: {adj_matrix_chat_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic scaling to the matrix\n",
    "adj_matrix_log_scaled = np.log1p(adj_matrix_combined)  # np.log1p is log(x + 1) to handle zeros\n",
    "\n",
    "# Convert back to DataFrame if needed\n",
    "adj_matrix_log_scaled = pd.DataFrame(adj_matrix_log_scaled, \n",
    "                                     index=adj_matrix_combined.index, \n",
    "                                     columns=adj_matrix_combined.columns)\n",
    "\n",
    "\n",
    "# Display the normalized adjacency matrix\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.heatmap(adj_matrix_log_scaled, annot=False, cmap='coolwarm', vmax=4)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix_one_hot = adj_matrix_combined.map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "# Display the one hot encoded adjacency matrix\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.heatmap(adj_matrix_one_hot, annot=False, cmap='gist_yarg')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = adj_matrix_one_hot.columns\n",
    "only_once = []\n",
    "for col in cols:\n",
    "    if (adj_matrix_one_hot[col].value_counts().loc[1] == 1): # get all columns that have only one connection to a chat and drop them\n",
    "        only_once.append(col)\n",
    "\n",
    "adj_matrix_one_hot_multiple = adj_matrix_one_hot.drop(axis=1, labels=only_once)\n",
    "adj_matrix_one_hot_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=11, random_state=1, n_init=\"auto\").fit(adj_matrix_one_hot_multiple)\n",
    "clusters = kmeans.labels_\n",
    "pd.Series(clusters).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-chat-clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
