{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/telegram_chat_clustering_2/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "import json\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory of the current notebook directory and add it to the python path to import custom modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the cleaned dataset and check, if we retained the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581498, 38)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(os.getcwd(), \"../data/csv/cleaned_data.csv\")\n",
    "dtypes_path = os.path.join(os.getcwd(), '../data/auxiliary/cleaned_data_dtypes.json')\n",
    "\n",
    "# load datatypes\n",
    "with open(dtypes_path, 'r') as f:\n",
    "    dtypes_dict = json.load(f)\n",
    "    \n",
    "# isolate datetime and non-datetime columns\n",
    "datetime_cols = [col for col, dtype in dtypes_dict.items() if dtype == 'datetime64[ns]']\n",
    "dtype_dict_nodate = {col: dtype for col, dtype in dtypes_dict.items() if dtype != 'datetime64[ns]'}\n",
    "\n",
    "# load cleaned dataset using the types defined above\n",
    "df = pd.read_csv(path, low_memory=False, parse_dates=datetime_cols, dtype=dtype_dict_nodate)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Saved Data Types</th>\n",
       "      <th>Current Data Types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chat_handle</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chat_type</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection_time</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_from_chat_handle</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_from_chat_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fwd_from_user_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_fwd</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_group_elem</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_reply</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_date</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_fwd_count</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_group_id</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_media_type</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_reactions</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_reactions_count</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_text</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_view_count</th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reply_to_message_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reply_to_top_message_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_first_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_last_name</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sender_username</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telegram_chat_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telegram_message_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telegram_sender_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television_message_id</th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television_original_message_id</th>\n",
       "      <td>Int64</td>\n",
       "      <td>Int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_author</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_description</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_title</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_url</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_webpage</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_image</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_video</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_document</th>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>message_text_lang</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>webpage_description_lang</th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Saved Data Types Current Data Types\n",
       "chat_handle                              object             object\n",
       "chat_name                                object             object\n",
       "chat_type                                object             object\n",
       "collection_time                  datetime64[ns]     datetime64[ns]\n",
       "fwd_from_chat_handle                     object             object\n",
       "fwd_from_chat_id                          Int64              Int64\n",
       "fwd_from_user_name                       object             object\n",
       "is_fwd                                     bool               bool\n",
       "is_group_elem                              bool               bool\n",
       "is_reply                                   bool               bool\n",
       "message_date                     datetime64[ns]     datetime64[ns]\n",
       "message_fwd_count                       float64            float64\n",
       "message_group_id                        float64            float64\n",
       "message_media_type                       object             object\n",
       "message_reactions                        object             object\n",
       "message_reactions_count                 float64            float64\n",
       "message_text                             object             object\n",
       "message_view_count                      float64            float64\n",
       "reply_to_message_id                       Int64              Int64\n",
       "reply_to_top_message_id                   Int64              Int64\n",
       "sender_first_name                        object             object\n",
       "sender_last_name                         object             object\n",
       "sender_username                          object             object\n",
       "telegram_chat_id                          int64              int64\n",
       "telegram_message_id                       int64              int64\n",
       "telegram_sender_id                        int64              int64\n",
       "television_message_id                     int64              int64\n",
       "television_original_message_id            Int64              Int64\n",
       "webpage_author                           object             object\n",
       "webpage_description                      object             object\n",
       "webpage_title                            object             object\n",
       "webpage_url                              object             object\n",
       "is_webpage                                 bool               bool\n",
       "is_image                                   bool               bool\n",
       "is_video                                   bool               bool\n",
       "is_document                                bool               bool\n",
       "message_text_lang                        object             object\n",
       "webpage_description_lang                 object             object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mismatched data-types: 0\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe to compare the original datatypes and the datatypes of the imported dataframe\n",
    "dtypes_df = pd.DataFrame({\n",
    "    'Saved Data Types': dtypes_dict,\n",
    "    'Current Data Types': df.dtypes\n",
    "})\n",
    "display(dtypes_df)\n",
    "differences_df = dtypes_df[dtypes_df['Saved Data Types'] != dtypes_df['Current Data Types']]\n",
    "print(f\"Number of mismatched data-types: {differences_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before we create the features, we preproess the columns that contain text we want to embed using SBERT. \n",
    "\n",
    "SBERT is a bidirectional encoder, meaning it considers both preceding and following words, thereby accounting for a sentence's structure and context. To preserve as much of this structure and context as possible, we will retain stopwords and regular punctuation. The preprocessing steps will include:\n",
    "\n",
    "1. **Combining Text Fields**: We will concatenate the webpage title and description into a single text field for embedding later on.\n",
    "\n",
    "2. **Isolating References:** We isolate telegram chat handles referenced in links and mentions\n",
    "\n",
    "3. **Handling Missing Values**: Any NaN values in the text columns will be replaced with empty strings to ensure seamless processing later on.\n",
    "\n",
    "4. **Removing Markdown Artifacts**: Since Telegram supports markdown, we will clean the text by removing any artifacts related to Telegram's markdown formatting.\n",
    "\n",
    "5. **Removing URLs**: We will remove any URLs present in the text.\n",
    "\n",
    "6. **Removing Emojis**: We remove the emojis, but retain a version of the text including them, as they might be usefull later on.\n",
    "\n",
    "7. **Normalize styled text**: Some Telegram Users use styled text (for example: ð’¾ð’¹ð‘œð“‡ð“Š). We normalize them to include them in the embedding. \n",
    "\n",
    "8. **Removing multiple whitespaces**\n",
    "\n",
    "\n",
    "If we have already performed the preprocessing in earlier runs of the notebook, we'll re-load the preprocessed dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed: True\n"
     ]
    }
   ],
   "source": [
    "# check if we already preprocessed the data in earlier runs. If so, load the preprocessed data.\n",
    "preprocessed_path = os.path.join(os.getcwd(), \"../data/preprocessed/df_preprocessed.pkl\")\n",
    "preprocessed = os.path.isfile(preprocessed_path)\n",
    "if preprocessed:\n",
    "    print(f\"Already preprocessed: {preprocessed}\")\n",
    "    df = pd.read_pickle(preprocessed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Text Fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add webpage titles to their description\n",
    "if not preprocessed:\n",
    "    df[\"webpage_texts\"] = df[\"webpage_title\"] + df[\"webpage_description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate Chat Handles referenced by Links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate telegram links\n",
    "if not preprocessed:\n",
    "    \n",
    "    url_extraction_pattern = r\"(https?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(www\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))\"\n",
    "    link_brackets_pattern = r\"[()\\[\\]]\"\n",
    "\n",
    "    extracted_links = (df[\"message_text\"]\n",
    "                    .str.replace(link_brackets_pattern, \" \", regex=True) # remove brackets, as they might interfere with the regex\n",
    "                    .str.strip(\"_\") # remove trailing whitespaces, as they are most likely markdown artifacts\n",
    "                    .str.strip(\"*\") # remove trailing asterisks, as they are most likely markdown artifacts\n",
    "                    .str.strip(\"~\") # remove trailing tildes, as they are most likely markdown artifacts\n",
    "                    .str.extractall(url_extraction_pattern)) \n",
    "\n",
    "    # reset index to convert the multiindex to columns we can work with\n",
    "    extracted_links = extracted_links.reset_index()\n",
    "    \n",
    "    # only keep telegram urls\n",
    "    extracted_links = extracted_links[(extracted_links[0].str.contains(r\"https?:\\/\\/t\\.me\", na=False)) | (extracted_links[1].str.contains(r\"www\\.t\\.me\", na=False))]\n",
    "    \n",
    "    # remove message identifiers that might be appended to the end of telegram urls\n",
    "    message_id_pattern = r\"/\\d+$\"\n",
    "    extracted_links[0] = extracted_links[0].str.replace(message_id_pattern, '', regex=True)\n",
    "    extracted_links[1] = extracted_links[1].str.replace(message_id_pattern, '', regex=True)\n",
    "    \n",
    "    # get the telegram chat handle, that should now be at the end of the url\n",
    "    chat_handle_pattern = r\"/(\\w+$)\"\n",
    "    extracted_links[0] = extracted_links[0].str.extract(chat_handle_pattern)\n",
    "    extracted_links[1] = extracted_links[1].str.extract(chat_handle_pattern)\n",
    "\n",
    "    # group extracted links by the message they were extracted from\n",
    "    grouped = extracted_links.groupby(\"level_0\")\n",
    "\n",
    "    # aggregate the links of each message\n",
    "    aggregated_links = grouped.agg(lambda x: list(x.dropna())).reset_index()\n",
    "    aggregated_links[\"combined\"] = aggregated_links.apply(lambda row: row.iloc[2] + row.iloc[3], axis=1)\n",
    "\n",
    "    # Prepare for merging\n",
    "    aggregated_links = aggregated_links[[\"level_0\", \"combined\"]]\n",
    "    aggregated_links.columns = ['original_index', 'telegram_links']\n",
    "    aggregated_links.set_index('original_index', inplace=True)\n",
    "\n",
    "    # add mentioned chat handles to the original df\n",
    "    df = df.join(aggregated_links, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NaN Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan values with empty strings\n",
    "if not preprocessed:\n",
    "    df[\"message_text\"] = df[\"message_text\"].fillna('')\n",
    "    df[\"webpage_texts\"] = df[\"webpage_texts\"].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls\n",
    "if not preprocessed:\n",
    "    url_pattern = r\"\\(?\\bhttps?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?|\\(?\\bwww\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?\"\n",
    "    df[\"message_text\"] = df[\"message_text\"].str.replace(url_pattern, '', regex=True)\n",
    "    df[\"webpage_texts\"] = df[\"webpage_texts\"].str.replace(url_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolate mentioned Chat Handles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate mentioned chats\n",
    "if not preprocessed:\n",
    "    mentions_pattern =  r\"@(\\w+)\" \n",
    "    extracted_mentions = df[\"message_text\"].str.extractall(mentions_pattern)\n",
    "    \n",
    "    # reset index to convert the multiindex to columns we can work with\n",
    "    extracted_mentions = extracted_mentions.reset_index()\n",
    "    \n",
    "    # we dont include trailing whitespaces, as they are most likely markdown artifacts.    \n",
    "    extracted_mentions[0] = extracted_mentions[0].str.strip(\"_\") \n",
    "    \n",
    "    # group extracted mentions by the message they were extracted from\n",
    "    grouped = extracted_mentions.groupby(\"level_0\")\n",
    "    \n",
    "    # aggregate the mentions of each message\n",
    "    aggregated_mentions = grouped.agg(lambda x: list(x.dropna())).reset_index()\n",
    "\n",
    "    # Prepare aggregated_mentions for merging\n",
    "    aggregated_mentions = aggregated_mentions[[\"level_0\", 0]]\n",
    "    aggregated_mentions.columns = ['original_index', 'mentions']\n",
    "    aggregated_mentions.set_index('original_index', inplace=True)\n",
    "\n",
    "    # add aggreagted links to the original df\n",
    "    df = df.join(aggregated_mentions, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine References:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not preprocessed:\n",
    "\n",
    "    # combine chat handles referenced in links and mentions into a single list. Remove handles referenced more than once per message.\n",
    "    df['referenced_chat_handles'] = df.apply(\n",
    "        lambda row: list(set((row['mentions'] if isinstance(row['mentions'], list) else []) +\n",
    "                    (row['telegram_links'] if isinstance(row['telegram_links'], list) else []))),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # convert empty lists to NaN\n",
    "    df['referenced_chat_handles'] = df['referenced_chat_handles'].apply(lambda x: x if x else np.nan)\n",
    "\n",
    "    with pd.option_context('display.max_colwidth', None):\n",
    "        display(df[[\"mentions\", \"telegram_links\",  \"referenced_chat_handles\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Markdown Artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove markdown-artifacts\n",
    "if not preprocessed:\n",
    "    bold_pattern = r\"\\*\\*|__\"\n",
    "    italic_pattern = r\"\\*|_\"\n",
    "    strikethrough_pattern = r\"~~\"\n",
    "    link_brackets_pattern = r\"[()\\[\\]]\"\n",
    "\n",
    "    df[\"message_text\"] = (\n",
    "        df[\"message_text\"]\n",
    "        .str.replace(bold_pattern, '', regex=True)\n",
    "        .str.replace(italic_pattern, '', regex=True)\n",
    "        .str.replace(strikethrough_pattern, '', regex=True)\n",
    "        .str.replace(link_brackets_pattern, '', regex=True)\n",
    "    )\n",
    "\n",
    "    df[\"webpage_texts\"] = (\n",
    "        df[\"webpage_texts\"]\n",
    "        .str.replace(bold_pattern, '', regex=True)\n",
    "        .str.replace(italic_pattern, '', regex=True)\n",
    "        .str.replace(strikethrough_pattern, '', regex=True)\n",
    "        .str.replace(link_brackets_pattern, '', regex=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls again, as some were surrounded by brackets before\n",
    "if not preprocessed:\n",
    "    url_pattern = r\"\\(?\\bhttps?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?|\\(?\\bwww\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)]\\)?\"\n",
    "    df[\"message_text\"] = df[\"message_text\"].str.replace(url_pattern, '', regex=True)\n",
    "    df[\"webpage_texts\"] = df[\"webpage_texts\"].str.replace(url_pattern, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Emojis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of emoji-unicodes using data from \"https://unicode.org/Public/emoji/15.1/\"\n",
    "if not preprocessed:\n",
    "    \n",
    "    def load_emoji_list(file_paths: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Load a list of all emoji from the given file paths.\n",
    "        Args:\n",
    "            file_paths (list): A list of file paths to load emoji sequences from.\n",
    "        Returns:\n",
    "            list: A list of unicode sequences representing the loaded emoji sequences.\n",
    "        \"\"\"\n",
    "        \n",
    "        unicode_list = []\n",
    "\n",
    "        # match lines with unicode, including ranges like 231A..231B \n",
    "        range_pattern = re.compile(r\"([0-9A-Fa-f]{4,6})\\.\\.([0-9A-Fa-f]{4,6})\\s*;\\s*\")\n",
    "        code_point_pattern = re.compile(r\"([0-9A-Fa-f]{4,6}(?:\\s[0-9A-Fa-f]{4,6})*)\\s*;\\s*\")\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                range_match = range_pattern.match(line)\n",
    "                \n",
    "                # add elements of ranges as individual codes to list\n",
    "                if range_match:\n",
    "                    start_code, end_code = range_match.groups()\n",
    "                    start_int = int(start_code, 16)\n",
    "                    end_int = int(end_code, 16)\n",
    "                    unicode_list.extend([chr(code) for code in range(start_int, end_int + 1)])\n",
    "                else:\n",
    "                    code_match = code_point_pattern.match(line)\n",
    "                    if code_match:\n",
    "                        code_points = code_match.group(1)       \n",
    "                        code_point_list = code_points.split()\n",
    "                        # create zwj sequences by combining all code points\n",
    "                        unicode_list.append(''.join([chr(int(code, 16)) for code in code_point_list]))\n",
    "\n",
    "        return unicode_list\n",
    "\n",
    "    # list the paths to the unicode-files\n",
    "    path_1 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-sequences.txt\")\n",
    "    path_2 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-test.txt\")\n",
    "    path_3 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-zwj-sequences.txt\")\n",
    "    file_paths = [path_1, path_2, path_3]\n",
    "\n",
    "    # load all emojis from the unicode-files\n",
    "    emoji_sequences = load_emoji_list(file_paths)\n",
    "\n",
    "    # create a regex pattern from the emoji sequence\n",
    "    emoji_pattern = '|'.join(re.escape(emoji) for emoji in emoji_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis (this will take a while, so we parallelize the process)\n",
    "\n",
    "def demojize_chunk(chunk, emoji_pattern):\n",
    "    # keep text including emojis in a seperate column\n",
    "    chunk[\"message_text_emoji\"] = chunk[\"message_text\"]\n",
    "    # remove emojis\n",
    "    chunk[\"message_text\"] = chunk[\"message_text\"].str.replace(emoji_pattern, \" \", regex=True)\n",
    "    return chunk\n",
    "\n",
    "n_jobs = 3  # Use three cores (seems to be fastest?)\n",
    "\n",
    "# apply the preprocessing in parallel to each chunk\n",
    "if not preprocessed:\n",
    "    chunks = np.array_split(df, n_jobs)\n",
    "    df_chunks = Parallel(n_jobs=n_jobs)(delayed(demojize_chunk)(chunk, emoji_pattern) for chunk in chunks)\n",
    "    df = pd.concat(df_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581498, 43)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Styled Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize styled scripts\n",
    "\n",
    "def normalize_style(text):\n",
    "    # remove characters that dont normalize\n",
    "    text = re.sub(r'[ð”„-ð”·ð’œ-ð“ð—”-ð—­ð—®-ð—¯]', '', text)\n",
    "    # normalize Unicode\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    return text\n",
    "\n",
    "if not preprocessed:\n",
    "    df[\"message_text\"] = df[\"message_text\"].apply(lambda x: normalize_style(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove multiple Whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated whitespaces\n",
    "if not preprocessed:\n",
    "    df['message_text'] = df['message_text'].str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed Data for future runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed dataframe\n",
    "if not preprocessed:\n",
    "    path = os.path.join(os.getcwd(), \"../data/preprocessed/df_preprocessed.pkl\")\n",
    "    df.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 0: Baseline Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create document embeddings based on a chats message text. \n",
    "\n",
    "This is the most frequently used approach to vectorizing Telegram-Chats and will serve as a baseline for comparison in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already downloaded. Loading...\n"
     ]
    }
   ],
   "source": [
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Message-Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the baseline chat-embeddings, we first define the functions we'll use to create the embeddings for each message. To speed the process up, we'll also provide a function to do so in parallel. \n",
    "\n",
    "\n",
    "-> REALLY???\n",
    "Afterwards, we'll take the mean of the message-embeddings of each chat to create document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, default_embedding, model):\n",
    "    \"\"\"\n",
    "    Get embeddings for the given text using a specified model.\n",
    "    Parameters:\n",
    "    text (str): The input text to encode.\n",
    "    default_embedding: The default embedding to return if the input text is empty or NaN.\n",
    "    model: The model used for encoding the text.\n",
    "    Returns:\n",
    "    numpy.ndarray: The embeddings of the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return default_embedding\n",
    "    \n",
    "    return model.encode(text, convert_to_tensor=False)\n",
    "\n",
    "\n",
    "def process_chunk_with_saving(chunk, chunk_index, default_embedding, model, tmp_dir):\n",
    "    \"\"\"\n",
    "    Process a chunk of text data by computing embeddings for each text and saving the embeddings for future use.\n",
    "    Args:\n",
    "        chunk (list): A list of text data.\n",
    "        chunk_index (int): The index of the chunk.\n",
    "        default_embedding: The default embedding to use if a text is empty or NaN.\n",
    "        model: The embedding model to use for computing embeddings.\n",
    "        tmp_dir (str): The directory to temporarily store the embeddings of the chunk.\n",
    "    Returns:\n",
    "        numpy.ndarray: The computed embeddings for the chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # create path to temporarily store this chunk\n",
    "    chunk_path = os.path.join(tmp_dir, f\"chunk_{chunk_index}.npy\")\n",
    "    \n",
    "    if os.path.isfile(chunk_path):\n",
    "        # load the precomputed embeddings if they were already created\n",
    "        print(f\"Chunk #{chunk_index} already embedded. Loading...\")\n",
    "        return np.load(chunk_path)\n",
    "    \n",
    "    print(f\"Processing chunk #{chunk_index}...\")\n",
    "    embeddings = []\n",
    "\n",
    "    for text in chunk:\n",
    "        embedding = get_embeddings(text, default_embedding, model)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    # Save the embeddings for this chunk\n",
    "    embeddings = np.array(embeddings)\n",
    "    np.save(chunk_path, embeddings)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the embeddings. \n",
    "To avoid redundant calculations, we'll only calculate the embeddings if we have not saved them yet. If they are already in our project, we'll simply load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created: True\n"
     ]
    }
   ],
   "source": [
    "# check if the embeddings were already saved.\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "message_embeddings_df_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.csv')\n",
    "already_vectorized = (os.path.isfile(message_embeddings_path) and os.path.isfile(message_embeddings_df_path))\n",
    "print(f\"Already created: {already_vectorized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already created. Loading Embeddings...\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings if they have not already been created\n",
    "\n",
    "if not already_vectorized:\n",
    "    \n",
    "    print(\"Embeddings not yet created. Vectorizing...\")\n",
    "\n",
    "    df_embeddings = df.copy()\n",
    "\n",
    "    # set environment variable to control tokenizers parallelism\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "    # define default embedding\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "\n",
    "    # split DataFrame into chunks for parallel processing\n",
    "    num_chunks = 3  # three seems to work fastest\n",
    "    df_chunks = np.array_split(df_embeddings[\"message_text\"], num_chunks)\n",
    "    \n",
    "    # set up the directory to save intermediate results\n",
    "    output_dir = os.path.join(os.getcwd(), \"../tmp\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # process each chunk in parallel and save intermediate results to limit the impact of crashes\n",
    "    results = Parallel(n_jobs=num_chunks)(\n",
    "        delayed(process_chunk_with_saving)(chunk, chunk_index, default_embedding, model, output_dir) \n",
    "        for chunk_index, chunk in enumerate(df_chunks)\n",
    "    )\n",
    "\n",
    "    # combine results into a single df\n",
    "    df_embeddings[\"message_text_embeddings\"] = np.concatenate(results).tolist()\n",
    "    \n",
    "    # save the final df with embeddings as a csv\n",
    "    df_embeddings.to_csv(message_embeddings_df_path)\n",
    "    print(df_embeddings.shape)\n",
    "    \n",
    "    # save the embeddings separately as a np array\n",
    "    embeddings_array = np.array(df_embeddings[\"message_text_embeddings\"].tolist())\n",
    "    np.save(message_embeddings_path, embeddings_array)\n",
    "\n",
    "else:\n",
    "    # loading the whole dataframe takes about 45min. Instead we'll load only the embeddings and add them to the dataframe we loaded earlier (this takes ca. 2min)\n",
    "    print(\"Embeddings already created. Loading Embeddings...\")\n",
    "    #feature = ['message_text_embeddings']\n",
    "    #embeddings = pd.read_csv(message_embeddings_df_path, skipinitialspace=True, usecols=feature)\n",
    "    embeddings = np.load(message_embeddings_path)\n",
    "    df_embeddings = df.copy()\n",
    "    df_embeddings[\"message_text_embeddings\"] = embeddings.tolist() # converting to list to save one array per row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Chat Representations\n",
    "\n",
    "Now we can create chat representations by averaging the embeddings of each message in a chat. \n",
    "\n",
    "To do so, we will take the mean of all messages belonging to a chat while ignoring the zero-vectors created for empty messages, as they could drag the average down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already created: True\n"
     ]
    }
   ],
   "source": [
    "base_path = os.path.join(os.getcwd(), \"../features/0_base_chat_vectors.npy\")\n",
    "already_created =  os.path.exists(base_path)\n",
    "print(f\"Already created: {already_created}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors found. Loading chat representations.\n"
     ]
    }
   ],
   "source": [
    "def aggregate_embeddings(embeddings: pd.Series) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Aggregate embeddings by taking their mean and ignoring zero-embeddings.\n",
    "    Args:\n",
    "        embedding (pd.Series(np.array)): Embeddings to aggregate\n",
    "    Returns:\n",
    "        np.array: The aggregated embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    non_zero_embeddings = [emb for emb in embeddings if not np.all(emb == 0)]\n",
    "    # vertically stack the non zero message embeddings into a matrix and take the mean for each column to aggregate embeddings\n",
    "    if non_zero_embeddings:\n",
    "        aggregation = np.mean(np.vstack(non_zero_embeddings), axis=0)\n",
    "        return aggregation\n",
    "    # return a zero vector if all embeddings are zero\n",
    "    else:\n",
    "        return np.zeros_like(embeddings.iloc[0])  \n",
    "    \n",
    "\n",
    "if not already_created:\n",
    "    print(\"No vectors found. Creating chat representations.\")\n",
    "    base_chat_representations = df_embeddings.groupby('telegram_chat_id')['message_text_embeddings'].apply(aggregate_embeddings)\n",
    "\n",
    "    # save the representations\n",
    "    base_chat_representations.to_pickle(base_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Vectors found. Loading chat representations.\")\n",
    "    base_chat_representations = pd.read_pickle(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: Filtered Chat Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Problem:**\n",
    "\n",
    "Datasets sourced from Telegram are often created by tracking forwarded messages across chats to identify new sources for data collection. Consequently, forwarded messages frequently appear in multiple chats, including both their original source and the chats where they were forwarded.\n",
    "\n",
    "Chat representations that include all messages from a chat often contain implicit connections between chats. This is because duplicated message text can make chats appear more similar than if they were merely discussing the same topics.\n",
    "\n",
    "**The Feature:**\n",
    "\n",
    "To address this, our first feature will be chat representations that exclude messages forwarded from or to other chats within the dataset. This way, we will be able to evaluate the impact of the forward-driven data collection process has on clustering and the similarity of chat representations.\n",
    "\n",
    "To create them, we'll:\n",
    "\n",
    "1. Filter the dataset to omitt any forwarded messages between chats in the dataset.\n",
    "\n",
    "2. Create aggregated embeddings and topic distributions based on the filtered messages to represent our chats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Filter the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter out implicit connections between chats containing forwarded messages and chats that contain their original versions, we simply remove all instances where both a forwarded message and its source are present in our dataset.\n",
    "\n",
    "To avoid repeating the operation each time we run the notebook, we'll save the indices of the remaining messages. These indices will be used in subsequent runs to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we already filtered the dataset\n",
    "feature_1_path = os.path.join(os.getcwd(), \"../features/1_implicit_ref_filtered.csv\")\n",
    "indices_path = os.path.join(os.getcwd(), \"../features/1_implicit_ref_filtered_indices.npy\")\n",
    "\n",
    "filtered = os.path.isfile(indices_path) and os.path.isfile(feature_1_path)\n",
    "print(f\"Data already filtered: {filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filtered: \n",
    "    \n",
    "    print(\"Data not yet filtered. Filtering...\")\n",
    "    \n",
    "    df_references_filtered = df_embeddings.copy()\n",
    "\n",
    "    # group messages according to their message text\n",
    "    grouped = df_references_filtered.groupby(\"message_text\")\n",
    "\n",
    "    # Filter the dataset. True indicates that either the source chat and original message of a duplicates are in our dataset, \n",
    "    # or that one of the duplicates chat id is the source chat of one of the duplicates.\n",
    "    fwds_with_source = grouped.apply(\n",
    "        lambda group: group['fwd_from_chat_id'].isin(group['telegram_chat_id']) | group['telegram_chat_id'].isin(group['fwd_from_chat_id']),\n",
    "        include_groups=False\n",
    "    )\n",
    "\n",
    "    # filter the dataframe to remove messages of which we have either the original or a forwarded instance of in the dataset \n",
    "    msg_to_keep = fwds_with_source[fwds_with_source == False]\n",
    "    msg_to_keep = msg_to_keep.reset_index()\n",
    "    msg_to_keep_indices = msg_to_keep[\"level_1\"].to_list() #level 1 contains the original row indices \n",
    "    df_references_filtered = df_references_filtered.loc[msg_to_keep_indices]\n",
    "\n",
    "    print(f\"Removed {abs(df.shape[0] - df_references_filtered.shape[0])} messages.\")\n",
    "\n",
    "    print(\"Saving filtered data...\")\n",
    "    df_references_filtered.to_csv(feature_1_path)    \n",
    "\n",
    "    print(\"Saving indices...\")\n",
    "    indices_array = np.array(df_references_filtered.index)\n",
    "    np.save(indices_path, indices_array)\n",
    "    \n",
    "else:\n",
    "    print(\"Data already filtered. Using saved indices to (re)filter the data...\")\n",
    "    filtered_rows_indices = np.load(indices_path)\n",
    "    df_references_filtered = df_embeddings.loc[filtered_rows_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2: Create Chat Representations\n",
    "\n",
    "Next, we create chat representations by aggregating the embeddings of the message left in the filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chat_path = os.path.join(os.getcwd(), \"../features/1_filtered_chat_vectors.npy\")\n",
    "already_created =  os.path.exists(filtered_chat_path)\n",
    "print(f\"Already created: {already_created}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_created:\n",
    "    print(\"No vectors found. Creating chat representations.\")\n",
    "    # for each chat: vertically stack the message embeddings into a matrix and take the mean for each column to create chat embeddings\n",
    "    filtered_chat_representations = df_references_filtered.groupby('telegram_chat_id')['message_text_embeddings'].apply(aggregate_embeddings) \n",
    "\n",
    "    # save the representations\n",
    "    filtered_chat_representations.to_pickle(filtered_chat_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Vectors found. Loading chat representations.\")\n",
    "    filtered_chat_representations = pd.read_pickle(filtered_chat_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Structural Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a representation of our chats structural attributes. Structural attributes describe a chats connections to other telegram entities. \n",
    "\n",
    "For our purposes, we will consider two kinds of connections:\n",
    "\n",
    "1. Forwarded messages between chats.\n",
    "\n",
    "2. Mentions of chats or other telegram-entities.\n",
    "\n",
    "To vectorise these connection, we will perform the following steps:\n",
    "\n",
    "1. We construct an adjacency matrix representing the frequency of forwarded messages from one chat to another in our dataset.\n",
    "\n",
    "2. We construct an adjacency matrix based on the chats referenced in links and mentions sent in each chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create the Forward Matrix\n",
    "\n",
    "An adjacency matrix is a standard representation of a graph where each cell indicates the number of connections between nodes. In our case, the columns represent the source chats of messages, and the row indices represent the chats in our dataset.\n",
    "\n",
    "Due to limitations in the data collection process, the current adjacency matrix does not capture all forward-based relationships between chats. Instead, it only reflects the incoming connections observed within the data collection timeframe.\n",
    "\n",
    "To create a comprehensive adjacency matrix, we will:\n",
    "\n",
    "1. Isolate Rows with forwarded messages\n",
    "\n",
    "\n",
    "2. Construct the Adjacency Matrix\n",
    "\n",
    "\n",
    "3. Add Chats with no forwarded messages\n",
    "\n",
    "We'll start with isolating rows with forwarded messages and create the initial adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate rows that contain messages forwarded from another chat\n",
    "fwd_messages = df[~(pd.isna(df[\"fwd_from_chat_id\"]))]\n",
    "\n",
    "# create the adjacency matrix \n",
    "adj_matrix_fwd = fwd_messages.pivot_table(\n",
    "                            index='telegram_chat_id', \n",
    "                            columns='fwd_from_chat_id', \n",
    "                            aggfunc='size', # count the number of occurrences of each combination of telegram_chat_id and fwd_from_chat_id\n",
    "                            fill_value=0) # fills all cells with no co-occurances of chat and source-chat with 0\n",
    "adj_matrix_fwd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check, if we have chats without any forwarded messages in our dataset and if they are already in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group messages by chat they were sent in\n",
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "\n",
    "# get the chat ids of all chats, that have zero messages forwarded from public chats\n",
    "def all_nans(series):\n",
    "    return series.isna().all()\n",
    "no_fwd_chats = grouped[\"fwd_from_chat_id\"].apply(all_nans)\n",
    "no_fwd_chats = no_fwd_chats[no_fwd_chats==True].index\n",
    "\n",
    "# check if the Adjacency Matrix already contains the chats we identified\n",
    "index_adj_matrix = adj_matrix_fwd.index\n",
    "in_matrix = []\n",
    "not_in_matrix = []\n",
    "\n",
    "for index in no_fwd_chats:\n",
    "    if index in (index_adj_matrix):\n",
    "        in_matrix.append(index)\n",
    "    else:\n",
    "        not_in_matrix.append(index)\n",
    "        \n",
    "print(f\"{len(in_matrix)}/{len(no_fwd_chats)} chats without forwarded messages are already in the matrix.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we found some chats containting no forwards, we'll add them to the matrix manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the adjacency matrix for non-forward chats\n",
    "non_fwd_adj_matrix = pd.DataFrame(index=not_in_matrix, columns=adj_matrix_fwd.columns)\n",
    "non_fwd_adj_matrix.fillna(0,inplace=True) # set all values to 0, as these chats have no connections.\n",
    "\n",
    "# add them to the initial matrix\n",
    "adj_matrix_fwd = pd.concat([adj_matrix_fwd, non_fwd_adj_matrix], axis=0)\n",
    "adj_matrix_fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to creating the correlation matrix, we need to check if each chat is represented in our adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_chat_count = df[\"telegram_chat_id\"].nunique()\n",
    "adj_matrix_chat_count = adj_matrix_fwd.shape[0]\n",
    "print(f\"Number of chats in dataset: {initial_chat_count}\")\n",
    "print(f\"Number of chats in Adjacency Matrix: {adj_matrix_chat_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply some regularisation to the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply logarithmic scaling to the matrix\n",
    "adj_matrix_fwd_log_scaled = np.log1p(adj_matrix_fwd)  # np.log1p is log(x + 1) to handle zeros\n",
    "\n",
    "adj_matrix_fwd_log_scaled = pd.DataFrame(adj_matrix_fwd_log_scaled, \n",
    "                                     index=adj_matrix_fwd.index, \n",
    "                                     columns=adj_matrix_fwd.columns)\n",
    "\n",
    "\n",
    "# display the normalized adjacency matrix\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.heatmap(adj_matrix_fwd_log_scaled, annot=False, cmap='coolwarm', vmax=4)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix_fwd_one_hot = adj_matrix_fwd.map(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "# display the one hot encoded adjacency matrix\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.heatmap(adj_matrix_fwd_one_hot, annot=False, cmap='gist_yarg')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the Reference Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we isolate all messages that contain a reference to another telegram entity based on links and mentions. \n",
    "\n",
    "To do so, we'll isolate the chat handles mentioned in each message and the message's origin-chat's ID. \n",
    "We'll also want to remove self-references, as we're only interesed in conncetions between chats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate references\n",
    "references = df[[\"telegram_chat_id\", \"chat_handle\", \"referenced_chat_handles\"]]\n",
    "\n",
    "# explode the \"mentions\" column to get a individual row for each reference in a message\n",
    "references_exploded = references.explode('referenced_chat_handles')\n",
    "\n",
    "# remove self-references\n",
    "references_exploded = references_exploded[~(references_exploded[\"chat_handle\"].str.lower() == references_exploded[\"referenced_chat_handles\"].str.lower())]\n",
    "references_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are some chat handles missing. \n",
    "\n",
    "As attempts to add the missing chat handles using the Telegram API were severely impacted by rate-limits, we will remove chats with missing handles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references_exploded = references_exploded[~pd.isna(references_exploded[\"chat_handle\"])]\n",
    "print(f\" Number of rows with missing chat_handle: {pd.isna(references_exploded[\"chat_handle\"]).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the adjacency matrix based on references to other telegram entities in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix_ref = references_exploded.pivot_table(\n",
    "    index='chat_handle', \n",
    "    columns='referenced_chat_handles', \n",
    "    aggfunc='size', # count the number of occurrences of each combination of telegram_chat_id and fwd_from_chat_id\n",
    "    fill_value=0) # fills all cells with no co-occurances of chat and source-chat with 0\n",
    "\n",
    "adj_matrix_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check if there are any chats in our dataset that contain no links or mentions to any other telegram entity or reference only themself. \n",
    "\n",
    "If so, we manually add them to our matrix and set all values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "\n",
    "# get all chats that have zero references to other chats in form of links and mentions\n",
    "references_per_group = grouped[\"referenced_chat_handles\"].sum()\n",
    "no_ref = references_per_group[references_per_group==0]\n",
    "\n",
    "# create a dictionary of available Chat-Handle/ID-pairings to get the handles corresponding to the IDs of the chats without references\n",
    "available_chats = df[[\"telegram_chat_id\",\"chat_handle\"]].value_counts()\n",
    "available_chats = available_chats.reset_index().drop(columns=[\"count\"], axis=1)\n",
    "available_chats = available_chats.set_index(\"telegram_chat_id\")\n",
    "available_chats = available_chats[\"chat_handle\"].to_dict()\n",
    "\n",
    "# get the chat handles of the chats containing no references.\n",
    "no_ref = no_ref.index.map(available_chats).tolist()\n",
    "\n",
    "# get the chat handles of chats that reference only themselves\n",
    "# aggregate all references of a chat into a single list per chat\n",
    "grouped_references = grouped[\"referenced_chat_handles\"].agg(lambda x: list(set([item for sublist in x if isinstance(sublist, list) for item in sublist])))\n",
    "# keep only chats that have references to exactly one other chat\n",
    "only_one_ref = grouped_references[grouped_references.apply(lambda x: len(x)) == 1]\n",
    "# map the 'telegram_chat_id' index to its corresponding chat handle\n",
    "only_one_ref.index = only_one_ref.index.map(available_chats)\n",
    "# reset index to make 'telegram_chat_id' a column again\n",
    "only_one_ref = only_one_ref.reset_index()\n",
    "# explode the 'referenced_chat_handles' column to separate list elements into individual rows\n",
    "only_one_ref = only_one_ref.explode(\"referenced_chat_handles\")\n",
    "# filter the rows where 'telegram_chat_id' is the same as 'referenced_chat_handles'\n",
    "only_self_ref = only_one_ref[only_one_ref[\"telegram_chat_id\"] == only_one_ref[\"referenced_chat_handles\"]]\n",
    "# extract the 'telegram_chat_id' column values where the chat references itself\n",
    "only_self_ref = only_self_ref[\"telegram_chat_id\"]\n",
    "only_self_ref = only_self_ref.tolist()\n",
    "\n",
    "# combine the list of chats containing no references or only references to themselves\n",
    "zero_ref = no_ref + only_self_ref\n",
    "\n",
    "# check if the Adjacency Matrix already contains the chats we identified\n",
    "index_adj_matrix = adj_matrix_ref.index\n",
    "in_matrix = []\n",
    "not_in_matrix = []\n",
    "\n",
    "for index in zero_ref:\n",
    "    if index in (index_adj_matrix):\n",
    "        in_matrix.append(index)\n",
    "    else:\n",
    "        not_in_matrix.append(index)\n",
    "        \n",
    "print(f\"{len(in_matrix)}/{len(zero_ref)} chats without forwarded messages are already in the matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the adjacency matrix for chats containing no references\n",
    "non_ref_adj_matrix = pd.DataFrame(index=not_in_matrix, columns=adj_matrix_ref.columns)\n",
    "non_ref_adj_matrix.fillna(0,inplace=True) # set all values to 0, as these chats have no references.\n",
    "\n",
    "# add them to the initial matrix\n",
    "adj_matrix_ref = pd.concat([adj_matrix_ref, non_ref_adj_matrix], axis=0)\n",
    "adj_matrix_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply some regularisation to the matrix and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply logarithmic scaling to the matrix\n",
    "adj_matrix_ref_log_scaled = np.log1p(adj_matrix_ref)  # np.log1p is log(x + 1) to handle zeros\n",
    "\n",
    "\n",
    "adj_matrix_ref_log_scaled = pd.DataFrame(adj_matrix_ref_log_scaled, \n",
    "                                     index=adj_matrix_ref.index, \n",
    "                                     columns=adj_matrix_ref.columns)\n",
    "\n",
    "\n",
    "# Display the normalized adjacency matrix\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.heatmap(adj_matrix_ref_log_scaled, annot=False, cmap='coolwarm', vmax=4)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: Webpage Previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include webpage information, we vectorize the webpage previews provided by telegram.\n",
    "\n",
    "To do so, we'll:\n",
    "\n",
    "1. Create Webpage Preview Embeddings\n",
    "\n",
    "2. Aggregate Chat Representations by combining them with the message embeddings created earlier and aggregating them on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create Webpage Preview Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the embeddings were already saved.\n",
    "webpage_embeddings_path = os.path.join(os.getcwd(), '../features/0_webpage_embeddings.npy')\n",
    "webpage_embeddings_df_path = os.path.join(os.getcwd(), '../features/0_webpage_embeddings.csv')\n",
    "wp_already_vectorized = (os.path.isfile(webpage_embeddings_path) and os.path.isfile(webpage_embeddings_df_path))\n",
    "print(f\"Already created: {wp_already_vectorized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate embeddings if they have not already been created\n",
    "\n",
    "if not wp_already_vectorized:\n",
    "    \n",
    "    print(\"Embeddings not yet created. Vectorizing...\")\n",
    "    \n",
    "    # set environment variable to control tokenizers parallelism\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "    # define default embedding\n",
    "    default_embedding = np.zeros((model.get_sentence_embedding_dimension(),))\n",
    "\n",
    "    # split df into chunks for parallel processing\n",
    "    num_chunks = 3 # Three seems to be fastest on the machine the code was written on.\n",
    "    df_chunks = np.array_split(df_embeddings[\"webpage_description\"], num_chunks)\n",
    "    \n",
    "    # set up the directory to save intermediate results\n",
    "    output_dir = os.path.join(os.getcwd(), \"../tmp\")\n",
    "    os.makedirs(output_dir, exist_ok=True)    \n",
    "\n",
    "    # process each chunk in parallel and save intermediate results to limit the impact of crashes\n",
    "    results = Parallel(n_jobs=num_chunks)(\n",
    "        delayed(process_chunk_with_saving)(chunk, chunk_index, default_embedding, model, output_dir) \n",
    "        for chunk_index, chunk in enumerate(df_chunks)\n",
    "    )\n",
    "\n",
    "    # combine results into a single df\n",
    "    df_embeddings[\"webpage_description_embeddings\"] = np.concatenate(results).tolist()\n",
    "    \n",
    "    # save the results as a csv-file\n",
    "    df_embeddings.to_csv(webpage_embeddings_df_path)\n",
    "    print(df_embeddings.shape)\n",
    "    \n",
    "    # save the embeddings seperatly as a numpy array\n",
    "    embeddings_array = np.array(df_embeddings[\"webpage_description_embeddings\"].tolist())\n",
    "    np.save(webpage_embeddings_path, embeddings_array)\n",
    "\n",
    "    \n",
    "else:\n",
    "    # loading the whole dataframe takes about 45min. Instead we'll load only the embeddings and add them to the dataframe we loaded earlier\n",
    "    print(\"Webpage embeddings already created. Loading Embeddings...\") \n",
    "    #feature = ['webpage_description_embeddings']\n",
    "    #embeddings = pd.read_csv(webpage_embeddings_df_path, skipinitialspace=True, usecols=feature)\n",
    "    embeddings = np.load(webpage_embeddings_path)\n",
    "    df_embeddings[\"webpage_description_embeddings\"] = embeddings.tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Aggregated Chat Representations \n",
    "\n",
    "Next, we create two chat representations: \n",
    "\n",
    "1) One based on the preview embeddings.\n",
    "\n",
    "2) One based on an aggregation of the preview and message embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpreview_chat_path = os.path.join(os.getcwd(), \"../features/03_webpreview_chat_vectors.npy\")\n",
    "already_created =  os.path.exists(webpreview_chat_path)\n",
    "print(f\"Already created: {already_created}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_created:\n",
    "    print(\"No vectors found. Creating chat representations.\")\n",
    "    # for each chat: vertically stack the message embeddings into a matrix and take the mean for each column to create chat embeddings\n",
    "    webpreview_chat_representations = df_embeddings.groupby('telegram_chat_id')['webpage_description_embeddings'].apply(aggregate_embeddings) \n",
    "\n",
    "    # save the representations\n",
    "    webpreview_chat_representations.to_pickle(webpreview_chat_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Vectors found. Loading chat representations.\")\n",
    "    webpreview_chat_representations = pd.read_pickle(webpreview_chat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_webpreview_chat_path = os.path.join(os.getcwd(), \"../features/03_msg_webpreview_chat_vectors.npy\")\n",
    "already_created =  os.path.exists(msg_webpreview_chat_path)\n",
    "print(f\"Already created: {already_created}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings(row):\n",
    "    \"\"\"\n",
    "    Combine message embeddings and webpage preview embeddings by taking their mean and ignoring zero embeddings\n",
    "    Args:\n",
    "        row (pd.Series(np.array)): a row of our dataframe, representing a message #TODO: check\n",
    "    Returns:\n",
    "        np.array: The aggregated embeddings\n",
    "    \"\"\"        \n",
    "    message_embedding = row[\"message_text_embeddings\"]\n",
    "    webpage_embedding = row[\"webpage_description_embeddings\"]\n",
    "    \n",
    "    # Create a list of non-zero embeddings\n",
    "    non_zero_embeddings = []\n",
    "    \n",
    "    if not np.all(message_embedding == 0):\n",
    "        non_zero_embeddings.append(message_embedding)\n",
    "    if not np.all(webpage_embedding == 0):\n",
    "        non_zero_embeddings.append(webpage_embedding)\n",
    "    \n",
    "    # If we have valid embeddings, return the mean; otherwise return a zero vector\n",
    "    if non_zero_embeddings:\n",
    "        return np.mean(np.vstack(non_zero_embeddings), axis=0)\n",
    "    else:\n",
    "        return np.zeros_like(message_embedding)  # Return a zero vector if there's no valid embedding\n",
    "\n",
    "# Check if vectors already exist\n",
    "if not already_created:\n",
    "    print(\"No vectors found. Creating chat representations.\")\n",
    "    \n",
    "    # Create a new column that combines both the message text embedding and webpage embeddings for each message\n",
    "    df_embeddings[\"combined_embedding\"] = df_embeddings.apply(combine_embeddings, axis=1)\n",
    "    \n",
    "    # Use the aggregated messages embeddings to create chat vectors, ignoring zero embeddings\n",
    "    msg_webpreview_chat_representations = df_embeddings.groupby('telegram_chat_id')['combined_embedding'].apply(aggregate_embeddings)\n",
    "\n",
    "    # Save the representations\n",
    "    msg_webpreview_chat_representations.to_pickle(msg_webpreview_chat_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Vectors found. Loading chat representations.\")\n",
    "    msg_webpreview_chat_representations = pd.read_pickle(msg_webpreview_chat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the created embeddings to check, if the aggregation worked as expects\n",
    "df_embeddings[[\"webpage_description_embeddings\", \"message_text_embeddings\", \"combined_embedding\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
