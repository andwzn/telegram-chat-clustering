{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Exploratory Data Analysis & Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import calplot\n",
    "import validators\n",
    "from typing import Union, Any\n",
    "from datetime import datetime\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data and drop the duplicated index column \"Unnamed: 0\". \n",
    "\n",
    "**Due to the size of the dataset, this might take some time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), '../data/csv/freiesth_scrape_2.csv')\n",
    "df = pd.read_csv(path, low_memory=False)\n",
    "df.drop(labels=\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start exploring the data by...\n",
    "\n",
    "1. Inspecting column types.\n",
    "\n",
    "2. Inspecting column values.\n",
    "\n",
    "3. Searching the dataframe for missing datapoints.\n",
    "\n",
    "4. Check for duplicate rows.\n",
    "\n",
    "5. Verify dates.\n",
    "\n",
    "6. Verify webpages.\n",
    "\n",
    "7. Checking the distribution of messages across chats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Inspect column types\n",
    "\n",
    "First, we inspect the column types to see if they match the data contained in them. To do so, we'll\n",
    "\n",
    "1. Inspect the datatypes per columns.\n",
    "\n",
    "2. Check the datatypes in object type columns. \n",
    "\n",
    "\n",
    "**Inspect the datatypes per columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some columns that don't match the type of the data contained in them:\n",
    "- `post_author`: Is `float64`, should be `object`.\n",
    "- `sender_display_name`: Is `float64`, should be `object`.\n",
    "- `collection_time`: Is `object`, should be `datetime`.\n",
    "- `message_date`: Is `object`, should be `datetime`.\n",
    "- `reply_to_message_id`: Is `float`, should be `int`.\n",
    "- `reply_to_top_message_id`: Is `float`, should be `int`.\n",
    "- `fwd_from_chat_id`: Is `float`, should be `int`.\n",
    "- `television_original_message_id`: Is `float`, should be `int`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate object_type columns**\n",
    "\n",
    "As the object dtype is a general-purpose type for columns with mixed or unknown data types, we need to make sure that they acutally contain the datatype we're looking for.\n",
    "\n",
    "In our dataset, they should actually contain strings. Let's check, if that's actually the case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of all columns of type \"object\"\n",
    "object_colums = df.dtypes[df.dtypes == \"object\"].index.to_list()\n",
    "\n",
    "# create a dataframe containing the type of each cell\n",
    "df_object_types =  df[object_colums].map(type)\n",
    "\n",
    "# print unique datatype for each column\n",
    "for column in df_object_types.columns:\n",
    "    print(f\"Types in column \\\"{column}\\\"\".upper())\n",
    "    print(df_object_types[column].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some of the columns contain elements of the type `float` -- presumably, those are nan-values, which are saved as floats in pandas.\n",
    "\n",
    "We will replace them with empty strings down the line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect column values\n",
    "\n",
    "To get a feeling for the data and to weed out obvious inconsistencies, we can inspect the unqiue values for each column. \n",
    "\n",
    "1. First, we'll take a look at the number of unique values per column. This way we can get a feeling for which columns we can reasonably inspect manually.\n",
    "\n",
    "2. Afterwards, we'll display the  values for each column with less or equal to 20 unique values and inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_per_column = df.nunique()\n",
    "pd.DataFrame(unique_values_per_column, columns=[\"Unique values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_20 = unique_values_per_column[unique_values_per_column<=20]\n",
    "\n",
    "for column in under_20.index:\n",
    "    print(f\"{df[column].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "\n",
    "- As we can see, many columns have a high degree of unique values. This high cardinality is expected due to the inherently variable nature of Telegram messages.\n",
    "\n",
    "\n",
    "- For some columns with high cardinality, the variability might be influenced by how the data is stored or measured rather than by the content itself. In these cases, we may need to transform or aggregate these columns to make them more suitable for analysis. \n",
    "   - For example, datetime columns could be aggregated into broader time periods such as hours, days, or weeks.\n",
    "\n",
    "\n",
    "- For columns with few unique values, no faulty or obviously inconsistent values have been found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Check for missing data\n",
    "\n",
    "To understand the extent of missing data, we will examine the percentage of missing values for each column in the dataset. This helps us identify which columns have significant amounts of missing data and might require imputation or other handling strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print missing values per column\n",
    "pd.DataFrame(df.isnull().sum().apply(lambda x: x/df.shape[0]).sort_values(ascending=False), columns = [\"Missing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these results are to be expected due to idiosyncrasies of the data collection process, Telegram's API, and the messenger's policy.\n",
    "\n",
    "For columns with more than 10% values missing, I will provide a brief overview of the reasons why this is the case.\n",
    "\n",
    "- `sender_display_name`: This value is usually not set because many users do not have a display name.\n",
    "\n",
    "\n",
    "- `post_author`: This value is only set in certain cases, for example, if an admin of the group sends a message.\n",
    "\n",
    "- `fwd_from_user_name`: This value is only set if a message was forwarded from a user, which seems to be rare for this dataset.\n",
    "\n",
    "- `sender_last_name`: This value is only set if a user provided a surname, which seems to be rare for this dataset.\n",
    "\n",
    "- `reply_to_top_message_id`: This value is only set if a message is a reply in a thread, which seems to be rare for this dataset.\n",
    "\n",
    "- `sender_first_name`: This value is only set if a user provided a first name, which seems to be rare for this dataset.\n",
    "\n",
    "- `reply_to_message_id`: This value is only set if a message is a reply to another message, which seems to be rare for this dataset.\n",
    "\n",
    "- `webpage_author/description/title`: These values are only set for messages that link to a webpage, which provides a preview to Telegram. As both not all messages contain links and not all links provide previews, missing values are to be expected.\n",
    "\n",
    "- `message_group_id`: This value is only set for messages that are part of a group (for example, photos in an album). As this is not the case for all messages, missing values are to be expected.\n",
    "\n",
    "- `fwd_from_chat_handle`: This value is only set for messages that were forwarded from another chat. As this is not the case for all messages, missing values are to be expected.\n",
    "\n",
    "- `television_original_message_id`: This value is only set for messages that were forwarded from another chat. As this is not the case for all messages, missing values are to be expected.\n",
    "\n",
    "- `fwd_from_chat_id`: Same as above.\n",
    "\n",
    "- `message_reactions_count`: This value is only set for messages from chats that allow reactions. As this is not the case for all chat types, missing values are to be expected.\n",
    "\n",
    "- `message_reactions`: This value is only set for messages from chats that allow reactions. As this is not the case for all chat types, missing values are to be expected.\n",
    "\n",
    "- `message_text`: Some message types, for example, photos in an album or media files, don't contain texts. Missing values are to be expected.\n",
    "\n",
    "- `message_fwd_count`: This value is only set for messages from chats that provide information on the forwarding counts through the API. As this is not the case for all chat types, missing values are to be expected.\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "- As we can see, most of the missing values actually point towards certain attributes of a message and should be considered in their analysis.\n",
    "\n",
    "- As they don't contain any relevant information and don't point to relevant information regarding a message, these columns with missing data can be dropped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Check for duplicates\n",
    "\n",
    "Next, we'll check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates  = df[df.duplicated()]\n",
    "print(f\"Duplicates found: {len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these duplicates might be referenced in other rows, it might be beneficial to keep them in order to maintain the integrity of these connections.\n",
    "\n",
    "Let's check if they are referenced in another row. Messages could be both referenced in `reply_to_message_id` or `reply_to_top_message_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_msg_ids = duplicates[\"telegram_message_id\"]\n",
    "print(f\"Duplicates referenced in `reply_to_messages`: {df[\"reply_to_message_id\"].isin(duplicate_msg_ids).value_counts().iloc[1]}\")\n",
    "print(f\"Duplicates referenced in `reply_to_top_message_id`: {df[\"reply_to_top_message_id\"].isin(duplicate_msg_ids).value_counts().iloc[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, removing duplicates might lead to the loss of important information regarding reply-chains in our dataset.\n",
    "\n",
    "We now have two options:\n",
    "\n",
    "1. If we determine that this information is not needed, we can proceed with dropping the duplicates.\n",
    "\n",
    "2. Otherwise, we need to be mindful of their potential influence and handle them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Verify dates\n",
    "\n",
    "Now, let's verify that our dataframe does not contain any major inconsistencies.\n",
    "\n",
    "While it's impractical to check every single data point, we can make reasonable claims about certain columns, particularly those containing dates and webpage information.\n",
    "\n",
    "We'll begin by checking if the message_date falls within the expected timeframe. Note that there might be outliers, as messages could have been forwarded to the scraped chat within the timeframe but were originally created outside of it.\n",
    "\n",
    "To start, we'll visually inspect the times messages were sent using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.to_datetime(df[\"message_date\"])\n",
    "\n",
    "# Create a series with a date index and the message count for each date\n",
    "messages_dates = dates.dt.date.value_counts().apply(lambda x: x/df.shape[0])\n",
    "messages_dates.index = pd.to_datetime(messages_dates.index)\n",
    "\n",
    "# visualize a data\n",
    "calplot.calplot(messages_dates, cmap='YlGn', colorbar=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- As we can see, most messages were actually sent during the data-collection timeframe (July 2023 - July 2024).\n",
    "- As expected, some messages were sent before the data collection timeframe. \n",
    "- The messages sent after the data collection period  will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Verify Webpages\n",
    "\n",
    "The column \"urls\" contains the urls of webpages referenced in a messages. To prepare them for further analysis down the line, we need to check if they adhere to valid url-formats.\n",
    "\n",
    "To to so, we'll isolate invalid URLs and evaluate them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if urls are valid and save results in a list. If a message has no url, we consider it valid.\n",
    "valid_url = df[\"webpage_url\"].apply(lambda x: validators.url(x) if pd.notnull(x) else True) \n",
    "\n",
    "# As validator returns specific error messages, if a message is invalide, we need to replace them with False to use the filter in boolean indexing\n",
    "invalid_url_filter = [False if elem == True else True for elem in valid_url]\n",
    "\n",
    "df.loc[invalid_url_filter, [\"webpage_url\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "- Some of the urls contain backslashes, that should be removed.\n",
    "- Some urls contain double dashes, which causes the url to be falsely flagged as invalid. These cases can be ignored.\n",
    "- `http://3.US-Militär/\t` is actually invalid and should be removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compare the Number of Messages per Chat\n",
    "\n",
    "Last but not least, let us take a look at some descriptive statistics on how many messages were collected for each chat in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats_message_counts = df.groupby(\"chat_name\").size()\n",
    "chats_message_counts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min & Distribution:** \n",
    "- As we can see, the distribution is highly skewed with the majority of chats containing 1 message, while a small number of chats make up a disproportionately large number of messages. This result is to be expected, as a quirk in the data collection process creates a lot of chats with only one message. \n",
    "\n",
    "**Max:** \n",
    "- The maximum is 100002. This, again, is to be expected, as the data collection process was limited to collect a maximum of 10.000 messages per chat. The two additional messages are presumably messages originating from this chat, that were found as forwarded messages in another chat. In this case, the data-collection software creates an entry for both chats. \n",
    "\n",
    "**Conclusion**: \n",
    "- As we need a certain amount of content and messages for the vectorisation of a chat, we should drop chats containing only few messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Check how many messages are part of a group\n",
    "\n",
    "Telegram allows users to send several media files in one message — for example, a photo album. Each file is included as its own message in our dataset, which might interfere with our data analysis.\n",
    "\n",
    "To decide how to handle them down the line, we'll:\n",
    "- Check how many messages have a group-ID, which indicates that they are part of an album.\n",
    "\n",
    "- Confirm that these messages are indeed media files sent as part of an album.\n",
    "\n",
    "- Examine the average group size to estimate how many messages we'd lose if we choose to drop them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check how many messages are part of the group:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Raw count: {df[\"message_group_id\"].notna().sum()}\")\n",
    "print(f\"Percentage of messages: {(df[\"message_group_id\"].notna().sum() / df.shape[0])*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm that they are messages containing media:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all grouped messages ant convert their ids to int\n",
    "grouped_msgs = df[df[\"message_group_id\"].notna()]\n",
    "grouped_msgs.loc[:, \"message_group_id\"] = grouped_msgs[\"message_group_id\"].astype(int)\n",
    "\n",
    "# check their mediatype\n",
    "print(\"Media Types of Messages with Group-ID:\")\n",
    "grouped_msgs[\"message_media_type\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the average group size:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_group_size = np.mean(grouped_msgs.groupby(\"message_group_id\").size())\n",
    "mean_group_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Estimated Loss if grouped messages are removed:**\n",
    "\n",
    "As we won't work with images, videos and documents (the types of data usually sent in an album) we might only want to keep the message in the album that contains the messages text that was included in the album.\n",
    "\n",
    "To get an estimate on how many messages we'd lose this way, we'll use the following formula: \n",
    "\n",
    "$\\text{Estimated Messages Loss} = (\\text{Average Group Size} - 1) \\times \\text{Number of Unique Groups}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_count = grouped_msgs[\"message_group_id\"].nunique()\n",
    "estimated_message_loss = (mean_group_size-1) * group_count\n",
    "\n",
    "print(f\"Mean group size: {round(mean_group_size,2)}\")\n",
    "print(f\"Estimated messages lost (Raw Count): {int(estimated_message_loss)}\")\n",
    "print(f\"Estimated messages lost (Percentage): {round(estimated_message_loss / df.shape[0] * 100,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "- 16% of all messages are part of a group.\n",
    "\n",
    "- All of them contain media.\n",
    "\n",
    "- On average, groups contain between 3 and 4 messages.\n",
    "\n",
    "- If we drop all grouped messages without a message text, we'll lose about 11.5% of all messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the initial exploration we found the following tasks we need to adress before moving on to engineering the features for clustering:\n",
    "\n",
    "- **Fix the faulty types.**\n",
    "\n",
    "- **Replace NaN values in object-type columns with empty strings**\n",
    "\n",
    "- **Drop `sender_display_name` and `post_author` columns**\n",
    "\n",
    "- **Drop messages sent after the data collection period**\n",
    "\n",
    "- **Clean urls (remove trailing backslashes and invalid links)**\n",
    "\n",
    "- **Remove chats containing only few messages**\n",
    "\n",
    "- **Drop grouped messages without text**\n",
    "\n",
    "- **Delete duplicate rows**\n",
    "\n",
    "- **(Optional) Aggregate datetime columns into broader time periods such as minutes, hours, days, or weeks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean Columns\n",
    "\n",
    "First, we'll drop unnecessary columns and correct any faulty data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "df.drop(labels=[\"sender_display_name\",\"post_author\"], axis=1, inplace=True)\n",
    "\n",
    "# convert columns to datetime\n",
    "df[\"collection_time\"] = pd.to_datetime(df[\"collection_time\"], errors='coerce')\n",
    "df[\"message_date\"] = pd.to_datetime(df[\"message_date\"], errors='coerce')\n",
    "\n",
    "# convert float columns to integer, while keeping NaN-values\n",
    "df['fwd_from_chat_id'] = pd.to_numeric(df['fwd_from_chat_id'], errors='coerce').astype('Int64')\n",
    "df['reply_to_message_id'] = pd.to_numeric(df['reply_to_message_id'], errors='coerce').astype('Int64')\n",
    "df['reply_to_top_message_id'] = pd.to_numeric(df['reply_to_top_message_id'], errors='coerce').astype('Int64')\n",
    "df['television_original_message_id'] = pd.to_numeric(df['reply_to_top_message_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replace NaN values in object-columns with empty strings. \n",
    "\n",
    "Next, we replace NaN values in object columns with empty strings to enable seamless text processing down the line. We'll reuse the object column list created earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of columns of type object\n",
    "object_column_names = df.dtypes[df.dtypes == \"object\"].index.to_list()\n",
    "\n",
    "# fill the NaN-values\n",
    "df.loc[:, object_column_names] = df.loc[:, object_column_names].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check, if there are any elements of the type float left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe containing the type of each cell\n",
    "df_object_types =  df[object_column_names].map(type)\n",
    "\n",
    "# print unique datatype for each column\n",
    "for column in df_object_types.columns:\n",
    "    print(f\"Types in column \\\"{column}\\\"\".upper())\n",
    "    print(df_object_types[column].value_counts())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop Messages sent after the data collection period\n",
    "\n",
    "Now we drop all messages that were sent after June 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_data_collection = pd.Timestamp(year=2024, month=6, day=30)\n",
    "filter_outliers_date = df[\"message_date\"].dt.date > end_data_collection.date()\n",
    "df = df[~filter_outliers_date]\n",
    "print(f\"Messages sent after end of data collection left: {(df[\"message_date\"].dt.date > end_data_collection.date()).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean URLs\n",
    "\n",
    "Now we can clean up the urls by removing backslashes and invalid urls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove trailing backspaces\n",
    "df.loc[:,\"webpage_url\"] = df[\"webpage_url\"].str.replace(\"\\\\\\\\$\", '', regex=True)\n",
    "\n",
    "# remove the urls manually found to be invalid\n",
    "invalid_urls = [\"http://3.US-Militär/\"]\n",
    "df.loc[df[\"webpage_url\"].isin(invalid_urls) , \"webpage_url\"] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Drop duplicates\n",
    "\n",
    "As we don't plan on using information on connection based on replies, we can drop the duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Duplicates deleted: {size_before - df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove Messages that are part of an album and contain no text\n",
    "\n",
    "Next, we'll remove all messages that are part of an album and don't contain the album-messages text, as they inflate the message count of chats in our dataset, as this might interfer with the results of our analysis.\n",
    "\n",
    "To make sure, that we don't lose any meaningful data, we'll check how many connections between chats (through forwarded messages) we lost by removing these messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save messages per group counts and count of chats connected through forwarded messages for later comparison\n",
    "group_message_counts = df[\"message_group_id\"].value_counts()\n",
    "message_per_group_counts_before = group_message_counts.value_counts()\n",
    "forwards_pairs_before = df[[\"telegram_chat_id\",\"fwd_from_chat_id\"]].value_counts()\n",
    "\n",
    "# drop all messages that are part of a group and contain no text.\n",
    "mask_album_msg_without_text = (~pd.isna(df[\"message_group_id\"])) & (df[\"message_text\"]=='')\n",
    "df_without_textless_grouped = df[~mask_album_msg_without_text]\n",
    "\n",
    "# we should have mostly one message for each group. Let's check this to affirm, that everything worked as it should. \n",
    "group_message_counts = df_without_textless_grouped[\"message_group_id\"].value_counts()\n",
    "message_per_group_counts = group_message_counts.value_counts()\n",
    "\n",
    "# create a dataframe to compare message counts per group before and after deleting messages without texts\n",
    "message_counts_summary_df = pd.concat([message_per_group_counts_before, message_per_group_counts], axis=1)\n",
    "message_counts_summary_df.columns = [\"with non-text messages\", \"without non-text messages\"]\n",
    "print(\"Group Sizes:\")\n",
    "display(message_counts_summary_df.sort_index())\n",
    "\n",
    "# calculate the amount of connections lost\n",
    "forwards_pairs = df_without_textless_grouped[[\"telegram_chat_id\",\"fwd_from_chat_id\"]].value_counts()\n",
    "print(f\"Forward pairs lost: {abs(len(forwards_pairs)-len(forwards_pairs_before))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some groups containing more than one message. Let's inspect them to rule out any issues with our dataset.\n",
    "To do so, we'll have to actually look into the groups to see what causes grouped messages to have different texts. \n",
    "\n",
    "To find a suitable starting point, we'll isolate the chats with the most cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albums_with_multiple_messages = group_message_counts[group_message_counts > 1].index\n",
    "groups_with_multiple_messages = df_without_textless_grouped[df_without_textless_grouped[\"message_group_id\"].isin(albums_with_multiple_messages)]\n",
    "top_chats = groups_with_multiple_messages.groupby(\"chat_name\").size().sort_values(ascending=False).head(5)\n",
    "\n",
    "print(f\"Chats with most grouped messages with different text: \\n{top_chats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual inspection of chats pointed towards a peculiarity in the way Telegram handles messages containing multiple documents, audio files, images, or videos sent in the same message.\n",
    "\n",
    "This is related to Telegram's \"Captions\" feature, which allows adding individual texts to elements of a grouped message.\n",
    "\n",
    "- For images and videos, this is directly possible via the [\"Captions\" feature](https://web.archive.org/web/20240302001613/https://telegram.org/blog/captions-places), which allows each image/video to be assigned a unique caption from the photo editing panel.\n",
    "\n",
    "- For documents and audio files, the option to assign individual captions for parts of grouped messages is not provided by Telegram. However, as GitHub user \"Neurotoxin001\" pointed out in [this thread](https://github.com/telegramdesktop/tdesktop/issues/8944), they can be added after sending a message by first sending the grouped files and editing each of them afterward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "\n",
    "- For \"classical\" Albums, Telegram provides only one caption. As we won't work with the individual elements of an album, we'll keep only the message containing the caption.\n",
    "\n",
    "- Telegram offers some workarounds to add individual captions to grouped messages containing files. Since each caption contains meaningful information on the chat's topic, we'll keep them. **To reflect their belonging to the same message, we may aggregate them later on.**\n",
    "\n",
    "- We lost 72 connections by removing grouped messages without text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Aggregate captions of grouped messages.\n",
    "\n",
    "To make our dataset and its message count reflect the way telegram actually displays messages more closely, we'll aggregate captions of different media-files sent in the same grouped message. \n",
    "\n",
    "As the captions might not be the only columns with differing values across messages of a group, we have to be careful not to lose meaningful information. \n",
    "\n",
    "To make sure we don't, we'll:\n",
    "\n",
    "1. Find columns that contain different values across messages belonging to a group (this might take a while)\n",
    "\n",
    "2. Aggregate them into a single message, applying different aggregation methods according to a column's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group our dataframe by messages group ids\n",
    "grouped = df_without_textless_grouped.groupby(\"message_group_id\")\n",
    "\n",
    "# create a list of all column names with different values in the same group\n",
    "differing_columns = set()\n",
    "for group_id, group in grouped:\n",
    "    differing_columns_group = [col for col in group.columns if group[col].nunique(dropna=False) > 1]\n",
    "    differing_columns.update(differing_columns_group)\n",
    "\n",
    "print(\"Columns with differing values across groups:\")\n",
    "print(differing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group our dataframe by messages group ids\n",
    "grouped = df_without_textless_grouped.groupby(\"message_group_id\")\n",
    "\n",
    "# Define aggregation functions\n",
    "def keep_min(series: pd.Series) -> Union[int,float,datetime]:\n",
    "    return series.min()\n",
    "\n",
    "def aggregate_text(texts: pd.Series, separator:str) -> str:\n",
    "    return separator.join(texts)\n",
    "\n",
    "def keep_available(series: pd.Series) -> Any:\n",
    "    return series.dropna().iloc[0] if not series.dropna().empty else np.nan\n",
    "\n",
    "def keep(series: pd.Series) -> Any:\n",
    "    return series.iloc[0] #simply keep the first value, as we expect them to be identical across group rows\n",
    "\n",
    "# intialise a dictionary with the aggregation methods we want to apply to the grouped message's columns as values and column names as keys.\n",
    "aggregation_methods = dict()\n",
    "\n",
    "# create a list of all column-names in our dataframe\n",
    "all_cols = df_without_textless_grouped.columns.to_list()\n",
    "\n",
    "# isolate all columns we want to keep unchanged and set their aggregation method to 'keep'\n",
    "columns_to_keep = [col for col in all_cols if col not in differing_columns]\n",
    "for col in columns_to_keep:\n",
    "    aggregation_methods[col] = keep\n",
    "\n",
    "# add methods for the columns we want to aggregate \n",
    "aggregation_methods[\"television_message_id\"] = keep_min\n",
    "aggregation_methods[\"message_date\"] = keep_min\n",
    "aggregation_methods[\"telegram_message_id\"] = keep_min\n",
    "aggregation_methods[\"collection_time\"] = keep_min\n",
    "aggregation_methods[\"message_media_type\"] = lambda x: aggregate_text(x, separator=',')\n",
    "aggregation_methods[\"message_text\"] = lambda x: aggregate_text(x, separator=' ')\n",
    "aggregation_methods[\"message_reactions_count\"] = keep_available\n",
    "aggregation_methods[\"message_reactions\"] = keep_available\n",
    "aggregation_methods[\"message_view_count\"] = 'mean'\n",
    "aggregation_methods[\"message_fwd_count\"] = 'mean'\n",
    "\n",
    "# aggregate messages for each group using the methods defined above\n",
    "aggregated_df = grouped.agg(aggregation_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As aggregation can mess with the types of our dataframe-columns, we might need to correct them. \n",
    "Let's check for inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare original and aggregated dtypes\n",
    "original_types = df_without_textless_grouped.dtypes\n",
    "aggregated_types = aggregated_df.dtypes\n",
    "\n",
    "dtype_comparison = pd.DataFrame({\n",
    "    \"original\": original_types,\n",
    "    \"aggregated\": aggregated_types\n",
    "})\n",
    "\n",
    "# print columns with different types between our original df and the newly aggregated df\n",
    "inconsistent_types = dtype_comparison[dtype_comparison[\"original\"]!=dtype_comparison[\"aggregated\"]]\n",
    "print(f\"Number of inconsistent types: {inconsistent_types.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index, as it is currently set to 'message_group_id'\n",
    "aggregated_df = aggregated_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare the group-sizes again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_message_counts = aggregated_df[\"message_group_id\"].value_counts()\n",
    "message_per_group_counts = group_message_counts.value_counts()\n",
    "\n",
    "message_counts_summary_df_2 = pd.concat([message_counts_summary_df, message_per_group_counts], axis=1)\n",
    "message_counts_summary_df_2.columns = [\"with non-text messages\", \"without non-text messages\", \"after aggregation\"]\n",
    "\n",
    "print(\"Group Sizes:\")\n",
    "display(message_counts_summary_df_2.sort_index())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every group is represented by one message.\n",
    "\n",
    "Finally, we can replace the grouped messages in our current dataframe with the ones we just aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all messages that are part of a group\n",
    "df_without_grouped_messages = df_without_textless_grouped[pd.isna(df_without_textless_grouped[\"message_group_id\"])]\n",
    "\n",
    "# replace them with the aggregated rows we created earlier\n",
    "df_with_aggregated_groups = pd.concat([df_without_grouped_messages, aggregated_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got rid of grouped messages without a text and aggregated those with several captions into a single row, we can take another look at our dataframe.\n",
    "\n",
    "We'll inspect:\n",
    "\n",
    "1. If there are any groups with more than one message left.\n",
    "\n",
    "2. How many messages we lost.\n",
    "\n",
    "3. If the average message length increased (We expect this to be the case, as we deleted messages without text and aggregated others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for groups with more than one message\n",
    "group_sizes = df_with_aggregated_groups.groupby(\"message_group_id\").size().value_counts()\n",
    "print(f\"Group Sizes in the dataframe:\")\n",
    "print(group_sizes)\n",
    "print()\n",
    "\n",
    "# calculate message loss\n",
    "message_loss = abs(df.shape[0]-df_with_aggregated_groups.shape[0])\n",
    "print(f\"Message Loss: {message_loss}\")\n",
    "print()\n",
    "\n",
    "# calculate average message length before and after aggregation:\n",
    "avg_msg_len_before = np.mean(df[\"message_text\"].apply(lambda x: len(x)))\n",
    "avg_msg_len_after  = np.mean(df_with_aggregated_groups[\"message_text\"].apply(lambda x: len(x)))\n",
    "print(f\"Average message length before aggregation: {avg_msg_len_before}\")\n",
    "print(f\"Average message length after aggregation: {avg_msg_len_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As estimated, the changes increased the average message length, reduced each group to one messsage and deleted about 11% of our overall dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Remove Chats containing only few messages\n",
    "\n",
    "As we need a certain amount of content for the vectorisation and clustering of a chat, we'll drop those with fewer than 1000 available messages. \n",
    "\n",
    "Once we're done, we can check the message count statistics for improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chats_message_counts = df_with_aggregated_groups.groupby(\"chat_name\").size()\n",
    "over_1000 = chats_message_counts[chats_message_counts > 1000]\n",
    "over_1000_chat_names = list(over_1000.index)\n",
    "df_over_1000 = df_with_aggregated_groups[df_with_aggregated_groups[\"chat_name\"].isin(over_1000_chat_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(over_1000.describe())\n",
    "print(f\"Messages removed: {abs(df_with_aggregated_groups.shape[0] - df_over_1000.shape[0])}\")\n",
    "print(f\"Chats removed: {abs(len(chats_message_counts)-len(over_1000))}\")\n",
    "print(\"\")\n",
    "print(f\"Messages remaining: {df_over_1000.shape[0]}\")\n",
    "print(f\"Chats remaining: {len(over_1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing 3508 chats containing 142145 messages, we're left with 351 chats with an average message count of 4506 and no fewer than 1005 messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we analyze and visualize the cleaned data to identify features and patterns that might be useful for clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compare Chat Types\n",
    "\n",
    "As the type of chat dictates which datapoints are available, we first have to check the distribution of chat-types across our dataset.\n",
    "\n",
    "Possible chat-types consist of:\n",
    "\n",
    "- **Broadcast-Channels**: One-to-many communication. There are no replies from Members of this chat, but the API reports view- and forward-counts.\n",
    "\n",
    "- **Megagroups**: Many-to-many communication. The API does not report detailled interaction-metrics, but there are replies.\n",
    "\n",
    "To analyse their differences, we'll:\n",
    "\n",
    "1. Check the distribution of chat-types in our dataset.\n",
    "\n",
    "\n",
    "2. Look for differences in the available data for each chat type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_over_1000.groupby(\"telegram_chat_id\")[\"chat_type\"].unique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of our channels are Broadcast-Channels.\n",
    "\n",
    "**Next, let's see if we can find patterns in the available data for each chat.**\n",
    "\n",
    "We will only keep messages that were forwarded from chats of the same type, as messages forwarded from other chat types might introduce information not natively available in the chat type we are analyzing. We'll use the complete dataset, to maximize the available datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe and dictionary containing all the chat_ids in our initial dataset and the chat id as index\n",
    "chats_type = df[[\"telegram_chat_id\", \"chat_type\"]].drop_duplicates()\n",
    "chats_type = chats_type.set_index(\"telegram_chat_id\")\n",
    "type_mapping = chats_type[\"chat_type\"].to_dict()\n",
    "\n",
    "# create a new column containing the type of the source chat\n",
    "df.loc[:, \"source_chat_type\"] = df[\"fwd_from_chat_id\"].map(type_mapping)\n",
    "\n",
    "# replace NaN values in the newly created column to enable comparison with the \"chat_type\" column\n",
    "df.loc[:, \"source_chat_type\"] = df[\"source_chat_type\"].fillna('')\n",
    "\n",
    "# get all messages that were forwarded from chats\n",
    "is_fwd = df[(df[\"fwd_from_user_name\"]=='')]\n",
    "is_fwd = df[df[\"is_fwd\"] == True]\n",
    "\n",
    "# get all messages not forwarded\n",
    "is_not_fwd = df[df[\"is_fwd\"] == False]\n",
    "\n",
    "# keep only messages that are forwarded and have the same source chat and chat type\n",
    "is_fwd = is_fwd[is_fwd[\"chat_type\"] == is_fwd[\"source_chat_type\"]]\n",
    "\n",
    "# add the cleaned forwarded messages and the non-forwarded messages back together\n",
    "cleaned_types = pd.concat([is_fwd, is_not_fwd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare missing data percentages per column for both types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seperate DataFrames for each \"chat_type\".\n",
    "df_broadcast = cleaned_types[cleaned_types[\"chat_type\"] == \"broadcast\"]\n",
    "df_megagroup = cleaned_types[cleaned_types[\"chat_type\"] == \"megagroup\"]\n",
    "\n",
    "# Calculate percentage of missing values per column for chats of type \"broadcast\". Keep only columns with a missing value percentag > 0\n",
    "missing_values_broadcast = df_broadcast.isna() | (df_broadcast == '')\n",
    "missing_percentag_broadcast = missing_values_broadcast.sum().apply(lambda x: x/df_broadcast.shape[0])\n",
    "missing_percentag_broadcast = missing_percentag_broadcast[missing_percentag_broadcast > 0]\n",
    "\n",
    "# Calculate percentage of missing values per column for chats of type \"megagroup\". Keep only columns with a missing value percentag > 0\n",
    "missing_values_megagroup= df_megagroup.isna() | (df_megagroup == '')\n",
    "missing_percentag_megagroup = missing_values_megagroup.sum().apply(lambda x: x/df_megagroup.shape[0])\n",
    "missing_percentag_megagroup =missing_percentag_megagroup[missing_percentag_megagroup > 0]\n",
    "\n",
    "# Combine the two DataFrames into one \n",
    "combined_missing_percentages = pd.concat([missing_percentag_broadcast, missing_percentag_megagroup], axis=1)\n",
    "combined_missing_percentages.columns = ['Missing Values Broadcast', 'Missing Values Megagroup']\n",
    "combined_missing_percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "\n",
    "To check for meaningful differences between chat-types, we'll look at columns with extremly high missing percentages (over 95%) and higher availability in the other:\n",
    "\n",
    "**message_fwd_count, message_view_count:** \n",
    "- Extremly high missing percentage for Broadcasts, as they are not provided by Telegram.\n",
    "\n",
    "**sender_first_name, sender_last_name**: \n",
    "- Only available in Megagroups, as they allow differfent users to send messages. The user recorded for broadcasts has usually only the channels name as a username.\n",
    "\n",
    "**fwd_from_chat_handle, fwd_from_chat_id, source_chat_type:** \n",
    "- Extremly high missing percentages for Megagroups. Indicates that there are few messages forwarded chats in Megagroups.\n",
    "\n",
    "**reply_to_message_id:**\n",
    "- Since Broadcasts do not support direct replies from recipients (only the broadcaster sends messages), the reply_to_message_id field is less relevant and often not used. However, it is possible for a broadcaster to forward messages with a reply status from other chats.\n",
    "\n",
    "**webpage_author, webpage_description, webpage_title, webpage_url:** \n",
    "- These fields are more often missing in Megagroups, suggesting a lower number of messages linking to webpages.\n",
    "\n",
    "**message_group_id:** \n",
    "- Low availability for Megagroups suggests that sending albums of media-files is less common. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect Message Type Ratios\n",
    "\n",
    "Next, we'll analyse the distribution of different message types for each chat. \n",
    "We'll focus on the following message-type-ratios across all chats:\n",
    "\n",
    "- **Forward Ratio**: The proportion of forwarded messages in each chat group.\n",
    "- **Reply Ratio**: The proportion of replies in each chat group.\n",
    "- **Webpage Ratio**: The proportion of messages containing a webpage URL in each chat group.\n",
    "- **Image Ratio**: The proportion of messages containing at least one image in each chat group.\n",
    "- **Video Ratio**: The proportion of messages containing at least one video in each chat group.\n",
    "- **Document Ratio**: The proportion of messages containing at least one document in each chat group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid SettingWithCopyWarning\n",
    "df_over_1000 = df_over_1000.copy()\n",
    "\n",
    "# create flag-columns \n",
    "df_over_1000.loc[:, \"is_webpage\"] = df_over_1000[\"webpage_url\"].apply(lambda x: False if x == '' else True)\n",
    "df_over_1000.loc[:, \"is_image\"] = df_over_1000[\"message_media_type\"].str.contains('MessageMediaPhoto|MessageMediaDocumentPhoto', regex=True)\n",
    "df_over_1000.loc[:, \"is_video\"] = df_over_1000[\"message_media_type\"].str.contains('MessageMediaDocumentVideo', regex=True)\n",
    "df_over_1000.loc[:, \"is_document\"] = df_over_1000[\"message_media_type\"].str.contains('MessageMediaDocumentDocument', regex=True)\n",
    "\n",
    "# create a series containing chat sizes\n",
    "chat_sizes = df_over_1000.groupby(\"telegram_chat_id\").size()\n",
    "# chat messages per chat\n",
    "messages_per_chat = df_over_1000.groupby(\"telegram_chat_id\")\n",
    "\n",
    "# create a series containing the ratio of forwarded messages per chat.\n",
    "fwd_msg_per_chat = messages_per_chat[\"is_fwd\"].sum()\n",
    "fwd_ratio_per_chat = fwd_msg_per_chat/chat_sizes\n",
    "\n",
    "# create a series containing the ratio of media-messages per chat.\n",
    "reply_msg_per_chat = messages_per_chat[\"is_reply\"].sum()\n",
    "reply_ratio_per_chat = reply_msg_per_chat/chat_sizes\n",
    "\n",
    "# create a series containing the ratio of webpage messaages per chat.\n",
    "webpage_msg_per_chat = messages_per_chat[\"is_webpage\"].sum()\n",
    "webpage_ratio_per_chat = webpage_msg_per_chat/chat_sizes\n",
    "\n",
    "# create a series containing the ratio of image messages per chat.\n",
    "image_msg_per_chat = messages_per_chat[\"is_image\"].sum()\n",
    "image_ratio_per_chat = image_msg_per_chat/chat_sizes\n",
    "\n",
    "# create a series containing the ratio of video messages per chat.\n",
    "video_msg_per_chat = messages_per_chat[\"is_video\"].sum()\n",
    "video_ratio_per_chat = video_msg_per_chat/chat_sizes\n",
    "\n",
    "# create a series containing the ratio of video messages per chat.\n",
    "doc_msg_per_chat = messages_per_chat[\"is_document\"].sum()\n",
    "doc_ratio_per_chat = doc_msg_per_chat/chat_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare the distribution of different message types across all chats. \n",
    "\n",
    "\n",
    "We'll use a violin plot, as it combines both information on the distribution’s density and spread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [fwd_ratio_per_chat, reply_ratio_per_chat, webpage_ratio_per_chat, image_ratio_per_chat, video_ratio_per_chat, doc_ratio_per_chat]\n",
    "labels = ['Forward Ratio...', 'Reply Ratio...', 'Webpage Ratio...', 'Image Ratio...', 'Video Ratio...', 'Document Ratio...']\n",
    "dfs = []\n",
    "\n",
    "# Create a DataFrame for visualisation\n",
    "for dataset, label in zip(data, labels):\n",
    "    \n",
    "    df_ratio = pd.DataFrame({\n",
    "        'Ratio': dataset,\n",
    "        'Metric': label\n",
    "    })\n",
    "    \n",
    "    dfs.append(df_ratio)\n",
    "ratios = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Create the violin plots\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.violinplot(x='Metric', y='Ratio', data=ratios, ax=ax, palette='pastel', inner=\"quart\", hue=\"Metric\", legend=False)\n",
    "ax.grid(True)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "ax.set_xlabel('...over all chats')\n",
    "ax.set_ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Ratio**:\n",
    "- The distribution is broad with a significant number of chats having a forward ratio between 5% and 20%.\n",
    "- There are a few chats with very high forward ratios (close to 1), indicating that some chats have a high proportion of forwarded messages. These could have the purpose of being \"Aggregators\".\n",
    "- The distributions are spread out, which makes forward ration a distinctive feature.\n",
    "\n",
    "**Reply Ratio:**\n",
    "- The distribution is very narrow and close to zero for most chats.\n",
    "- This indicates that replies are relatively rare in the chats of our dataset.\n",
    "- As most of our chats are Broadcast Channels and don't allow replies this is to be expected.\n",
    "\n",
    "**Webpage Ratio:**\n",
    "- The distribution shows a moderate density around a ratio of 0 to 0.2.\n",
    "- This suggests that a small but consistent proportion of messages contain webpage URLs.\n",
    "\n",
    "**Image Ratio:**\n",
    "- The distribution is wider and peaks around 0.3 to 0.5, indicating that many chat have a substantial proportion of image messages.\n",
    "- There are also chats with very high image ratios, showing that some chats predominantly share images.\n",
    "\n",
    "**Video Ratio:**\n",
    "- The distribution is somewhat similar to the image ratio but slightly lower, peaking around 0.2 to 0.4.\n",
    "- This indicates that video messages are common but less frequent than image messages.\n",
    "\n",
    "**Document Ratio:**\n",
    "- The distribution is very narrow and close to zero for most chat chats.\n",
    "- This indicates that document messages are quite rare across the chats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect available webpage information\n",
    "Telegram can provide additional information about webpages, such as the author, title, and a brief summary of their content. To determine the usefulness of this information, we need to assess how commonly it appears in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_messages = df_over_1000[df_over_1000[\"is_webpage\"] == True]\n",
    "nr_webpage_messages = webpage_messages.shape[0]\n",
    "\n",
    "nr_has_webpage_author = webpage_messages[webpage_messages[\"webpage_author\"]!=''].shape[0]\n",
    "nr_has_webpage_description = webpage_messages[webpage_messages[\"webpage_description\"]!=''].shape[0]\n",
    "nr_has_webpage_title = webpage_messages[webpage_messages[\"webpage_title\"]!=''].shape[0]\n",
    "\n",
    "print(f\"Percentage of webpage messages with author: {round(nr_has_webpage_author/nr_webpage_messages*100, 2)}\")\n",
    "print(f\"Percentage of webpage messages with description: {round(nr_has_webpage_description/nr_webpage_messages*100, 2)}\")\n",
    "print(f\"Percentage of webpage messages with title: {round(nr_has_webpage_title/nr_webpage_messages*100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings:\n",
    "- As we can see, most messages in our dataset that contain a reference to webpage also provide a description and a title. \n",
    "\n",
    "- Authors are less frequently provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Connections between chats\n",
    "\n",
    "We have established, that a meaningful amount of messages in our chats are forwarded from other chats. Let's take a closer look at these connections. A connection is defined as a pair of a source chat (the chat a message was forwaded from) and a target chat (the chat we found the message in).\n",
    "\n",
    "To inspect these connections, we'll calculate:\n",
    "\n",
    "1) The frequency distribution of source chats in our dataset.\n",
    "\n",
    "2) How often source chats appear across different chats.\n",
    "\n",
    "2) The amount of unique connections per chat.\n",
    "\n",
    "3) The percentage of unique connections out of all connections per chat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all messages forwarded from chats (and not from users)\n",
    "forwarded_messages = df_over_1000[(df_over_1000[\"is_fwd\"])&((df_over_1000[\"fwd_from_user_name\"]==''))]\n",
    "\n",
    "# get all connections between chats & drop messages forwarded from private chat (which lead to fwd_from_chat_id being unknown)\n",
    "connections = forwarded_messages[[\"telegram_chat_id\",\"fwd_from_chat_id\"]]\n",
    "connections = connections[~pd.isna(connections[\"fwd_from_chat_id\"])]\n",
    "\n",
    "# calculate the number of times source chats occurs in our dataset\n",
    "frequency_of_source_chats = connections[[\"fwd_from_chat_id\"]].value_counts()\n",
    "\n",
    "# display the distribution of connection-frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "frequency_of_source_chats.plot(kind='hist', bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Source Chat Frequencies')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group forwarded messages by chat\n",
    "grouped = connections.groupby(\"telegram_chat_id\")\n",
    "\n",
    "# get unique source chats for each chat\n",
    "source_chats_per_chat = grouped[\"fwd_from_chat_id\"].unique()\n",
    "\n",
    "# count occurances of source chats across chats in our dataset\n",
    "source_occurances_count = {}\n",
    "for group in source_chats_per_chat:\n",
    "    for source in group:\n",
    "        if source in source_occurances_count:\n",
    "            source_occurances_count[source] += 1\n",
    "        else:\n",
    "            source_occurances_count[source] = 1\n",
    "\n",
    "print(\"Occurances of source chats across different chats:\")\n",
    "pd.Series(source_occurances_count).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique connections between chats\n",
    "unique_connections = connections.drop_duplicates()\n",
    "\n",
    "# group connections by chat ID\n",
    "forwarded_messages_per_chat = forwarded_messages.groupby(\"telegram_chat_id\").size()\n",
    "unique_connections_per_chat = unique_connections.groupby(\"telegram_chat_id\").size()\n",
    "\n",
    "# create a dataFrame to align the indices of the metrics calculated above\n",
    "metrics_df = pd.DataFrame({\n",
    "    'forwarded_messages_count': forwarded_messages_per_chat,\n",
    "    'unique_connections_count': unique_connections_per_chat\n",
    "})\n",
    "\n",
    "#unique_percentages_per chat = metrics_df\n",
    "metrics_df[\"unique_connection_percentage\"] = metrics_df[\"unique_connections_count\"]/metrics_df[\"forwarded_messages_count\"]\n",
    "\n",
    "print(\"Unique connection count per chat:\")\n",
    "display(metrics_df[\"unique_connections_count\"].describe())\n",
    "print()\n",
    "\n",
    "print(\"Unique connection percentages per chat:\")\n",
    "display(metrics_df[\"unique_connection_percentage\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings\n",
    "\n",
    "\n",
    "**Source Chat frequencies**\n",
    "- Ignoring some outlier, wich ocurr between 4000 and 9000 times, a majority of source chats in our dataset are relatively unique and seldome occur more than 500 times. \n",
    "\n",
    "**Source Chat distribution across chats**\n",
    "- The majority of source chats have limited spread, appearing in a few chats only. However, the presence of source chats that appear in many different chats suggests there are a few highly influential or central chats in the network.\n",
    "\n",
    "**Unique connections per chat**\n",
    "- On average, chats have few unique connections, with 75% of the chats having a unique connection percentage of 25.28% or less. This points towards a significant portion of forwarded message originating from repeated sources.\n",
    "\n",
    "- Some outliers get their forwarded messages only from a single chat or exclusively from unique connections. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Check languages of messages,  webpages and chats\n",
    "\n",
    "Now, we'll inspect the language of the chats and messages in our dataset. \n",
    "To do so, we'll use a fasttext-model for language identification, which can be found [here](https://fasttext.cc/docs/en/language-identification.html).\n",
    "\n",
    "As the model was trained on UTF-8 data, it expects UTF-8 as input. This sould be the case, as pandas `read_csv`-function imports text in UTF-8 by default.\n",
    "\n",
    "We'll approach the task differntly for each text category:\n",
    "\n",
    "1. For messages, we'll simply check the messages text. \n",
    "\n",
    "2. For webpages, we'll check the description (if provided)\n",
    "\n",
    "3. For chats, well aggregate their messages and webpage description to inspect the most prominent languages per chat.\n",
    "\n",
    "4. If we find multilingual chats, we'll inspect their language composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download model:\n",
    "\n",
    "First, we need to download the fasttext-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the fasttext model\n",
    "\n",
    "fasttext_path = os.path.join(os.getcwd(), '../data/models/lid.176.bin')\n",
    "\n",
    "if not os.path.isfile(fasttext_path):\n",
    "    !wget -O {fasttext_path} https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "else:\n",
    "    print(\"Fasttext-Model already downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify languages:\n",
    "\n",
    "Now we can use the model to identify the languages of our messages and webpage descriptions. This might take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "fasttext_model = fasttext.load_model(fasttext_path)\n",
    "\n",
    "def identify_language(text):\n",
    "    lang_detected = fasttext_model.predict(text)\n",
    "    return lang_detected[0][0]\n",
    "\n",
    "# clean the webpage text, as the model expect text without newlines\n",
    "df_over_1000.loc[:,\"webpage_description\"] = df_over_1000[\"webpage_description\"].str.replace(\"\\n\",\" \")\n",
    "\n",
    "# detect message-  and webpage-languages. If the column contains empty text, the language is set to nan\n",
    "df_over_1000.loc[:,\"message_text_lang\"] = df_over_1000[\"message_text\"].apply(lambda x: identify_language(x) if not x == '' else np.nan)\n",
    "df_over_1000.loc[:,\"webpage_description_lang\"] = df_over_1000[\"webpage_description\"].apply(lambda x: identify_language(x) if not x == '' else np.nan)\n",
    "\n",
    "# clean the output\n",
    "df_over_1000.loc[:,\"message_text_lang\"] = df_over_1000[\"message_text_lang\"].str.replace(\"__label__\",\"\")\n",
    "df_over_1000.loc[:,\"webpage_description_lang\"] = df_over_1000[\"webpage_description_lang\"].str.replace(\"__label__\",\"\")\n",
    "\n",
    "# print the new columns\n",
    "pd.concat([df_over_1000[\"message_text_lang\"], df_over_1000[\"webpage_description_lang\"]], axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the fasttext-model returns ISO-639 language code, which we'll resolve for easier interpretation. \n",
    "\n",
    "The dataset used for this purpose can be found [here](https://github.com/datasets/language-codes/blob/master/data/language-codes-full.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_path = os.path.join(os.getcwd(), \"../data/auxiliary/language-codes-full.csv\")\n",
    "iso639_mapping = pd.read_csv(iso_path)\n",
    "\n",
    "# create a dictionary of alpha2 encodings and delete the key 'nan'\n",
    "alpha2 = iso639_mapping.set_index(\"alpha2\")[\"English\"].to_dict()\n",
    "alpha2 = {k: v for k, v in alpha2.items() if not pd.isna(k)}\n",
    "\n",
    "df_over_1000.loc[:,\"message_text_lang\"] = df_over_1000[\"message_text_lang\"].map(alpha2, na_action='ignore')\n",
    "df_over_1000.loc[:,\"webpage_description_lang\"] = df_over_1000[\"webpage_description_lang\"].map(alpha2, na_action='ignore')\n",
    "pd.concat([df_over_1000[\"message_text_lang\"], df_over_1000[\"webpage_description_lang\"]], axis=1).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Language Distributions of Messages and Webpages:\n",
    "\n",
    "Let's take a look at the language distributions of the messages and webpage description in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate language distributions for messages\n",
    "language_distribution = df_over_1000[\"message_text_lang\"].value_counts()\n",
    "\n",
    "# group all languages with fewer than 2500 messages into \"other\"\n",
    "others = language_distribution[language_distribution < 2500].sum()\n",
    "language_distribution = language_distribution[language_distribution > 2500]\n",
    "language_distribution[\"Other\"] = others\n",
    "\n",
    "# calculate language distributions for webpage descriptions\n",
    "language_distribution_wp = df_over_1000[\"webpage_description_lang\"].value_counts()\n",
    "\n",
    "# group all languages with fewer than 2500 messages into \"other\"\n",
    "others_wp = language_distribution_wp[language_distribution_wp < 2500].sum()\n",
    "language_distribution_wp = language_distribution_wp[language_distribution_wp > 2500]\n",
    "language_distribution_wp[\"Other\"] = others_wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distributions\n",
    "\n",
    "# define color scheme\n",
    "colors = sns.color_palette('pastel')\n",
    "\n",
    "# create figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "def hide_small_pct(pct):\n",
    "    return ('%.0f%%' % pct) if pct > 5 else ''\n",
    "\n",
    "# create message pie chart\n",
    "explode_messages = [0.1 if i == 'German' else 0 for i in language_distribution.index] # highlight the german wedge\n",
    "wedges, texts, autotexts = ax1.pie(language_distribution, colors=colors,\n",
    "                                  explode=explode_messages, autopct=hide_small_pct)\n",
    "ax1.set_title(\"Language Distribution (Messages)\")\n",
    "\n",
    "# create webpage pie chart\n",
    "explode_wp = [0.1 if i == 'German' else 0 for i in language_distribution_wp.index] # higlight the german wedge\n",
    "wedges_wp, texts_wp, autotexts_wp = ax2.pie(language_distribution_wp, colors=colors,\n",
    "                                  explode=explode_wp, autopct=hide_small_pct)\n",
    "ax2.set_title(\"Language Distribution (Webpage Descriptions)\")\n",
    "\n",
    "# Combine wedges and labels from both pie charts for the legend\n",
    "all_wedges = wedges + wedges_wp\n",
    "all_labels = list(language_distribution.index) + list(language_distribution_wp.index)\n",
    "unique_labels = list(dict.fromkeys(all_labels))\n",
    "filtered_wedges = [wedges[all_labels.index(label)] for label in unique_labels]\n",
    "\n",
    "# Create legend\n",
    "fig.legend(filtered_wedges, unique_labels,\n",
    "           title=\"Languages\",\n",
    "           loc=\"center left\",\n",
    "           bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "# display plot\n",
    "plt.setp(autotexts, size=8, weight=\"bold\")\n",
    "plt.setp(autotexts_wp, size=8, weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect Languages of Chats\n",
    "\n",
    "Now we can inspect the majority language of our chats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group messages and webpage descriptions by chat\n",
    "grouped = df_over_1000.groupby(\"telegram_chat_id\")\n",
    "message_lang_per_chat = grouped[\"message_text_lang\"].value_counts()\n",
    "webpage_lang_per_chat = grouped[\"webpage_description_lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the multi-indexed series into a dataframe\n",
    "#       index:      chat id\n",
    "#       columns:    language names\n",
    "#       cells:      number of messages or webpages in a chat in a language\n",
    "\n",
    "message_lang_per_chat_df = message_lang_per_chat.unstack(fill_value=0)\n",
    "webpage_lang_per_chat_df = webpage_lang_per_chat.unstack(fill_value=0)\n",
    "\n",
    "# sum the language counts of messages and webpages for each chat\n",
    "lang_per_chat = message_lang_per_chat_df.add(webpage_lang_per_chat_df, fill_value=0)\n",
    "\n",
    "# get the language with most frequent messages/webpages per chat\n",
    "most_frequent_lang = lang_per_chat.idxmax(axis=1)\n",
    "\n",
    "# calculate the distribution of most frequent languages in our chats\n",
    "most_frequent_lang_distr = most_frequent_lang.value_counts()\n",
    "\n",
    "# group all languages with fewer than 5 chats into \"other\"\n",
    "others_chats = most_frequent_lang_distr[most_frequent_lang_distr < 5].sum()\n",
    "language_distribution_chats = most_frequent_lang_distr[most_frequent_lang_distr > 5]\n",
    "language_distribution_chats[\"Other\"] = others_chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distributions\n",
    "# define color scheme\n",
    "colors = sns.color_palette('pastel')\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# create chat distribution pie chart\n",
    "explode_messages = [0.1 if i == 'German' else 0 for i in most_frequent_lang_distr.index] # highlight the german wedge\n",
    "wedges, texts, autotexts = ax.pie(most_frequent_lang_distr.values, colors=colors,\n",
    "                                  explode=explode_messages, autopct=hide_small_pct)\n",
    "ax.set_title(\"Language Distribution (Chats):\" )\n",
    "\n",
    "ax.legend(wedges, language_distribution_chats.index,\n",
    "          title=\"Languages\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=8, weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for multilingual chats\n",
    "\n",
    "Finally, we can check how many chats contain a substancial amount of messages in different languages. \n",
    "\n",
    "For our purpose, every chat with more than 25% of messages and webpage description in another language than the most frequent one is considered \"multilingual\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.75\n",
    "\n",
    "# calculate messages + webpage description sums for each chat\n",
    "text_count_per_chat = lang_per_chat.sum(axis=1)\n",
    "\n",
    "# calculate languge percentages for each chat\n",
    "language_per_chat_pct = lang_per_chat.div(text_count_per_chat, axis=0)\n",
    "\n",
    "# get chats, where the most frequent language has a percentage < 75%\n",
    "multilingual_chats = language_per_chat_pct[language_per_chat_pct.max(axis=1) < threshold]\n",
    "\n",
    "# get descriptive statistics about the language variety in multilingual chats by inspecting the pecentage values of the most common language\n",
    "print(\"Percentage made up of the most frequent languages per chat:\")\n",
    "multilingual_chats.max(axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Language Compositions\n",
    "\n",
    "We have confirmed, that there are multilingual chats. Now, let's look at their language composition.\n",
    "\n",
    "To isolate the composition of each chat, we'll:\n",
    "\n",
    "1. **Set Threshold:** Setting a threshold to 1%, meaning, we'll consider each language that makes up more of 1% of a chat a part of its language composition.\n",
    "\n",
    "2. **Create a Boolean Index:** Identify languages that make up a percentage of a chat's content above the specified threshold. This Boolean index will indicate whether each language in a chat meets this condition.\n",
    "\n",
    "3. **Calculate the Dot Product of the Boolean Index and Column Names:** Performing a dot product between this Boolean index and the column names will generate a string of column names (representing languages) that pass the threshold, as `True` is treated as 1 and `False`is treated as 0. Accordingly, only the column names corresponding to True are kept. \n",
    "We'll concatenated them with semicolons, as some language names contain commata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = 0.01\n",
    "multilingual_chats_composition = (multilingual_chats>treshold).dot(multilingual_chats.columns+';').str.rstrip(';')\n",
    "\n",
    "print(\"Top 10 Language Combinations:\")\n",
    "multilingual_chats_composition.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings:\n",
    "\n",
    "**Message- and Webpage-Languages:**\n",
    "- The majority of our chats, messages and webpage descriptions are German. \n",
    "\n",
    "- However, there's a significant amount of English texts in our datasets.\n",
    "\n",
    "- A small, but notable amout of Content is Russian and Chinese.\n",
    "\n",
    "**Chat Languages:**\n",
    "\n",
    "- The majority of chats contain predominently german content.\n",
    "\n",
    "- A small subgroup of chats contains predominently english content.\n",
    "\n",
    "**Multilingual Chats:**\n",
    "\n",
    "- Even in chats where the majority language makes up less than 75% of all content, the majority usually still accounts for more than 50% of the chat.\n",
    "\n",
    "- Such multilingual chats make up a small part of the chats in our dataset.\n",
    "\n",
    "- If we consider each language that makes up more than 1% of a chat's content, the most prominent language combination is German and English.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analyse duplicated messages between chats\n",
    "\n",
    "Lastly, we'll check how many message texts are occurring in more than one chat. \n",
    "\n",
    "This might happen, if a message was forwarded from one chat to the other. In this case it is saved twice - once for the source chat and once for the target chat. \n",
    "\n",
    "We'll approach the task by:\n",
    "\n",
    "1. Calculating the number of duplicated texts, including those within the same chat.\n",
    "\n",
    "2. Calculating the number of duplicated texts between chats.\n",
    "\n",
    "3. Checking if there are any discernable relationships between messages with the same text.\n",
    "\n",
    "4. Calculating descriptive statistics regarding the amount of times a duplicated message text occurs in our dataset. \n",
    "\n",
    "5. Finding the chats that shares the most duplicated messages with other chats.\n",
    "\n",
    "\n",
    "#### Calculate the number of duplicated texts in the whole dataset\n",
    "\n",
    "We'll start off by isolating messages with duplicated texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all duplicates while keeping all occurances of a message with duplicated message text\n",
    "duplicates_msg_filter = df_over_1000[\"message_text\"].duplicated(keep=False)\n",
    "duplicates_msg_text = df_over_1000[duplicates_msg_filter]\n",
    "\n",
    "# drop messages with an empty message text to avoid them being counted as duplicates of each other\n",
    "duplicates_msg_text = duplicates_msg_text[duplicates_msg_text[\"message_text\"]!='']\n",
    "\n",
    "count_duplicated_messages = duplicates_msg_text.shape[0]\n",
    "print(f\" Number of messages with duplicated texts: {count_duplicated_messages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm, that there are only duplicates left in our dataframe\n",
    "grouped = duplicates_msg_text.groupby(\"message_text\")\n",
    "occurance_counts = grouped.size()\n",
    "print(f\"Duplicated message text with fewest occurances in our dataset: {min(occurance_counts)}\")\n",
    "print(f\"Only duplicates left: {min(occurance_counts) > 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the number of duplicated texts between chats\n",
    "\n",
    "Next, we'll check how many of these messages are duplicated across more than one chat. \n",
    "\n",
    "This is important, as some chats send the same message text in each of their messages, leading to a high count of duplicates that are not necessarily an indication of a connection between chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique telegram_chat_ids in each group\n",
    "unique_chat_id_counts = grouped['telegram_chat_id'].nunique()\n",
    "\n",
    "# isolate the indices of message texts that occur in more than one chat\n",
    "duplicated_message_texts = unique_chat_id_counts[unique_chat_id_counts > 1].index\n",
    "\n",
    "# isolate messages with duplicated texts across different chats \n",
    "duplicates_msg_text_btw_chats = df_over_1000[df_over_1000['message_text'].isin(duplicated_message_texts)]\n",
    "\n",
    "# TODO: might mess with analysis later on\n",
    "# remove duplicates that are in the same chat\n",
    "duplicates_msg_text_btw_chats = duplicates_msg_text_btw_chats.drop_duplicates([\"message_text\", \"telegram_chat_id\"])\n",
    "\n",
    "# create a copy to avoid warnings\n",
    "duplicates_msg_text_btw_chats = duplicates_msg_text_btw_chats.copy()\n",
    "\n",
    "# drop all messages with single character texts, as duplicates of single characters between chats might be coincidental.\n",
    "duplicates_msg_text_btw_chats.loc[:,\"message_text_len\"] = duplicates_msg_text_btw_chats[\"message_text\"].str.len()\n",
    "duplicates_msg_text_btw_chats = duplicates_msg_text_btw_chats[duplicates_msg_text_btw_chats[\"message_text_len\"]>1]\n",
    "\n",
    "# TODO: Check if all important info is kept\n",
    "# drop all messages with texts consisting only of punctuation, as the similarity between chats might be coincidental.\n",
    "pattern_non_word = r\"^[^\\w\\s]+$\"\n",
    "filter_non_word = ~duplicates_msg_text_btw_chats[\"message_text\"].str.match(pattern_non_word)\n",
    "duplicates_msg_text_btw_chats = duplicates_msg_text_btw_chats[filter_non_word]\n",
    "\n",
    "# confirm that there are only messages with multiple occurances in more than one chat left\n",
    "test_groups = duplicates_msg_text_btw_chats.groupby(\"message_text\")\n",
    "chat_count = test_groups[\"telegram_chat_id\"].nunique()\n",
    "print(f\"Chat count of duplicated message with fewest occurances in different chats: {min(chat_count)}\")\n",
    "print(f\"Only duplicates left: {min(chat_count) > 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_duplicates_msg_text_btw_chats = duplicates_msg_text_btw_chats.shape[0]\n",
    "print(f\" Number of messages with duplicated texts in different chats: {count_duplicates_msg_text_btw_chats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine relationships between messages with identical texts\n",
    "\n",
    "Now that we identified messages with duplicates in different chats, we can check for relationships between these messages.\n",
    "\n",
    "There are two kinds of relationship we'll consider in this step:\n",
    "\n",
    "1. Message texts are duplicates because they were forwarded from the same source chat.\n",
    "\n",
    "2. Message texts are duplicates because one of them is the original and the other is a forwarded instance of this original message.\n",
    "\n",
    "Let's take a look at the makeup of the duplicates in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group messages according to their message text\n",
    "grouped = duplicates_msg_text_btw_chats.groupby(\"message_text\")\n",
    "\n",
    "# count messages with the same text that share a source chat\n",
    "duplicates_shared_source = grouped.apply(lambda group: group[\"fwd_from_chat_id\"].duplicated(keep=False).sum(), \n",
    "                                         include_groups=False)\n",
    "\n",
    "count_duplicates_shared_source = duplicates_shared_source.sum()\n",
    "print(f\"Number of messages with the same text that share a source chat: {count_duplicates_shared_source}\")\n",
    "print(f\"Percentage out of all messages duplicated across chats: {round(count_duplicates_shared_source/count_duplicates_msg_text_btw_chats, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if messages with identical texts include both a message with duplicated text and the original version from its source chat\n",
    "duplicates_with_source = grouped.apply(lambda group: group['fwd_from_chat_id'].isin(group['telegram_chat_id']),\n",
    "                                       include_groups=False)\n",
    "\n",
    "# sum how many duplicates with a source exist\n",
    "count_duplicates_with_source = duplicates_with_source.sum()\n",
    "print(f\"Number of messages with the same text that share a source chat: {count_duplicates_with_source}\")\n",
    "print(f\"Percentage out of all messages duplicated across chats: {round(count_duplicates_with_source/count_duplicates_msg_text_btw_chats, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical analysis of duplicated messages between chats.\n",
    "\n",
    "Now we can further examine the properties of the duplicated messages. \n",
    "1. We'll start of by taking a closer look at the amount of times duplicated messages occur in our dataset. \n",
    "\n",
    "2. Afterwards we'll inspect the average amount of duplicated messages in our dataset's chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_duplicated_message = grouped.size()\n",
    "\n",
    "# Calculate the 90th and 95th percentiles\n",
    "p90 = counts_duplicated_message.quantile(0.90)\n",
    "p95 = counts_duplicated_message.quantile(0.95)\n",
    "p99 = counts_duplicated_message.quantile(0.99)\n",
    "\n",
    "display(counts_duplicated_message.describe())\n",
    "print(f'90th percentile: {p90}')\n",
    "print(f'95th percentile: {p95}')\n",
    "print(f'99th percentile: {p99}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "As we can see, a majority of duplicated messages between chats only occurs twice in our dataset. \n",
    "\n",
    "Only one percent of the duplicated messages appear more than 7 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: \n",
    "grouped_chats = duplicates_msg_text_btw_chats.groupby(\"telegram_chat_id\")\n",
    "size_chat_duplicates = grouped_chats.size()\n",
    "size_chat_duplicates.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "Our chats share about 624 message text with other chats. However, there's, indicating a skewed distribution with significant outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chats with most duplicated message texts\n",
    "\n",
    "Lastly, we'll identify the chats that share the most messages with other chats and save them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_chats = duplicates_msg_text_btw_chats.groupby(\"telegram_chat_id\")\n",
    "top_10 = grouped_chats.size().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Save the data types of our dataframe as a JSON file\n",
    "top_10_path = os.path.join(os.getcwd(), '../data/auxiliary/top_duplicate_chats.txt')\n",
    "with open(top_10_path, 'w') as f:\n",
    "    f.write(','.join(map(str, top_10.index.to_list())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export cleanded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data cleaned and initial analyses complete, we will now export the dataset to a CSV file. This file will be used for feature engineering and the actual experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save cleaned dataframe as csv\n",
    "file_path = os.path.join(os.getcwd(), '../data/csv/cleaned_data.csv')\n",
    "df_over_1000.to_csv(file_path, index=False)\n",
    "\n",
    "# Save the data types of our dataframe as a JSON file\n",
    "dtypes_dict = df_over_1000.dtypes.apply(lambda x: x.name).to_dict()\n",
    "dtypes_path = os.path.join(os.getcwd(), '../data/auxiliary/cleaned_data_dtypes.json')\n",
    "with open(dtypes_path, 'w') as f:\n",
    "    json.dump(dtypes_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram-chat-clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
