{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # to avoid warning when using a tokenizer to calculate the coherence score during evaluation of a topic model\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from bertopic import BERTopic\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import silhouette_score\n",
    "#from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from bertopic import BERTopic\n",
    "from typing import Optional, Tuple, Dict, Union, List\n",
    "import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# download stopwords and tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Get the parent directory of the current notebook directory and add it to the python path to import custom modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from util.clustering_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), '../data/preprocessed/df_preprocessed.pkl')\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Chat-Text-Aggregations for Topic-Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the interpretability of the topics generated by BERTopic, we need to provide it with the message texts we used to create features for each chat. \n",
    "\n",
    "To make sure that the \"chat-text-aggregations\" used for this purpose are as meaningful as possible, we will perform the following operations: \n",
    "\n",
    "- **Basic preprocessing**, including lowercasing, stop word removal, removal of punctuation and digits, and tokenization.\n",
    "\n",
    "- **Removing custom stopwords** specific to the Telegram domain, such as:\n",
    "\n",
    "    - Telegram chat handles, which are frequently used to \"sign\" each  message in broadcast chats and could distort the analysis of the most common words.\n",
    "\n",
    "    - Common social media call-to-action phrases, such as \"share,\" \"follow,\" and \"comment,\" which are often repeated irrespective of topic.\n",
    "\n",
    "- **Multilingual processing:** Since our corpus is multilingual, language-dependent preprocessing will be applied only to messages in the most frequent languages, as the other languages contribute only a marginal number of messages.\n",
    "- **TF-IDF filtering:** We will filter out words below a certain TF-IDF threshold to ensure that only distinctive terms are included in the aggregation.\n",
    "\n",
    "Afterwards, we will aggregate the messages and webpage previews for each chat into a single string. This string, along with the chat vector representations created earlier, will be passed to BERTopic as a basis for its topic description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Preprocessing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the text data in a specified column of a DataFrame by performing several cleaning operations:\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): the DataFrame containing the text to preprocess.\n",
    "        text_column (str): the name of the column that contains the text to clean.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with additional columns for cleaned (`<text_column>_cleaned`) and tokenized \n",
    "        (`<text_column>_preprocessed`) text.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing messages...\")\n",
    "\n",
    "    # get stop words \n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "    cta_stop_words_en = {'click', 'tap', 'press', 'subscribe', 'follow', 'share', 'like', 'comment',\n",
    "                        'join', 'sign', 'visit', 'download', 'register', 'give', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "    cta_stop_words_de = {'klicken', 'tippen', 'drücken', 'abonnieren', 'folgen', 'teilen', 'mögen', 'kommentieren',\n",
    "                        'beitreten', 'anmelden', 'besuchen', 'herunterladen', 'registrieren', 'geben', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "\n",
    "    stop_words_en = stop_words_en.union(cta_stop_words_en)\n",
    "    stop_words_de = stop_words_de.union(cta_stop_words_de)\n",
    "    print(\"Stop words loaded\")\n",
    "\n",
    "    # get frequent chat handles\n",
    "    frequent_chat_handles = df[\"referenced_chat_handles\"].explode().value_counts()\n",
    "    frequent_chat_handles = frequent_chat_handles[frequent_chat_handles > 100].index.tolist()\n",
    "    print(\"Frequent chat handles loaded\")\n",
    "\n",
    "    # create regex patterns\n",
    "    def create_pattern(words):\n",
    "        return rf'\\b(?:{\"|\".join(map(re.escape, words))})\\b'\n",
    "\n",
    "    frequent_chat_pattern = create_pattern(frequent_chat_handles)\n",
    "    stop_words_en_pattern = create_pattern(stop_words_en)\n",
    "    stop_words_de_pattern = create_pattern(stop_words_de)\n",
    "    print(\"Regex-Patterns created\")\n",
    "    \n",
    "    # remove the most frequent chat handles  #TODO: Seems not to work\n",
    "    df[f\"{text_column}_cleaned\"] = df[text_column].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"Handles removed\")\n",
    "\n",
    "    # remove URLs\n",
    "    pattern = r\"(https?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(www\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(t\\.me\\/[^\\s.,?!)]*)\"\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(pattern, '', regex=True).str.strip()\n",
    "    print(\"URLs removed\")\n",
    "\n",
    "    # lowercase text\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.lower()\n",
    "    print(\"Lowercase\")\n",
    "    \n",
    "    # remove punctuation\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    print(\"Punctuation removed\")\n",
    "\n",
    "    # remove the most frequent chat handles that included an @\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"Handles with @ removed\")\n",
    "\n",
    "    # remove punctuation again\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    print(\"Punctuation removed\")\n",
    "\n",
    "    # remove digits\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(r'\\d+', '', regex=True).str.strip()\n",
    "    print(\"Digits removed\")\n",
    "\n",
    "    # remove english stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"].str.replace(stop_words_en_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"English stop words removed\")\n",
    "\n",
    "    # remove german stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"].str.replace(stop_words_de_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"German stop words removed\")\n",
    "\n",
    "    # fill NaN with empty string\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].fillna('')\n",
    "    \n",
    "    # tokenize text\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_cleaned\"].apply(lambda x: word_tokenize(x) if x else [])\n",
    "\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_preprocessed\"].apply(lambda x: ' '.join(x))\n",
    "    print(\"Tokenized\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_msg_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_msgs_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_msg_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"message_text\")\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"message_text_preprocessed\"].to_pickle(preprocessen_msg_path)\n",
    "    print(\"Preprocessed messages saved\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading preprocessed messages...\")\n",
    "    preprocessed_msg = pd.read_pickle(preprocessen_msg_path)\n",
    "    df[\"message_text_preprocessed\"] = preprocessed_msg\n",
    "\n",
    "# display five random samples\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df[[\"message_text\", \"message_text_preprocessed\", \"referenced_chat_handles\"]].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_web_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_web_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_web_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"webpage_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of emoji-unicodes using data from \"https://unicode.org/Public/emoji/15.1/\"\n",
    "if not already_preprocessed:\n",
    "\n",
    "    # list the paths to the unicode-files\n",
    "    path_1 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-sequences.txt\")\n",
    "    path_2 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-test.txt\")\n",
    "    path_3 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-zwj-sequences.txt\")\n",
    "    file_paths = [path_1, path_2, path_3]\n",
    "\n",
    "    # load all emojis from the unicode-files\n",
    "    emoji_sequences = load_emoji_list(file_paths)\n",
    "\n",
    "    # create a regex pattern from the emoji sequence\n",
    "    emoji_pattern = '|'.join(re.escape(emoji) for emoji in emoji_sequences)\n",
    "    print(\"Emoji pattern created\")\n",
    "\n",
    "    def demojize_chunk(chunk, emoji_pattern):\n",
    "        # remove emojis\n",
    "        chunk[\"webpage_description_preprocessed\"] = chunk[\"webpage_description_preprocessed\"].str.replace(emoji_pattern, \" \", regex=True)\n",
    "        return chunk\n",
    "\n",
    "    n_jobs = 3  # Use three cores (seems to be fastest?)\n",
    "\n",
    "    # remove emojis in parallel for each chunk\n",
    "    chunks = np.array_split(df, n_jobs)\n",
    "    df_chunks = Parallel(n_jobs=n_jobs)(delayed(demojize_chunk)(chunk, emoji_pattern) for chunk in chunks)\n",
    "    df = pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"webpage_description_preprocessed\"].to_pickle(preprocessen_web_path)\n",
    "    print(\"Preprocessed messages saved\")    \n",
    "\n",
    "# simply load the preprocessed data, if it was already preprocessed\n",
    "else:\n",
    "    print(\"Loading preprocessed webpage previews...\")\n",
    "    preprocessed_web_previews = pd.read_pickle(preprocessen_web_path)\n",
    "    df[\"webpage_description_preprocessed\"] = preprocessed_web_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply TF-IDF-Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_msgs_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"message_text_lang\"] == \"English\"][\"message_text_preprocessed\"]\n",
    "    german_texts = df[df[\"message_text_lang\"] == \"German\"][\"message_text_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"message_text_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"message_text_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"message_text_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"message_text_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # TODO: Change column name\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"message_text_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"message_text_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"message_text_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_web_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"webpage_description_lang\"] == \"English\"][\"webpage_description_preprocessed\"]\n",
    "    german_texts = df[df[\"webpage_description_lang\"] == \"German\"][\"webpage_description_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold_web(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"webpage_description_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"webpage_description_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"webpage_description_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"webpage_description_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"webpage_description_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold_web(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"webpage_description_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf-filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"webpage_description_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create Chat-Text-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_texts = chat_texts.astype(str)\n",
    "chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create Webpage-Preview-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_webpage_previews = grouped[\"webpage_description_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_webpage_previews = chat_webpage_previews.astype(str)\n",
    "chat_webpage_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(os.getcwd(), '../features/0_base_chat_vectors.npy')\n",
    "base_chat_vectors = np.load(base_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {base_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {base_chat_vectors.iloc[0].shape}\")\n",
    "base_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Functions used to run the experiment**\n",
    "\n",
    "use KeyBERTInspired\n",
    "\n",
    "Define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_bertTopic(chat_embeddings: pd.Series, chat_texts: pd.Series, used_embedding_model: SentenceTransformer, hdbscan_model=None, verbose=True): \n",
    "\n",
    "#     # prepare the embeddings for dimensionality reduction by stacking them\n",
    "#     chat_embeddings = np.vstack(chat_embeddings)\n",
    "\n",
    "#     if verbose:\n",
    "#         print_log(\"apply_bertTopic\", \"Preparation\", \"Completed ✓\")\n",
    "\n",
    "#     # create your representation model\n",
    "#     representation_model = KeyBERTInspired() #TODO: Configure?\n",
    "\n",
    "#     #TODO: Random state?\n",
    "\n",
    "#     # initiate the BERTopic model\n",
    "#     docs = chat_texts.tolist()\n",
    "        \n",
    "#     #cluster_model = KMeans(n_clusters=14) #9 #15->gut\n",
    "\n",
    "#     if hdbscan_model is None:\n",
    "#         topic_model = BERTopic(embedding_model=used_embedding_model, \n",
    "#                             verbose=verbose, \n",
    "#                             calculate_probabilities=True, \n",
    "#                             representation_model=representation_model,\n",
    "#                             )#hdbscan_model=cluster_model \n",
    "#     else:\n",
    "#         topic_model = BERTopic(embedding_model=used_embedding_model, \n",
    "#                             verbose=verbose, \n",
    "#                             calculate_probabilities=True, \n",
    "#                             representation_model=representation_model,\n",
    "#                             hdbscan_model=hdbscan_model)\n",
    "\n",
    "#     if verbose:\n",
    "#         print_log(\"apply_bertTopic\", \"Loading Model\", \"Completed ✓\")\n",
    "        \n",
    "#     # fit the model to the reduced embeddings\n",
    "#     topics, propabilities = topic_model.fit_transform(embeddings = chat_embeddings, documents = docs)\n",
    "#     if verbose:\n",
    "#         print_log(\"apply_bertTopic\", \"Model Fitting\", \"Completed ✓\")\n",
    "\n",
    "#     return topics, propabilities, topic_model\n",
    "\n",
    "\n",
    "\n",
    "# def get_evaluations(chat_embeddings: pd.Series, propabilities: np.ndarray, topic_model: BERTopic, text_aggregations: pd.Series) -> Union[float, float, float, int, int]:\n",
    "#     \"\"\"\n",
    "#     Get several evaluation metrics for a given topic model.\n",
    "#     Parameters:\n",
    "#         chat_embeddings (pd.Series): The chat embeddings.\n",
    "#         propabilities (pd.Series): The propabilities.\n",
    "#         topic_model (BERTopic): The topic model.\n",
    "#         text_aggregations (pd.Series): The text aggregations.\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing the coherence score, silhouette score, davies bouldin score, topic count and noise count.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # get the document info and topics\n",
    "#     document_info = topic_model.get_document_info(text_aggregations)\n",
    "#     topics = document_info['Topic']\n",
    "    \n",
    "#     # prepare the embeddings by stacking them\n",
    "#     chat_embeddings = np.vstack(chat_embeddings)    \n",
    "\n",
    "#     # get the silhouette score while ignoring the \"Other\" topic\n",
    "#     valid_indices_filter = topics != -1\n",
    "#     filtered_embeddings = chat_embeddings[valid_indices_filter]\n",
    "#     filtered_topics = topics[valid_indices_filter]\n",
    "#     silhouette_score_result = silhouette_score(X=filtered_embeddings, labels=filtered_topics)\n",
    "#     #print(f'Silhouette Score: {silhouette_score_result}')\n",
    "    \n",
    "#     # get davies bouldin score\n",
    "#     davies_bouldin_score_result = davies_bouldin_score(X=filtered_embeddings, labels=filtered_topics)\n",
    "#     #print(f'Davies-Bouldin Score: {davies_bouldin_score_result}')\n",
    "    \n",
    "#     # calculate the number of topics found\n",
    "#     topic_count = len(np.unique(topics))\n",
    "#     #print(f'Topic Count: {topic_count}')\n",
    "    \n",
    "#     # calculate the number of noise points\n",
    "#     noise_count = len(topics[topics == -1])\n",
    "#     #print(f'Noise Count: {noise_count}')\n",
    "    \n",
    "#     # calculate the coherence score\n",
    "#     # preprocess documents is not necessary, as we already preprocessed the text aggregations\n",
    "    \n",
    "#     # get the vectorizer\n",
    "#     vectorizer = topic_model.vectorizer_model\n",
    "#     tokenizer = vectorizer.build_tokenizer()\n",
    "    \n",
    "#     # extract features for coherence evaluation\n",
    "#     #words = vectorizer.get_feature_names()\n",
    "#     tokens = [tokenizer(doc) for doc in text_aggregations] \n",
    "#     dictionary = corpora.Dictionary(tokens)\n",
    "#     corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "#     topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "#                 for topic in range(len(set(topics))-1)]\n",
    "\n",
    "#     # calculate the coherence score\n",
    "#     coherence_model = CoherenceModel(topics=topic_words, \n",
    "#                                     texts=tokens, \n",
    "#                                     corpus=corpus,\n",
    "#                                     dictionary=dictionary, \n",
    "#                                     coherence='c_v') \n",
    "#     coherence_score_result = coherence_model.get_coherence()    \n",
    "    \n",
    "#     #print(f'Coherence Score: {coherence_score_result}')\n",
    "\n",
    "#     return coherence_score_result, silhouette_score_result, davies_bouldin_score_result, topic_count, noise_count\n",
    "\n",
    "\n",
    "\n",
    "# def run_experiment(\n",
    "#     chat_embeddings: pd.Series, \n",
    "#     chat_texts: pd.Series, \n",
    "#     n: int, \n",
    "#     topic_model_dir_path: str,\n",
    "#     feature_name: str,\n",
    "#     used_embedding_model: SentenceTransformer,\n",
    "#     hdbscan_model: Optional[object] = None\n",
    "# ) -> Tuple[Dict[str, float], List[int], List[float], BERTopic]:\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Run the BERTopic model multiple times, calculate average evaluation metrics and return and save the \"most average\" model for manual inspection.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - chat_embeddings (pd.Series): The chat embeddings.\n",
    "#     - chat_texts (pd.Series): The aggregated chat texts.\n",
    "#     - n (int): Number of times to run the model.\n",
    "#     - topic_model_dir_path (str): Directory path to save topic models.\n",
    "#     - hdbscan_model (optional): A pre-defined HDBSCAN model to use. If None, a default model will be created.\n",
    "\n",
    "#     Returns:\n",
    "#     - avg_evaluation_metrics (dict): A dictionary containing average evaluation metrics.\n",
    "#     - selected_topics (List[int]): The topics assigned by the most average model.\n",
    "#     - selected_probabilities (List[float]): The probabilities for each document's topic assignment by the most average model.\n",
    "#     - avg_model (BERTopic): The BERTopic model that is closest to the average metrics.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     coherence_scores = []\n",
    "#     silhouette_scores = []\n",
    "#     davies_bouldin_scores = []\n",
    "#     topic_counts = []\n",
    "#     noise_counts = []\n",
    "    \n",
    "#     # save topics and propabilities of each model to return them for the most average model later on\n",
    "#     topics_list = []\n",
    "#     propabilities_list = []\n",
    "\n",
    "#     for i in range(n):\n",
    "        \n",
    "#         print(f\"\\n#### Running Model {i+1}/{n}... ####\")\n",
    "        \n",
    "#         print_log(\"run_experiment\", \"Fitting Model\", \"Fitting Model\")\n",
    "#         topics, propabilities, topic_model = apply_bertTopic(chat_embeddings, chat_texts, used_embedding_model, hdbscan_model) #Remove hdbscan_model? #TODO: propabilities\n",
    "#         topics_list.append(topics)\n",
    "#         propabilities_list.append(propabilities)\n",
    "#         print_log(\"run_experiment\", \"Fitting Model\", \"Completed ✓\")\n",
    "\n",
    "#         print_log(\"run_experiment\", \"Evaluating Model\", \"Calculating Evaluation Metrics\")\n",
    "#         # calculate evaluation metrics\n",
    "#         (cs_base_embeddings,\n",
    "#         ss_base_embeddings,\n",
    "#         db_base_embeddings,\n",
    "#         topic_count_base_embeddings, \n",
    "#         noise_base_embeddings) = get_evaluations(chat_embeddings, \n",
    "#                                                 propabilities, \n",
    "#                                                 topic_model, \n",
    "#                                                 chat_texts)\n",
    "        \n",
    "#         # append the evaluation metrics\n",
    "#         coherence_scores.append(cs_base_embeddings)\n",
    "#         silhouette_scores.append(ss_base_embeddings)\n",
    "#         davies_bouldin_scores.append(db_base_embeddings)\n",
    "#         topic_counts.append(topic_count_base_embeddings)\n",
    "#         noise_counts.append(noise_base_embeddings)\n",
    "#         print_log(\"run_experiment\", \"Evaluating Model\", \"Completed ✓\")\n",
    "\n",
    "#         print_log(\"run_experiment\", \"Saving Topic Model\", \"Saving Model\")\n",
    "#         # save model\n",
    "#         topic_model_path =  os.path.join(topic_model_dir_path, f\"{feature_name}_topic_model_{i}\")\n",
    "#         os.makedirs(topic_model_dir_path, exist_ok=True)\n",
    "#         topic_model.save(topic_model_path)\n",
    "#         print_log(\"run_experiment\", \"Saving Topic Model\", \"Completed ✓\")\n",
    "\n",
    "#     print(f\"\\n#### Calculating Averages ####\")\n",
    "#     print_log(\"run_experiment\", \"Calculate Average Evaluation Metrics\", \"Calculating\")    \n",
    "#     # calculate average evaluation metrics\n",
    "#     avg_coherence_scores = np.mean(coherence_scores)\n",
    "#     avg_silhouette_scores = np.mean(silhouette_scores)\n",
    "#     avg_davies_bouldin_scores = np.mean(davies_bouldin_scores)\n",
    "#     avg_topic_counts = np.mean(topic_counts)\n",
    "#     avg_noise_counts = np.mean(noise_counts)\n",
    "    \n",
    "#     # create dictionary for the evaluation metrics\n",
    "#     avg_evaluation_metrics = {\n",
    "#         \"avg_coherence_scores\": avg_coherence_scores,\n",
    "#         \"avg_silhouette_scores\": avg_silhouette_scores,\n",
    "#         \"avg_davies_bouldin_scores\": avg_davies_bouldin_scores,\n",
    "#         \"avg_topic_counts\": avg_topic_counts,\n",
    "#         \"avg_noise_counts\": avg_noise_counts\n",
    "#     }\n",
    "#     print_log(\"run_experiment\", \"Calculate Average Evaluation Metrics\", \"Completed ✓\")\n",
    "    \n",
    "#     print_log(\"run_experiment\", \"Found Most Average Model\", \"Calculating\")\n",
    "    \n",
    "#     # find the model with evaluation results closests to the average evaluation metrics\n",
    "#     # convert evaluation metrics to a matrix for easier comparison with the average value vector\n",
    "#     evaluation_matrix = np.stack([coherence_scores, silhouette_scores, davies_bouldin_scores, topic_counts, noise_counts])\n",
    "#     average_value_vector = np.array([avg_coherence_scores, avg_silhouette_scores, avg_davies_bouldin_scores, avg_topic_counts, avg_noise_counts])\n",
    "#     average_value_vector = average_value_vector[:, np.newaxis]\n",
    "\n",
    "#     # find the model with the smallest difference to the average evaluation metrics\n",
    "#     differences = np.abs(evaluation_matrix - average_value_vector)\n",
    "#     smallest_diff_idx = np.argmin(np.sum(differences, axis=0))\n",
    "    \n",
    "#     # load the model with the smallest difference to the average evaluation metrics\n",
    "#     average_model_path =  os.path.join(topic_model_dir_path, f\"{feature_name}_topic_model_{smallest_diff_idx}\")\n",
    "#     avg_model = BERTopic.load(average_model_path)\n",
    "#     print_log(\"run_experiment\", \"Found Most Average Model\", \"Completed ✓\")\n",
    "    \n",
    "#     # delete all other models\n",
    "#     print_log(\"run_experiment\", \"Deleted Remaining Models\", \"Deleting\")\n",
    "#     for i in range(n):\n",
    "#         if i != smallest_diff_idx:\n",
    "#             os.remove(os.path.join(topic_model_dir_path, f\"{feature_name}_topic_model_{i}\"))\n",
    "            \n",
    "#     # rename the model \n",
    "#     os.rename(average_model_path, os.path.join(topic_model_dir_path, f\"avg_{feature_name}_topic_model\"))\n",
    "#     print_log(\"run_experiment\", \"Deleted Remaining Models\", \"Completed ✓\")\n",
    "        \n",
    "#     return avg_evaluation_metrics, topics[smallest_diff_idx], propabilities[smallest_diff_idx], avg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "    \n",
    "# set the parameters for the HDBSCAN model\n",
    "alpha = 1.0\n",
    "min_cluster_size = 7\n",
    "min_samples = 5\n",
    "# Running Parameters: {'alpha': 1.0, 'hdbscan_model__min_cluster_size': 7, 'hdbscan_model__min_samples': 5}\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        alpha=alpha,\n",
    "                        prediction_data=True)\n",
    "\n",
    "# run the experiment\n",
    "base_evaluation_metrics, topics, probabilities, topic_model = run_experiment(\n",
    "    chat_embeddings=base_chat_vectors, \n",
    "    chat_texts=chat_texts, \n",
    "    n=50, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/base_embeddings/topic_models/\"),\n",
    "    feature_name=\"base\",\n",
    "    used_embedding_model=transformer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in base_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and explore the results of the  most average topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_topic_visualisations(topic_model, embeddings, texts_aggregations):\n",
    "#     # Visualize topics\n",
    "#     #topic_model.visualize_topics().show()  \n",
    "\n",
    "#     print(\"Topic Map:\")\n",
    "#     # UMAP dimensionality reduction\n",
    "#     from umap import UMAP\n",
    "#     import numpy as np\n",
    "#     docs = texts_aggregations.tolist()\n",
    "#     embeddings = np.vstack(embeddings)\n",
    "#     reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "    \n",
    "#     # Visualize documents using UMAP embeddings\n",
    "#     topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings).show()\n",
    "\n",
    "#     print(\"Bar Chart, displaying the top 13 topics and top 20 words per topic:\")\n",
    "#     # Visualize bar chart for top 13 topics and 20 words per topic\n",
    "#     topic_model.visualize_barchart(top_n_topics=13, n_words=20).show()\n",
    "\n",
    "#     print(\"Hierarchical Topics:\")\n",
    "#     # Visualize hierarchical topics\n",
    "#     hierarchical_topics = topic_model.hierarchical_topics(texts_aggregations)\n",
    "#     topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics).show()\n",
    "\n",
    "create_topic_visualisations(topic_model, base_chat_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Inspect the most representative messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = topic_model.get_topic_info()[\"Topic\"]\n",
    "topics = topics.values[topics.values != -1]\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create!!!\n",
    "# def create_topic_vectors(topic_model: BERTopic, chat_vectors: pd.Series) -> pd.Series:\n",
    "#     \"\"\" Create a topic vector for each topic by averaging the chat vectors assigned to the topic.\n",
    "\n",
    "#     Parameters:\n",
    "#         topic_model (BERTopic): the topic model\n",
    "#         chat_vectors (pd.Series): the chat vectors used to create the topic vectors\n",
    "\n",
    "#     Returns:\n",
    "#         pd.Series: _description_\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # create a map with chat ids and their corresponding topics\n",
    "#     topics = topic_model.topics_\n",
    "#     chat_ids= chat_vectors.index\n",
    "#     id_topic_map = pd.Series(topics, index=chat_ids, name='Topic')\n",
    "\n",
    "#     # calculate the mean vector of the chat vectors assigned to each topic\n",
    "#     topic_vectors = {}\n",
    "    \n",
    "#     # iterate over all topics (excluding the \"Other\" topic)\n",
    "#     topics = topic_model.get_topic_info()[\"Topic\"]\n",
    "#     topics = topics.values[topics.values != -1]\n",
    "#     for topic in topics:\n",
    "#         topic_chat_vectors = chat_vectors[id_topic_map == topic]\n",
    "#         topic_vector = np.mean(np.vstack(topic_chat_vectors), axis=0)\n",
    "#         topic_vectors[topic] = topic_vector\n",
    "        \n",
    "#     topic_vectors = pd.Series(topic_vectors, name=\"Mean_Vector\")\n",
    "    \n",
    "#     return topic_vectors\n",
    "\n",
    "topic_vectors = create_topic_vectors(topic_model, base_chat_vectors)\n",
    "topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top n messages closest to the topic vector they were assigned to**\n",
    "\n",
    "First, we add the message-text-embeddings created in `02_feature_engineering` back to the messages in the DataFrame. \n",
    "We will then use them to compare the messages to the topic vectors created earlier to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the message embeddings\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# create a series where each element is an message-vector\n",
    "message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "# check if the message embeddings have the same shape as the dataframe\n",
    "assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "message_embeddings_series\n",
    "\n",
    "# add the message embeddings to the dataframe\n",
    "df[\"message_vector\"] = message_embeddings_series\n",
    "df[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_messages = get_representative_texts(df = df,\n",
    "                                                   topic_model = topic_model,\n",
    "                                                   topic_vectors = topic_vectors,\n",
    "                                                   chat_vectors = base_chat_vectors,\n",
    "                                                   n = 10,\n",
    "                                                   feature_name = \"base\",\n",
    "                                                   text_column = \"message_text\",\n",
    "                                                   text_embeddings_column = \"message_vector\",\n",
    "                                                   text_preprocessed_column = \"message_text_preprocessed\")\n",
    "\n",
    "# save representative messages\n",
    "import json\n",
    "representative_messages_path = os.path.join(os.getcwd(), '../results/base_embeddings/representative_messages.json')\n",
    "representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "with open(representative_messages_path, 'w') as jsonfile:\n",
    "    json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtered Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the filtered Chat Embeddings\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_path = os.path.join(os.getcwd(), '../features/1_filtered_chat_vectors.npy')\n",
    "filtered_chat_vectors = np.load(filtered_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {filtered_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {filtered_chat_vectors.iloc[0].shape}\")\n",
    "filtered_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create filtered Chat-Message-Text-Aggregations\n",
    "\n",
    "Now we filter the dataset to remove all Forwarded/Original-Message-Pairs using the indices we saved in  `02_feature_engineering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_path = os.path.join(os.getcwd(), \"../features/1_implicit_ref_filtered_indices.npy\")\n",
    "filtered_rows_indices = np.load(indices_path)\n",
    "df_references_filtered = df.loc[filtered_rows_indices]\n",
    "df_references_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset already contains the preprocessed Message-Text, we simply need to aggregate the message texts in order to create Chat-Message-Text-Aggregation that exclude Original/Forward-Pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_references_filtered.groupby(\"telegram_chat_id\")\n",
    "filtered_chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "filtered_chat_texts = filtered_chat_texts.astype(str)\n",
    "filtered_chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings. Again, we will run the model multiple times and inspect the average results of the topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to reuse it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "    \n",
    "# set the parameters for the HDBSCAN model\n",
    "alpha = 1.0\n",
    "min_cluster_size = 7\n",
    "min_samples = 5\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        alpha=alpha,\n",
    "                        prediction_data=True)\n",
    "\n",
    "# run the experiment\n",
    "filtered_evaluation_metrics, topics, probabilities, topic_model = run_experiment(\n",
    "    chat_embeddings=filtered_chat_vectors, \n",
    "    chat_texts=filtered_chat_texts, \n",
    "    n=50, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/filtered_embeddings/topic_models/\"),\n",
    "    feature_name=\"filtered\",\n",
    "    used_embedding_model=transformer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in filtered_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, filtered_chat_vectors, filtered_chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Inspect the most representative messages\n",
    "\n",
    "Again, we'll inspect the most representative messages for each topic by comparing them to topic vectors derived from the topic assignments returned by the topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = create_topic_vectors(topic_model, filtered_chat_vectors)\n",
    "topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top n messages closest to the topic vector they were assigned to**\n",
    "\n",
    "Again, we add the embeddings created in `02_feature_engineering` back to the messages in the DataFrame and use them to compare them to the topic vectors created earlier to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the df to only include the messages that were used in the filtered chat vectors. We can use the indices saved in the feature engineering notebook\n",
    "filtered_rows_indices = np.load(indices_path)\n",
    "df_filtered = df.loc[filtered_rows_indices]\n",
    "\n",
    "# load the message embeddings\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# create a series where each element is a message-vector\n",
    "message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "# check if the message embeddings have the same shape as the dataframe\n",
    "assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "message_embeddings_series\n",
    "\n",
    "# add the message embeddings to the dataframe\n",
    "df_filtered[\"message_vector\"] = message_embeddings_series\n",
    "df_filtered[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_messages = get_representative_texts(df = df_filtered,\n",
    "                                                   topic_model = topic_model,\n",
    "                                                   topic_vectors = topic_vectors,\n",
    "                                                   chat_vectors = filtered_chat_vectors,\n",
    "                                                   n = 10,\n",
    "                                                   feature_name = \"filtered\",\n",
    "                                                   text_column = \"message_text\",\n",
    "                                                   text_embeddings_column = \"message_vector\",\n",
    "                                                   text_preprocessed_column = \"message_text_preprocessed\")\n",
    "\n",
    "# save representative messages\n",
    "import json\n",
    "representative_messages_path = os.path.join(os.getcwd(), '../results/filtered_embeddings/representative_messages.json')\n",
    "representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "with open(representative_messages_path, 'w') as jsonfile:\n",
    "    json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Webpage Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created based on webpage previews in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpreview_path = os.path.join(os.getcwd(), '../features/3_webpreview_chat_vectors.npy')\n",
    "webpreview_chat_vectors = np.load(webpreview_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {webpreview_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {webpreview_chat_vectors.iloc[0].shape}\")\n",
    "webpreview_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "    \n",
    "# set the parameters for the HDBSCAN model\n",
    "alpha = 1.0\n",
    "min_cluster_size = 7\n",
    "min_samples = 5\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        alpha=alpha,\n",
    "                        prediction_data=True)\n",
    "\n",
    "# run the experiment\n",
    "webpreview_evaluation_metrics, topics, probabilities, topic_model = run_experiment(\n",
    "    chat_embeddings=webpreview_chat_vectors, \n",
    "    chat_texts=chat_webpage_previews, \n",
    "    n=50, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/webpreview_embeddings/topic_models/\"),\n",
    "    feature_name=\"webpreview\",\n",
    "    used_embedding_model=transformer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in webpreview_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualize and explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, webpreview_chat_vectors, chat_webpage_previews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Inspect the most representative messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = create_topic_vectors(topic_model, webpreview_chat_vectors)\n",
    "topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top n messages closest to the topic vector they were assigned to**\n",
    "\n",
    "Again, we add the embeddings created in `02_feature_engineering` back to the messages in the DataFrame and use them to compare them to the topic vectors created earlier to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the message embeddings\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# create a series where each element is an message-vector\n",
    "message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "# check if the message embeddings have the same shape as the dataframe\n",
    "assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "message_embeddings_series\n",
    "\n",
    "# add the message embeddings to the dataframe\n",
    "df[\"message_vector\"] = message_embeddings_series\n",
    "df[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_messages = get_representative_texts(df,\n",
    "                                                   topic_model,\n",
    "                                                   topic_vectors,\n",
    "                                                   webpreview_chat_vectors,\n",
    "                                                   10,\n",
    "                                                   \"webpreview\",\n",
    "                                                   \"message_text\",\n",
    "                                                   \"message_vector\",\n",
    "                                                   \"message_text_preprocessed\")\n",
    "\n",
    "# save representative messages\n",
    "import json\n",
    "representative_messages_path = os.path.join(os.getcwd(), '../results/webpreview_embeddings/representative_messages.json')\n",
    "representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "with open(representative_messages_path, 'w') as jsonfile:\n",
    "    json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the most representative Webpage-Previews\n",
    "\n",
    "Now, we will inspect the most representative Webpage-Previews. To do so, we'll again compare the Webpage-Preview-Embeddings to the Topic-Vectors created earlier and inspect the most similar ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webpage-preview embeddings\n",
    "webpreview_embeddings_path = os.path.join(os.getcwd(), '../features/3_webpage_embeddings.npy')\n",
    "\n",
    "# create a series where each element is an message-vector\n",
    "webpreview_embeddings = np.load(webpreview_embeddings_path)\n",
    "\n",
    "# add the message embeddings to the dataframe.\n",
    "df[\"webpreview_vector\"] = webpreview_embeddings.tolist() \n",
    "df[[\"webpage_title\", \"webpage_description\", \"webpreview_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_webpreviews = get_representative_texts(df, \n",
    "                                                      topic_model, \n",
    "                                                      topic_vectors, \n",
    "                                                      webpreview_chat_vectors, \n",
    "                                                      10, \n",
    "                                                      \"webpreview\", \n",
    "                                                      \"webpage_description\",\n",
    "                                                      \"webpreview_vector\",\n",
    "                                                      \"webpage_description_preprocessed\")\n",
    "\n",
    "# save representative messages\n",
    "import json\n",
    "representative_webpreview_path = os.path.join(os.getcwd(), '../results/webpreview_embeddings/representative_webpreviews.json')\n",
    "representative_webpreviews = {int(topic): messages for topic, messages in representative_webpreviews.items()} # convert keys to int\n",
    "with open(representative_webpreview_path, 'w') as jsonfile:\n",
    "    json.dump(representative_webpreviews, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, text in representative_webpreviews.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, text in enumerate(text):\n",
    "        print(f\"{i+1}. {text.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined Message & Webpage-Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the two kinds of text embeddings and inspect the changes in clustering results.\n",
    "\n",
    "#### 1. Combine Message-Text- and Webpage-Preview-Vectors\n",
    "\n",
    "First, we load the chat-vectors we created by combining the two webpage-preview- and message-vectors by taking their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_vectors_path = os.path.join(os.getcwd(), '../features/3_msg_webpreview_chat_vectors.npy')\n",
    "combined_vectors = np.load(combine_vectors_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create combined Chat-Text-Aggregations\n",
    "\n",
    "Next, we aggregate the Text-Aggregations for Webpage-Previews and Chat-Messages in order to use them  to make the topics interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame to combine the texts\n",
    "combined_text_dataframe = pd.DataFrame({\n",
    "    \"chat_texts\": chat_texts,\n",
    "    \"chat_webpage_previews\": chat_webpage_previews\n",
    "})\n",
    "\n",
    "# combine the texts\n",
    "combined_text_dataframe[\"combined_texts\"] = combined_text_dataframe[\"chat_texts\"] + \" \" + combined_text_dataframe[\"chat_webpage_previews\"]\n",
    "\n",
    "# calculate the length of the texts\n",
    "combined_text_dataframe[\"chat_texts_len\"] = combined_text_dataframe[\"chat_texts\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"chat_webpage_previews_len\"] = combined_text_dataframe[\"chat_webpage_previews\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"combined_texts_len\"] = combined_text_dataframe[\"combined_texts\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# check if the combined arrays are the same length as the original arrays combined\n",
    "assert combined_text_dataframe[\"combined_texts_len\"].equals(combined_text_dataframe[\"chat_texts_len\"] + combined_text_dataframe[\"chat_webpage_previews_len\"])\n",
    "\n",
    "# get the combined texts\n",
    "combined_texts = combined_text_dataframe[\"combined_texts\"]\n",
    "combined_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the Combined Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "    \n",
    "# set the parameters for the HDBSCAN model\n",
    "alpha = 1.0\n",
    "min_cluster_size = 7\n",
    "min_samples = 5\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        alpha=alpha,\n",
    "                        prediction_data=True)\n",
    "\n",
    "# run the experiment\n",
    "combined_webpreview_evaluation_metrics, topics, probabilities, topic_model = run_experiment(\n",
    "    chat_embeddings=combined_vectors, \n",
    "    chat_texts=combined_texts, \n",
    "    n=50, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/combined_webpreview_embeddings/topic_models/\"),\n",
    "    feature_name=\"combined_webpreview\",\n",
    "    used_embedding_model=transformer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in combined_webpreview_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Visualise and explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, combined_vectors, combined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inspect the most representative messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = create_topic_vectors(topic_model, combined_vectors)\n",
    "topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top n messages closest to the topic vector they were assigned to**\n",
    "\n",
    "Like with the features used before, we add the embeddings created in `02_feature_engineering` back to the messages in the DataFrame and use them to compare them to the topic vectors created earlier to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Combine message and webpage embeddings ??? (maybe don't?)\n",
    "\n",
    "# load the message embeddings\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# create a series where each element is an message-vector\n",
    "message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "# check if the message embeddings have the same shape as the dataframe\n",
    "assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "message_embeddings_series\n",
    "\n",
    "# add the message embeddings to the dataframe\n",
    "df[\"message_vector\"] = message_embeddings_series\n",
    "df[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_messages = get_representative_texts(df = df,\n",
    "                                                   topic_model = topic_model,\n",
    "                                                   topic_vectors = topic_vectors,\n",
    "                                                   chat_vectors = combined_vectors,\n",
    "                                                   n = 10,\n",
    "                                                   feature_name = \"combined_webpreview\",\n",
    "                                                   text_column = \"message_text\",\n",
    "                                                   text_embeddings_column = \"message_vector\",\n",
    "                                                   text_preprocessed_column = \"message_text_preprocessed\")\n",
    "\n",
    "# save representative messages\n",
    "import json\n",
    "representative_messages_path = os.path.join(os.getcwd(), '../results/combined_webpreview_embeddings/representative_messages.json')\n",
    "representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "with open(representative_messages_path, 'w') as jsonfile:\n",
    "    json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Inspect the most representative Webpage-Previews\n",
    "\n",
    "Again, we will inspect the most representative Webpage-Previews using the same approach as before. We will again reuse the topic vectors created while inspecting the most representative messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webpage-preview embeddings\n",
    "webpreview_embeddings_path = os.path.join(os.getcwd(), '../features/3_webpage_embeddings.npy')\n",
    "\n",
    "# create a series where each element is an message-vector\n",
    "webpreview_embeddings = np.load(webpreview_embeddings_path)\n",
    "\n",
    "# add the message embeddings to the dataframe.\n",
    "df[\"webpreview_vector\"] = webpreview_embeddings.tolist() \n",
    "df[[\"webpage_title\", \"webpage_description\", \"webpreview_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_webpreviews = get_representative_texts(df, \n",
    "                                                      topic_model, \n",
    "                                                      topic_vectors, \n",
    "                                                      combined_vectors, \n",
    "                                                      10, \n",
    "                                                      \"combined_webpreview\", \n",
    "                                                      \"webpage_description\",\n",
    "                                                      \"webpreview_vector\",\n",
    "                                                      \"webpage_description_preprocessed\")\n",
    "\n",
    "# save representative webpreviews\n",
    "import json\n",
    "representative_webpreview_path = os.path.join(os.getcwd(), '../results/combined_webpreview_embeddings/representative_webpreviews.json')\n",
    "representative_webpreviews = {int(topic): messages for topic, messages in representative_webpreviews.items()} # convert keys to int\n",
    "with open(representative_webpreview_path, 'w') as jsonfile:\n",
    "    json.dump(representative_webpreviews, jsonfile, indent=4)\n",
    "\n",
    "# print representative webpreviews\n",
    "for topic, text in representative_webpreviews.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, text in enumerate(text):\n",
    "        print(f\"{i+1}. {text.strip()}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "#TODO: What about titles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structural Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we conduct chat-clustering using a chats structural attributes. Structural attributes are a chats connections to other telegram entities. \n",
    "\n",
    "For our purposes, we have considered two kinds of connections:\n",
    "\n",
    "1. Forwarded (fwd) messages between chats.\n",
    "\n",
    "2. Textual references (ref) to chats or other telegram-entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Chat-Vectors\n",
    "\n",
    "To vectorize these connections, we created chat-chat-matrices based on forwards and text based references between chats in `02_feature_engineering`, which we'll load now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "fwd_log_path = os.path.join(os.getcwd(), '../features/2_log_fwd_vectors.pkl')\n",
    "fwd_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_fwd_vectors.pkl')\n",
    "ref_log_path = os.path.join(os.getcwd(), '../features/2_log_ref_vectors.pkl')\n",
    "ref_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_ref_vectors.pkl')\n",
    "\n",
    "# load the chat vectors\n",
    "fwd_log_vectors = pd.read_pickle(fwd_log_path)\n",
    "fwd_onehot_vectors = pd.read_pickle(fwd_onehot_path)\n",
    "ref_log_vectors = pd.read_pickle(ref_log_path)\n",
    "ref_onehot_vectors = pd.read_pickle(ref_onehot_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the forward-based and the reference-based chat-vectors to create our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the vectors\n",
    "structure_log_vectors = fwd_log_vectors.combine(ref_log_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "structure_onehot_vectors = fwd_onehot_vectors.combine(ref_onehot_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the original vectors\n",
    "dimension_fwd_vectors = len(fwd_log_vectors.iloc[1])\n",
    "dimension_ref_vectors = len(ref_log_vectors.iloc[1])\n",
    "assert len(structure_log_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors\n",
    "assert len(structure_onehot_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the Structural Vectors\n",
    "\n",
    "Eventhough they are no Text-Embeddings we will pass the structural vectors to BERTopic for clustering. This is possible, as BERTopic can accept any kind of numerical custom embeddings instead of generating them from text.\n",
    "\n",
    "We will again use the filtered Message-Text-Aggregations to make the topics interpretable. These documents will only be used for topic labeling and interpretation. The clustering itself will be entirely driven by the chat-chat-matrices we pass as embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _, topic_model = apply_bertTopic(structure_log_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualise and Explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, structure_log_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ss_structural_embeddings, \n",
    "topic_count_structural_embeddings, \n",
    "noise_structural_embeddings) = get_evaluations(structure_log_vectors,\n",
    "                                               propabilities,\n",
    "                                               topic_model, \n",
    "                                               chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combined Structural Vectors & Message Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we conduct chat-clustering using a chats structural attributes. Structural attributes are a chats connections to other telegram entities. \n",
    "\n",
    "For our purposes, we have considered two kinds of connections:\n",
    "\n",
    "1. Forwarded (fwd) messages between chats.\n",
    "\n",
    "2. Textual references (ref) to chats or other telegram-entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Chat-Vectors\n",
    "\n",
    "To vectorize these connections, we created chat-chat-matrices based on forwards and text based references between chats in `02_feature_engineering`, which we'll load now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "fwd_log_path = os.path.join(os.getcwd(), '../features/2_log_fwd_vectors.pkl')\n",
    "fwd_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_fwd_vectors.pkl')\n",
    "ref_log_path = os.path.join(os.getcwd(), '../features/2_log_ref_vectors.pkl')\n",
    "ref_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_ref_vectors.pkl')\n",
    "\n",
    "# load the chat vectors\n",
    "fwd_log_vectors = pd.read_pickle(fwd_log_path)\n",
    "fwd_onehot_vectors = pd.read_pickle(fwd_onehot_path)\n",
    "ref_log_vectors = pd.read_pickle(ref_log_path)\n",
    "ref_onehot_vectors = pd.read_pickle(ref_onehot_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the forward-based and the reference-based chat-vectors to create our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the vectors\n",
    "structure_log_vectors = fwd_log_vectors.combine(ref_log_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "structure_onehot_vectors = fwd_onehot_vectors.combine(ref_onehot_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the original vectors\n",
    "dimension_fwd_vectors = len(fwd_log_vectors.iloc[1])\n",
    "dimension_ref_vectors = len(ref_log_vectors.iloc[1])\n",
    "assert len(structure_log_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors\n",
    "assert len(structure_onehot_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Concenate the Features\n",
    "\n",
    "To supplement the text embeddings with structural information while still using BERTopic for clustering, we'll concatenate the new feature vectors and the chat embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the message embedding based chat vectors with the log-scaled structure-based chat vectors\n",
    "combined_vectors = base_chat_vectors.combine(structure_log_vectors, lambda x, y: np.concatenate([x, y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the message text embeddings and the structure-based vectors\n",
    "assert len(combined_vectors.iloc[1]) == len(base_chat_vectors.iloc[1]) + len(structure_log_vectors.iloc[1])\n",
    "\n",
    "combined_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the Combined Chat Vectors\n",
    "\n",
    "Now, we can cluster the resulting combined vectors using BERTopic. As we're only use the Messages as textual feature, we will reuse the filtered Chat-Message-Text-Aggregations we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _, topic_model = apply_bertTopic(combined_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, combined_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ss_msg_structural_embeddings, \n",
    " topic_count_msg_structural_embeddings,\n",
    " noise_structural_embeddings) = get_evaluations(combined_vectors,\n",
    "                                                propabilities, \n",
    "                                                topic_model, \n",
    "                                                chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the different approaches based on the evaluation data we collected for each feature and feature combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame({\n",
    "    \"Features\": [\"Msg\", \"Filtered\", \"Webpreview\", \"Msg + Webpreview\", \"Structural\", \"Msg + Structural\"],\n",
    "    \"Silhouette Score\": [ss_base_embeddings, ss_filtered_embeddings, ss_webpreview_embeddings, ss_msg_webpreview_embeddings, ss_structural_embeddings, ss_msg_structural_embeddings],\n",
    "    \"Topic Count\": [topic_count_base_embeddings, topic_count_filtered_embeddings, topic_count_webpreview_embeddings, topic_count_msg_webpreview_embeddings, topic_count_structural_embeddings, topic_count_msg_structural_embeddings], \n",
    "    \"Noise Instances\": [noise_base_embeddings, noise_filtered_embeddings, noise_webpreview_embeddings, noise_count_msg_webpreview_embeddings, noise_structural_embeddings, noise_structural_embeddings]\n",
    "}).set_index(\"Features\")\n",
    "\n",
    "evaluation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram_chat_clustering_03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
