{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from bertopic import BERTopic\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# download stopwords and tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), '../data/preprocessed/df_preprocessed.pkl')\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Chat-Text-Aggregations for Topic-Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the interpretability of the topics generated by BERTopic, we need to provide it with the texts for each chat. \n",
    "\n",
    "To make sure that the \"chat-text-aggregations\" used for this purpose are as meaningful as possible, we will perform the following operations: \n",
    "\n",
    "- **Basic preprocessing**, including lowercasing, stop word removal, removal of punctuation and digits, and tokenization.\n",
    "\n",
    "- **Removing custom stopwords** specific to the Telegram domain, such as:\n",
    "\n",
    "    - Telegram chat handles, which are frequently used to \"sign\" each  message in broadcast chats and could distort the analysis of the most common words.\n",
    "\n",
    "    - Common social media call-to-action phrases, such as \"share,\" \"follow,\" and \"comment,\" which are often repeated irrespective of topic.\n",
    "\n",
    "- **Multilingual processing:** Since our corpus is multilingual, language-dependent preprocessing will be applied only to messages in the most frequent languages, as the other languages contribute only a marginal number of messages.\n",
    "- **TF-IDF filtering:** We will filter out words below a certain TF-IDF threshold to ensure that only distinctive terms are included in the aggregation.\n",
    "\n",
    "Afterwards, we will aggregate the messages and webpage previews for each chat into a single string. This string, along with the chat vector representations created earlier, will be passed to BERTopic as a basis for its topic description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Preprocessing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df:pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "\n",
    "    print(\"Preprocessing messages...\")\n",
    "\n",
    "    # get stop words \n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "    cta_stop_words_en = {'click', 'tap', 'press', 'subscribe', 'follow', 'share', 'like', 'comment',\n",
    "                        'join', 'sign', 'visit', 'download', 'register', 'give', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "    cta_stop_words_de = {'klicken', 'tippen', 'drücken', 'abonnieren', 'folgen', 'teilen', 'mögen', 'kommentieren',\n",
    "                        'beitreten', 'anmelden', 'besuchen', 'herunterladen', 'registrieren', 'geben', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "\n",
    "    stop_words_en = stop_words_en.union(cta_stop_words_en)\n",
    "    stop_words_de = stop_words_de.union(cta_stop_words_de)\n",
    "    print(\"Stop words loaded\")\n",
    "\n",
    "    # get frequent chat handles\n",
    "    frequent_chat_handles = df[\"referenced_chat_handles\"].explode().value_counts()\n",
    "    frequent_chat_handles = frequent_chat_handles[frequent_chat_handles > 100].index.tolist()\n",
    "    print(\"Frequent chat handles loaded\")\n",
    "\n",
    "    # create regex patterns\n",
    "    def create_pattern(words):\n",
    "        return rf'\\b(?:{\"|\".join(map(re.escape, words))})\\b'\n",
    "\n",
    "    frequent_chat_pattern = create_pattern(frequent_chat_handles)\n",
    "    stop_words_en_pattern = create_pattern(stop_words_en)\n",
    "    stop_words_de_pattern = create_pattern(stop_words_de)\n",
    "    print(\"Regex-Patterns created\")\n",
    "    \n",
    "    # remove the most frequent chat handles  #TODO: Seems not to work\n",
    "    df[f\"{text_column}_cleaned\"] = df[text_column].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"Handles removed\")\n",
    "\n",
    "    # remove URLs\n",
    "    pattern = r\"(https?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(www\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(t\\.me\\/[^\\s.,?!)]*)\"\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(pattern, '', regex=True).str.strip()\n",
    "    print(\"URLs removed\")\n",
    "\n",
    "    # lowercase text\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.lower()\n",
    "    print(\"Lowercase\")\n",
    "    \n",
    "    # remove punctuation\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    print(\"Punctuation removed\")\n",
    "\n",
    "    # remove the most frequent chat handles that included an @\n",
    "    #df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    #print(\"Handles with @ removed\")\n",
    "\n",
    "    # remove punctuation again\n",
    "    #df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    #print(\"Punctuation removed\")\n",
    "\n",
    "    # remove digits\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(r'\\d+', '', regex=True).str.strip()\n",
    "    print(\"Digits removed\")\n",
    "\n",
    "    # remove english stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"].str.replace(stop_words_en_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"English stop words removed\")\n",
    "\n",
    "    # remove german stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"].str.replace(stop_words_de_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"German stop words removed\")\n",
    "\n",
    "    # fill NaN with empty string\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].fillna('')\n",
    "    \n",
    "    # tokenize text\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_cleaned\"].apply(lambda x: word_tokenize(x) if x else [])\n",
    "\n",
    "    #TODO:  Lemmatize???\n",
    "\n",
    "    #TODO: Remove Emoj\n",
    "\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_preprocessed\"].apply(lambda x: ' '.join(x))\n",
    "    print(\"Tokenized\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_msg_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_msgs_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_msg_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"message_text\")\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"message_text_preprocessed\"].to_pickle(preprocessen_msg_path)\n",
    "    print(\"Preprocessed messages saved\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading preprocessed messages...\")\n",
    "    preprocessed_msg = pd.read_pickle(preprocessen_msg_path)\n",
    "    df[\"message_text_preprocessed\"] = preprocessed_msg\n",
    "\n",
    "# display five random samples\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df[[\"message_text\", \"message_text_preprocessed\", \"referenced_chat_handles\"]].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_web_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_web_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_web_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"webpage_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of emoji-unicodes using data from \"https://unicode.org/Public/emoji/15.1/\"\n",
    "if not already_preprocessed:\n",
    "    \n",
    "    def load_emoji_list(file_paths: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Load a list of all emoji from the given file paths.\n",
    "        Args:\n",
    "            file_paths (list): A list of file paths to load emoji sequences from.\n",
    "        Returns:\n",
    "            list: A list of unicode sequences representing the loaded emoji sequences.\n",
    "        \"\"\"\n",
    "        \n",
    "        unicode_list = []\n",
    "\n",
    "        # match lines with unicode, including ranges like 231A..231B \n",
    "        range_pattern = re.compile(r\"([0-9A-Fa-f]{4,6})\\.\\.([0-9A-Fa-f]{4,6})\\s*;\\s*\")\n",
    "        code_point_pattern = re.compile(r\"([0-9A-Fa-f]{4,6}(?:\\s[0-9A-Fa-f]{4,6})*)\\s*;\\s*\")\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                range_match = range_pattern.match(line)\n",
    "                \n",
    "                # add elements of ranges as individual codes to list\n",
    "                if range_match:\n",
    "                    start_code, end_code = range_match.groups()\n",
    "                    start_int = int(start_code, 16)\n",
    "                    end_int = int(end_code, 16)\n",
    "                    unicode_list.extend([chr(code) for code in range(start_int, end_int + 1)])\n",
    "                else:\n",
    "                    code_match = code_point_pattern.match(line)\n",
    "                    if code_match:\n",
    "                        code_points = code_match.group(1)       \n",
    "                        code_point_list = code_points.split()\n",
    "                        # create zwj sequences by combining all code points\n",
    "                        unicode_list.append(''.join([chr(int(code, 16)) for code in code_point_list]))\n",
    "        print(\"Emoji sequences loaded\")\n",
    "        return unicode_list\n",
    "\n",
    "    # list the paths to the unicode-files\n",
    "    path_1 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-sequences.txt\")\n",
    "    path_2 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-test.txt\")\n",
    "    path_3 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-zwj-sequences.txt\")\n",
    "    file_paths = [path_1, path_2, path_3]\n",
    "\n",
    "    # load all emojis from the unicode-files\n",
    "    emoji_sequences = load_emoji_list(file_paths)\n",
    "\n",
    "    # create a regex pattern from the emoji sequence\n",
    "    emoji_pattern = '|'.join(re.escape(emoji) for emoji in emoji_sequences)\n",
    "    print(\"Emoji pattern created\")\n",
    "\n",
    "    def demojize_chunk(chunk, emoji_pattern):\n",
    "        # remove emojis\n",
    "        chunk[\"webpage_description_preprocessed\"] = chunk[\"webpage_description_preprocessed\"].str.replace(emoji_pattern, \" \", regex=True)\n",
    "        return chunk\n",
    "\n",
    "    n_jobs = 3  # Use three cores (seems to be fastest?)\n",
    "\n",
    "    # remove emojis in parallel for each chunk\n",
    "    chunks = np.array_split(df, n_jobs)\n",
    "    df_chunks = Parallel(n_jobs=n_jobs)(delayed(demojize_chunk)(chunk, emoji_pattern) for chunk in chunks)\n",
    "    df = pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"webpage_description_preprocessed\"].to_pickle(preprocessen_web_path)\n",
    "    print(\"Preprocessed messages saved\")    \n",
    "\n",
    "# simply load the preprocessed data, if it was already preprocessed\n",
    "else:\n",
    "    print(\"Loading preprocessed webpage previews...\")\n",
    "    preprocessed_web_previews = pd.read_pickle(preprocessen_web_path)\n",
    "    df[\"webpage_description_preprocessed\"] = preprocessed_web_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply TF-IDF-Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_msgs_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"message_text_lang\"] == \"English\"][\"message_text_preprocessed\"]\n",
    "    german_texts = df[df[\"message_text_lang\"] == \"German\"][\"message_text_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"message_text_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"message_text_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"message_text_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"message_text_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # TODO: Change column name\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"message_text_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"message_text_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"message_text_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_web_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"webpage_description_lang\"] == \"English\"][\"webpage_description_preprocessed\"]\n",
    "    german_texts = df[df[\"webpage_description_lang\"] == \"German\"][\"webpage_description_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold_web(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"webpage_description_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"webpage_description_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"webpage_description_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"webpage_description_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"webpage_description_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold_web(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"webpage_description_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf-filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"webpage_description_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create Chat-Text-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_texts = chat_texts.astype(str)\n",
    "chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create Webpage-Preview-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_webpage_previews = grouped[\"webpage_description_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_webpage_previews = chat_webpage_previews.astype(str)\n",
    "chat_webpage_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(os.getcwd(), '../features/0_base_chat_vectors.npy')\n",
    "base_chat_vectors = np.load(base_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {base_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {base_chat_vectors.iloc[0].shape}\")\n",
    "base_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# prepare the embeddings for dimensionality reduction by stacking them\n",
    "chat_embeddings = np.vstack(base_chat_vectors)\n",
    "\n",
    "# Compute the Elbow Curve\n",
    "inertia = []\n",
    "K = range(1, 60) \n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(chat_embeddings)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(K, inertia, 'bo-', markersize=8)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use KeyBERTInspired¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bertTopic(chat_embeddings: pd.Series, chat_texts: pd.Series): \n",
    "\n",
    "    # prepare the embeddings for dimensionality reduction by stacking them\n",
    "    chat_embeddings = np.vstack(chat_embeddings)\n",
    "    print(\"Preperation: Done\")\n",
    "    \n",
    "    # apply dimensionality reduction (We use PCA and 5 dimensions, as suggested by the BERTopic documentation)\n",
    "    #pca = PCA(n_components=5)\n",
    "    #reduced_embeddings = pca.fit_transform(chat_embeddings)\n",
    "    #print(\"Dimensionality Reduction: Done\")\n",
    "\n",
    "    # create your representation model\n",
    "    representation_model = KeyBERTInspired()\n",
    "\n",
    "    # initiate the BERTopic model\n",
    "    docs = chat_texts.tolist()\n",
    "    #cluster_model = KMeans(n_clusters=14) #9 #15->gut\n",
    "    topic_model = BERTopic(embedding_model=model, \n",
    "                           verbose=True, \n",
    "                           calculate_probabilities=True, \n",
    "                           representation_model=representation_model,    \n",
    "                           #hdbscan_model=cluster_model\n",
    "                           ) \n",
    "    print(\"Loading model: Done\")\n",
    "        \n",
    "    # fit the model to the reduced embeddings\n",
    "    topics, propabilities = topic_model.fit_transform(embeddings = chat_embeddings, documents = docs)\n",
    "    print(\"Model fitting: Done\")\n",
    "\n",
    "    return topics, propabilities, topic_model\n",
    "\n",
    "topics, propabilities, topic_model = apply_bertTopic(base_chat_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "nbformat.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_visualisations(topic_model, embeddings, texts_aggregations):\n",
    "    # Visualize topics\n",
    "    #topic_model.visualize_topics().show()  \n",
    "\n",
    "    print(\"Topic Map:\")\n",
    "    # UMAP dimensionality reduction\n",
    "    from umap import UMAP\n",
    "    import numpy as np\n",
    "    docs = texts_aggregations.tolist()\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "    \n",
    "    # Visualize documents using UMAP embeddings\n",
    "    topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings).show()\n",
    "\n",
    "    print(\"Bar Chart, displaying the top 13 topics and top 20 words per topic:\")\n",
    "    # Visualize bar chart for top 13 topics and 20 words per topic\n",
    "    topic_model.visualize_barchart(top_n_topics=13, n_words=20).show()\n",
    "\n",
    "    print(\"Hierarchical Topics:\")\n",
    "    # Visualize hierarchical topics\n",
    "    hierarchical_topics = topic_model.hierarchical_topics(texts_aggregations)\n",
    "    topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics).show()\n",
    "\n",
    "create_topic_visualisations(topic_model, base_chat_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- News without lies -> spannend, weil repetetiv -> News aggregators? -> \n",
    "- Einige Cluster zeigen Einfluss von \"Signaturen\" -> z.B. .... -> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtered Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the filtered Chat Embeddings\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_path = os.path.join(os.getcwd(), '../features/1_filtered_chat_vectors.npy')\n",
    "filtered_chat_vectors = np.load(filtered_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {filtered_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {filtered_chat_vectors.iloc[0].shape}\")\n",
    "filtered_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create filtered Chat-Message-Text-Aggregations\n",
    "\n",
    "Now we filter the dataset to remove all forwarded/original.Message-Pairs using the indices we saveed in  `02_feature_engineering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_path = os.path.join(os.getcwd(), \"../features/1_implicit_ref_filtered_indices.npy\")\n",
    "filtered_rows_indices = np.load(indices_path)\n",
    "df_references_filtered = df.loc[filtered_rows_indices]\n",
    "df_references_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset already contains the preprocessed Message-Text, we simply need to aggregate them again to create Chat-Message-Text-Aggregation that exclude Original/Forward-Pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_references_filtered.groupby(\"telegram_chat_id\")\n",
    "filtered_chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "filtered_chat_texts = filtered_chat_texts.astype(str)\n",
    "filtered_chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chat_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chat_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topics, propabilities, topic_model = apply_bertTopic(filtered_chat_vectors, filtered_chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Webpage Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpreview_path = os.path.join(os.getcwd(), '../features/3_webpreview_chat_vectors.npy')\n",
    "webpreview_chat_vectors = np.load(webpreview_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {webpreview_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {webpreview_chat_vectors.iloc[0].shape}\")\n",
    "webpreview_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, propabilities, topic_model = apply_bertTopic(webpreview_chat_vectors, chat_webpage_previews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualize and explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, webpreview_chat_vectors, chat_webpage_previews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined Message & Webpage-Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the two kinds of text embeddings and inspect the changes in clustering results.\n",
    "\n",
    "#### 1. Combine Message-Text- and Webpage-Preview-Vectors\n",
    "\n",
    "First, we load the chat-vectors we created by combining the two webpage-preview- and message-vectors by taking their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_vectors_path = os.path.join(os.getcwd(), '../features/3_msg_webpreview_chat_vectors.npy')\n",
    "combined_vectors = np.load(combine_vectors_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create combined Chat-Text-Aggregations\n",
    "\n",
    "Next, we aggregate the Text-Aggregations for Webpage-Previews and Chat-Messages in order to use them  to make the topics interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame to combine the texts\n",
    "combined_text_dataframe = pd.DataFrame({\n",
    "    \"chat_texts\": chat_texts,\n",
    "    \"chat_webpage_previews\": chat_webpage_previews\n",
    "})\n",
    "\n",
    "# combine the texts\n",
    "combined_text_dataframe[\"combined_texts\"] = combined_text_dataframe[\"chat_texts\"] + \" \" + combined_text_dataframe[\"chat_webpage_previews\"]\n",
    "\n",
    "# calculate the length of the texts\n",
    "combined_text_dataframe[\"chat_texts_len\"] = combined_text_dataframe[\"chat_texts\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"chat_webpage_previews_len\"] = combined_text_dataframe[\"chat_webpage_previews\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"combined_texts_len\"] = combined_text_dataframe[\"combined_texts\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# check if the combined arrays are the same length as the original arrays combined\n",
    "assert combined_text_dataframe[\"combined_texts_len\"].equals(combined_text_dataframe[\"chat_texts_len\"] + combined_text_dataframe[\"chat_webpage_previews_len\"])\n",
    "\n",
    "# get the combined texts\n",
    "combined_texts = combined_text_dataframe[\"combined_texts\"]\n",
    "combined_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the Combined Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _, topic_model = apply_bertTopic(combined_vectors, combined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, combined_vectors, combined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structural Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we conduct chat-clustering using a chats structural attributes. Structural attributes are a chats connections to other telegram entities. \n",
    "\n",
    "For our purposes, we will consider two kinds of connections:\n",
    "\n",
    "1. Forwarded messages between chats.\n",
    "\n",
    "2. Mentions of chats or other telegram-entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Matrices\n",
    "\n",
    "To vectorize these connections, we created chat-chat-co-occurence-matrices based on forwards and text based references between chats in `02_feature_engineering`, which we'll load now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "fwd_log_path = os.path.join(os.getcwd(), '../features/2_log_fwd_matrix.pkl')\n",
    "fwd_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_fwd_matrix.pkl')\n",
    "ref_log_path = os.path.join(os.getcwd(), '../features/2_log_ref_matrix.pkl')\n",
    "ref_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_ref_matrix.pkl')\n",
    "\n",
    "# load the co-occurrence matrices\n",
    "fwd_log_matrix = pd.read_pickle(fwd_log_path)\n",
    "fwd_onehot_matrix = pd.read_pickle(fwd_onehot_path)\n",
    "ref_log_matrix = pd.read_pickle(ref_log_path)\n",
    "ref_onehot_matrix = pd.read_pickle(ref_onehot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_onehot_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create concated versions of the one hot encoded matrices\n",
    "fwd_matrix = np.concatenate([fwd_onehot_matrix, ref_onehot_matrix], axis=1)\n",
    "\n",
    "# check if the dimensions of the resulting combined matrix  are correct\n",
    "dimension_fwd = fwd_onehot_matrix.shape[1]\n",
    "dimension_ref = ref_onehot_matrix.shape[1]\n",
    "assert fwd_matrix.shape[1] == dimension_fwd + dimension_ref\n",
    "\n",
    "# create concated versions of the log scaled matrices\n",
    "ref_matrix = np.concatenate([fwd_log_matrix, ref_log_matrix], axis=1)\n",
    "\n",
    "# check if the dimensions of the resulting combined matrix  are correct\n",
    "dimension_fwd = fwd_log_matrix.shape[1]\n",
    "dimension_ref = ref_log_matrix.shape[1]\n",
    "assert ref_matrix.shape[1] == dimension_fwd + dimension_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Concenate the Features\n",
    "\n",
    "To supplement the text embeddings with structural information while still using BERTopic for clustering, we'll concatenate the new feature vectors with the chat embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_vectors(chat_vectors, structural_vectors):\n",
    "        \n",
    "    # create a DataFrame to combine the texts\n",
    "    structual_info_df = pd.DataFrame({\n",
    "        \"chat_embeddings\": base_chat_vectors,\n",
    "        \"structural_information\": chat_webpage_previews\n",
    "    })\n",
    "    \n",
    "    # combine the vectors\n",
    "    structual_info_df[\"concatenated_vectors\"] = structual_info_df[\"chat_embeddings\"] + structual_info_df[\"structural_information\"]\n",
    "    \n",
    "    # check if the combined arrays are the same length as the original arrays combined\n",
    "    dimension_chat_vectors = chat_vectors.shape[1]\n",
    "    dimension_structural_vectors = structural_vectors.shape[1]    \n",
    "    assert dimension_chat_vectors + dimension_structural_vectors == structual_info_df[\"concatenated_vectors\"].iloc[0].shape\n",
    "    \n",
    "    return structual_info_df[\"concatenated_vectors\"]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram_chat_clustering_03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
