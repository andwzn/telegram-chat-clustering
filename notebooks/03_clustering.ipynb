{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from bertopic import BERTopic\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# download stopwords and tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), '../data/preprocessed/df_preprocessed.pkl')\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Chat-Text-Aggregations for Topic-Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the interpretability of the topics generated by BERTopic, we need to provide it with the texts for each chat. \n",
    "\n",
    "To make sure that the \"chat-text-aggregations\" used for this purpose are as meaningful as possible, we will perform the following operations: \n",
    "\n",
    "- **Basic preprocessing**, including lowercasing, stop word removal, removal of punctuation and digits, and tokenization.\n",
    "\n",
    "- **Removing custom stopwords** specific to the Telegram domain, such as:\n",
    "\n",
    "    - Telegram chat handles, which are frequently used to \"sign\" each  message in broadcast chats and could distort the analysis of the most common words.\n",
    "\n",
    "    - Common social media call-to-action phrases, such as \"share,\" \"follow,\" and \"comment,\" which are often repeated irrespective of topic.\n",
    "\n",
    "- **Multilingual processing:** Since our corpus is multilingual, language-dependent preprocessing will be applied only to messages in the most frequent languages, as the other languages contribute only a marginal number of messages.\n",
    "- **TF-IDF filtering:** We will filter out words below a certain TF-IDF threshold to ensure that only distinctive terms are included in the aggregation.\n",
    "\n",
    "Afterwards, we will aggregate the messages and webpage previews for each chat into a single string. This string, along with the chat vector representations created earlier, will be passed to BERTopic as a basis for its topic description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Preprocessing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df:pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "\n",
    "    print(\"Preprocessing messages...\")\n",
    "\n",
    "    # get stop words \n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "    cta_stop_words_en = {'click', 'tap', 'press', 'subscribe', 'follow', 'share', 'like', 'comment',\n",
    "                        'join', 'sign', 'visit', 'download', 'register', 'give', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "    cta_stop_words_de = {'klicken', 'tippen', 'drücken', 'abonnieren', 'folgen', 'teilen', 'mögen', 'kommentieren',\n",
    "                        'beitreten', 'anmelden', 'besuchen', 'herunterladen', 'registrieren', 'geben', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "\n",
    "    stop_words_en = stop_words_en.union(cta_stop_words_en)\n",
    "    stop_words_de = stop_words_de.union(cta_stop_words_de)\n",
    "    print(\"Stop words loaded\")\n",
    "\n",
    "    # get frequent chat handles\n",
    "    frequent_chat_handles = df[\"referenced_chat_handles\"].explode().value_counts()\n",
    "    frequent_chat_handles = frequent_chat_handles[frequent_chat_handles > 100].index.tolist()\n",
    "    print(\"Frequent chat handles loaded\")\n",
    "\n",
    "    # create regex patterns\n",
    "    def create_pattern(words):\n",
    "        return rf'\\b(?:{\"|\".join(map(re.escape, words))})\\b'\n",
    "\n",
    "    frequent_chat_pattern = create_pattern(frequent_chat_handles)\n",
    "    stop_words_en_pattern = create_pattern(stop_words_en)\n",
    "    stop_words_de_pattern = create_pattern(stop_words_de)\n",
    "    print(\"Regex-Patterns created\")\n",
    "    \n",
    "    # remove the most frequent chat handles  #TODO: Seems not to work\n",
    "    df[f\"{text_column}_cleaned\"] = df[text_column].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"Handles removed\")\n",
    "\n",
    "    # remove URLs\n",
    "    pattern = r\"(https?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(www\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(t\\.me\\/[^\\s.,?!)]*)\"\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(pattern, '', regex=True).str.strip()\n",
    "    print(\"URLs removed\")\n",
    "\n",
    "    # lowercase text\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.lower()\n",
    "    print(\"Lowercase\")\n",
    "    \n",
    "    # remove punctuation\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    print(\"Punctuation removed\")\n",
    "\n",
    "    # remove the most frequent chat handles that included an @\n",
    "    #df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    #print(\"Handles with @ removed\")\n",
    "\n",
    "    # remove punctuation again\n",
    "    #df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    #print(\"Punctuation removed\")\n",
    "\n",
    "    # remove digits\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(r'\\d+', '', regex=True).str.strip()\n",
    "    print(\"Digits removed\")\n",
    "\n",
    "    # remove english stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"].str.replace(stop_words_en_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"English stop words removed\")\n",
    "\n",
    "    # remove german stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"].str.replace(stop_words_de_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"German stop words removed\")\n",
    "\n",
    "    # fill NaN with empty string\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].fillna('')\n",
    "    \n",
    "    # tokenize text\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_cleaned\"].apply(lambda x: word_tokenize(x) if x else [])\n",
    "\n",
    "    #TODO:  Lemmatize???\n",
    "\n",
    "    #TODO: Remove Emoj\n",
    "\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_preprocessed\"].apply(lambda x: ' '.join(x))\n",
    "    print(\"Tokenized\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_msg_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_msgs_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_msg_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"message_text\")\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"message_text_preprocessed\"].to_pickle(preprocessen_msg_path)\n",
    "    print(\"Preprocessed messages saved\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading preprocessed messages...\")\n",
    "    preprocessed_msg = pd.read_pickle(preprocessen_msg_path)\n",
    "    df[\"message_text_preprocessed\"] = preprocessed_msg\n",
    "\n",
    "# display five random samples\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df[[\"message_text\", \"message_text_preprocessed\", \"referenced_chat_handles\"]].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_web_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_web_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_web_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"webpage_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of emoji-unicodes using data from \"https://unicode.org/Public/emoji/15.1/\"\n",
    "if not already_preprocessed:\n",
    "    \n",
    "    def load_emoji_list(file_paths: list[str]) -> list[str]:\n",
    "        \"\"\"\n",
    "        Load a list of all emoji from the given file paths.\n",
    "        Args:\n",
    "            file_paths (list): A list of file paths to load emoji sequences from.\n",
    "        Returns:\n",
    "            list: A list of unicode sequences representing the loaded emoji sequences.\n",
    "        \"\"\"\n",
    "        \n",
    "        unicode_list = []\n",
    "\n",
    "        # match lines with unicode, including ranges like 231A..231B \n",
    "        range_pattern = re.compile(r\"([0-9A-Fa-f]{4,6})\\.\\.([0-9A-Fa-f]{4,6})\\s*;\\s*\")\n",
    "        code_point_pattern = re.compile(r\"([0-9A-Fa-f]{4,6}(?:\\s[0-9A-Fa-f]{4,6})*)\\s*;\\s*\")\n",
    "\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                range_match = range_pattern.match(line)\n",
    "                \n",
    "                # add elements of ranges as individual codes to list\n",
    "                if range_match:\n",
    "                    start_code, end_code = range_match.groups()\n",
    "                    start_int = int(start_code, 16)\n",
    "                    end_int = int(end_code, 16)\n",
    "                    unicode_list.extend([chr(code) for code in range(start_int, end_int + 1)])\n",
    "                else:\n",
    "                    code_match = code_point_pattern.match(line)\n",
    "                    if code_match:\n",
    "                        code_points = code_match.group(1)       \n",
    "                        code_point_list = code_points.split()\n",
    "                        # create zwj sequences by combining all code points\n",
    "                        unicode_list.append(''.join([chr(int(code, 16)) for code in code_point_list]))\n",
    "        print(\"Emoji sequences loaded\")\n",
    "        return unicode_list\n",
    "\n",
    "    # list the paths to the unicode-files\n",
    "    path_1 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-sequences.txt\")\n",
    "    path_2 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-test.txt\")\n",
    "    path_3 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-zwj-sequences.txt\")\n",
    "    file_paths = [path_1, path_2, path_3]\n",
    "\n",
    "    # load all emojis from the unicode-files\n",
    "    emoji_sequences = load_emoji_list(file_paths)\n",
    "\n",
    "    # create a regex pattern from the emoji sequence\n",
    "    emoji_pattern = '|'.join(re.escape(emoji) for emoji in emoji_sequences)\n",
    "    print(\"Emoji pattern created\")\n",
    "\n",
    "    def demojize_chunk(chunk, emoji_pattern):\n",
    "        # remove emojis\n",
    "        chunk[\"webpage_description_preprocessed\"] = chunk[\"webpage_description_preprocessed\"].str.replace(emoji_pattern, \" \", regex=True)\n",
    "        return chunk\n",
    "\n",
    "    n_jobs = 3  # Use three cores (seems to be fastest?)\n",
    "\n",
    "    # remove emojis in parallel for each chunk\n",
    "    chunks = np.array_split(df, n_jobs)\n",
    "    df_chunks = Parallel(n_jobs=n_jobs)(delayed(demojize_chunk)(chunk, emoji_pattern) for chunk in chunks)\n",
    "    df = pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"webpage_description_preprocessed\"].to_pickle(preprocessen_web_path)\n",
    "    print(\"Preprocessed messages saved\")    \n",
    "\n",
    "# simply load the preprocessed data, if it was already preprocessed\n",
    "else:\n",
    "    print(\"Loading preprocessed webpage previews...\")\n",
    "    preprocessed_web_previews = pd.read_pickle(preprocessen_web_path)\n",
    "    df[\"webpage_description_preprocessed\"] = preprocessed_web_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply TF-IDF-Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_msgs_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"message_text_lang\"] == \"English\"][\"message_text_preprocessed\"]\n",
    "    german_texts = df[df[\"message_text_lang\"] == \"German\"][\"message_text_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"message_text_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"message_text_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"message_text_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"message_text_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # TODO: Change column name\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"message_text_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"message_text_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"message_text_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_web_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"webpage_description_lang\"] == \"English\"][\"webpage_description_preprocessed\"]\n",
    "    german_texts = df[df[\"webpage_description_lang\"] == \"German\"][\"webpage_description_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold_web(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"webpage_description_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"webpage_description_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"webpage_description_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"webpage_description_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"webpage_description_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold_web(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"webpage_description_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf-filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"webpage_description_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create Chat-Text-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_texts = chat_texts.astype(str)\n",
    "chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create Webpage-Preview-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_webpage_previews = grouped[\"webpage_description_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_webpage_previews = chat_webpage_previews.astype(str)\n",
    "chat_webpage_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(os.getcwd(), '../features/0_base_chat_vectors.npy')\n",
    "base_chat_vectors = np.load(base_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {base_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {base_chat_vectors.iloc[0].shape}\")\n",
    "base_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use KeyBERTInspired¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bertTopic(chat_embeddings: pd.Series, chat_texts: pd.Series): \n",
    "\n",
    "    # prepare the embeddings for dimensionality reduction by stacking them\n",
    "    chat_embeddings = np.vstack(chat_embeddings)\n",
    "    print(\"Preperation: Done\")\n",
    "    \n",
    "    # apply dimensionality reduction (We use PCA and 5 dimensions, as suggested by the BERTopic documentation)\n",
    "    #pca = PCA(n_components=5)\n",
    "    #reduced_embeddings = pca.fit_transform(chat_embeddings)\n",
    "    #print(\"Dimensionality Reduction: Done\")\n",
    "\n",
    "    # create your representation model\n",
    "    representation_model = KeyBERTInspired() #TODO: Configure?\n",
    "    \n",
    "    #TODO: Random state?\n",
    "\n",
    "    # initiate the BERTopic model\n",
    "    docs = chat_texts.tolist()\n",
    "    #cluster_model = KMeans(n_clusters=14) #9 #15->gut\n",
    "    topic_model = BERTopic(embedding_model=model, \n",
    "                           verbose=True, \n",
    "                           calculate_probabilities=True, \n",
    "                           representation_model=representation_model)\n",
    "    #hdbscan_model=cluster_model \n",
    "    print(\"Loading model: Done\")\n",
    "        \n",
    "    # fit the model to the reduced embeddings\n",
    "    topics, propabilities = topic_model.fit_transform(embeddings = chat_embeddings, documents = docs)\n",
    "    print(\"Model fitting: Done\")\n",
    "\n",
    "    return topics, propabilities, topic_model\n",
    "\n",
    "topics, propabilities, topic_model = apply_bertTopic(base_chat_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_visualisations(topic_model, embeddings, texts_aggregations):\n",
    "    # Visualize topics\n",
    "    #topic_model.visualize_topics().show()  \n",
    "\n",
    "    print(\"Topic Map:\")\n",
    "    # UMAP dimensionality reduction\n",
    "    from umap import UMAP\n",
    "    import numpy as np\n",
    "    docs = texts_aggregations.tolist()\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "    \n",
    "    # Visualize documents using UMAP embeddings\n",
    "    topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings).show()\n",
    "\n",
    "    print(\"Bar Chart, displaying the top 13 topics and top 20 words per topic:\")\n",
    "    # Visualize bar chart for top 13 topics and 20 words per topic\n",
    "    topic_model.visualize_barchart(top_n_topics=13, n_words=20).show()\n",
    "\n",
    "    print(\"Hierarchical Topics:\")\n",
    "    # Visualize hierarchical topics\n",
    "    hierarchical_topics = topic_model.hierarchical_topics(texts_aggregations)\n",
    "    topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics).show()\n",
    "\n",
    "create_topic_visualisations(topic_model, base_chat_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluations(chat_embeddings, propabilities, topic_model, text_aggregations):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns:\n",
    "        silhouette_score_result (float): The silhouette score of the chat embeddings\n",
    "        topic_count (int): The number of topics (including the \"Other\" (-1) topic)\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the document info and topics\n",
    "    document_info = topic_model.get_document_info(text_aggregations)\n",
    "    topics = document_info['Topic']\n",
    "    \n",
    "    # prepare the embeddings by stacking them\n",
    "    chat_embeddings = np.vstack(chat_embeddings)    \n",
    "\n",
    "    # get the silhouette score while ignoring the \"Other\" topic\n",
    "    valid_indices_filter = topics != -1\n",
    "    filtered_embeddings = chat_embeddings[valid_indices_filter]\n",
    "    filtered_topics = topics[valid_indices_filter]\n",
    "    silhouette_score_result = silhouette_score(X=filtered_embeddings, labels=filtered_topics)\n",
    "    print(f'Silhouette Score: {silhouette_score_result}')\n",
    "    \n",
    "    # calculate the number of topics found\n",
    "    topic_count = len(np.unique(topics))\n",
    "    print(f'Topic Count: {topic_count}')\n",
    "    \n",
    "    # calculate the number of noise points\n",
    "    noise_count = len(topics[topics == -1])\n",
    "    print(f'Noise Count: {noise_count}')\n",
    "    \n",
    "    return silhouette_score_result, topic_count, noise_count\n",
    "    \n",
    "(ss_base_embeddings, \n",
    " topic_count_base_embeddings, \n",
    " noise_base_embeddings) = get_evaluations(base_chat_vectors, \n",
    "                                          propabilities, \n",
    "                                          topic_model, \n",
    "                                          chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtered Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the filtered Chat Embeddings\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_path = os.path.join(os.getcwd(), '../features/1_filtered_chat_vectors.npy')\n",
    "filtered_chat_vectors = np.load(filtered_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {filtered_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {filtered_chat_vectors.iloc[0].shape}\")\n",
    "filtered_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create filtered Chat-Message-Text-Aggregations\n",
    "\n",
    "Now we filter the dataset to remove all forwarded/original.Message-Pairs using the indices we saveed in  `02_feature_engineering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_path = os.path.join(os.getcwd(), \"../features/1_implicit_ref_filtered_indices.npy\")\n",
    "filtered_rows_indices = np.load(indices_path)\n",
    "df_references_filtered = df.loc[filtered_rows_indices]\n",
    "df_references_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset already contains the preprocessed Message-Text, we simply need to aggregate them again to create Chat-Message-Text-Aggregation that exclude Original/Forward-Pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_references_filtered.groupby(\"telegram_chat_id\")\n",
    "filtered_chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "filtered_chat_texts = filtered_chat_texts.astype(str)\n",
    "filtered_chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, propabilities, topic_model = apply_bertTopic(filtered_chat_vectors, filtered_chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, filtered_chat_vectors, filtered_chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ss_filtered_embeddings, \n",
    " topic_count_filtered_embeddings,\n",
    " noise_filtered_embeddings) = get_evaluations(filtered_chat_vectors,\n",
    "                                              propabilities,\n",
    "                                              topic_model,\n",
    "                                              filtered_chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Webpage Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpreview_path = os.path.join(os.getcwd(), '../features/3_webpreview_chat_vectors.npy')\n",
    "webpreview_chat_vectors = np.load(webpreview_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {webpreview_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {webpreview_chat_vectors.iloc[0].shape}\")\n",
    "webpreview_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Load or download the model\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, propabilities, topic_model = apply_bertTopic(webpreview_chat_vectors, chat_webpage_previews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualize and explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, webpreview_chat_vectors, chat_webpage_previews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ss_webpreview_embeddings, \n",
    " topic_count_webpreview_embeddings,\n",
    " noise_webpreview_embeddings) = get_evaluations(webpreview_chat_vectors, \n",
    "                                                propabilities,\n",
    "                                                topic_model,\n",
    "                                                chat_webpage_previews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined Message & Webpage-Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the two kinds of text embeddings and inspect the changes in clustering results.\n",
    "\n",
    "#### 1. Combine Message-Text- and Webpage-Preview-Vectors\n",
    "\n",
    "First, we load the chat-vectors we created by combining the two webpage-preview- and message-vectors by taking their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_vectors_path = os.path.join(os.getcwd(), '../features/3_msg_webpreview_chat_vectors.npy')\n",
    "combined_vectors = np.load(combine_vectors_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create combined Chat-Text-Aggregations\n",
    "\n",
    "Next, we aggregate the Text-Aggregations for Webpage-Previews and Chat-Messages in order to use them  to make the topics interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame to combine the texts\n",
    "combined_text_dataframe = pd.DataFrame({\n",
    "    \"chat_texts\": chat_texts,\n",
    "    \"chat_webpage_previews\": chat_webpage_previews\n",
    "})\n",
    "\n",
    "# combine the texts\n",
    "combined_text_dataframe[\"combined_texts\"] = combined_text_dataframe[\"chat_texts\"] + \" \" + combined_text_dataframe[\"chat_webpage_previews\"]\n",
    "\n",
    "# calculate the length of the texts\n",
    "combined_text_dataframe[\"chat_texts_len\"] = combined_text_dataframe[\"chat_texts\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"chat_webpage_previews_len\"] = combined_text_dataframe[\"chat_webpage_previews\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"combined_texts_len\"] = combined_text_dataframe[\"combined_texts\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# check if the combined arrays are the same length as the original arrays combined\n",
    "assert combined_text_dataframe[\"combined_texts_len\"].equals(combined_text_dataframe[\"chat_texts_len\"] + combined_text_dataframe[\"chat_webpage_previews_len\"])\n",
    "\n",
    "# get the combined texts\n",
    "combined_texts = combined_text_dataframe[\"combined_texts\"]\n",
    "combined_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the Combined Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _, topic_model = apply_bertTopic(combined_vectors, combined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, combined_vectors, combined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ss_msg_webpreview_embeddings, \n",
    " topic_count_msg_webpreview_embeddings, \n",
    " noise_count_msg_webpreview_embeddings) = get_evaluations(combined_vectors,\n",
    "                                                          propabilities, \n",
    "                                                          topic_model,\n",
    "                                                          combined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structural Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we conduct chat-clustering using a chats structural attributes. Structural attributes are a chats connections to other telegram entities. \n",
    "\n",
    "For our purposes, we have considered two kinds of connections:\n",
    "\n",
    "1. Forwarded (fwd) messages between chats.\n",
    "\n",
    "2. Textual references (ref) to chats or other telegram-entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Chat-Vectors\n",
    "\n",
    "To vectorize these connections, we created chat-chat-matrices based on forwards and text based references between chats in `02_feature_engineering`, which we'll load now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "fwd_log_path = os.path.join(os.getcwd(), '../features/2_log_fwd_vectors.pkl')\n",
    "fwd_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_fwd_vectors.pkl')\n",
    "ref_log_path = os.path.join(os.getcwd(), '../features/2_log_ref_vectors.pkl')\n",
    "ref_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_ref_vectors.pkl')\n",
    "\n",
    "# load the chat vectors\n",
    "fwd_log_vectors = pd.read_pickle(fwd_log_path)\n",
    "fwd_onehot_vectors = pd.read_pickle(fwd_onehot_path)\n",
    "ref_log_vectors = pd.read_pickle(ref_log_path)\n",
    "ref_onehot_vectors = pd.read_pickle(ref_onehot_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the forward-based and the reference-based chat-vectors to create our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the vectors\n",
    "structure_log_vectors = fwd_log_vectors.combine(ref_log_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "structure_onehot_vectors = fwd_onehot_vectors.combine(ref_onehot_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the original vectors\n",
    "dimension_fwd_vectors = len(fwd_log_vectors.iloc[1])\n",
    "dimension_ref_vectors = len(ref_log_vectors.iloc[1])\n",
    "assert len(structure_log_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors\n",
    "assert len(structure_onehot_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the Structural Vectors\n",
    "\n",
    "Eventhough they are no Text-Embeddings we will pass the structural vectors to BERTopic for clustering. This is possible, as BERTopic can accept any kind of numerical custom embeddings instead of generating them from text.\n",
    "\n",
    "We will again use the filtered Message-Text-Aggregations to make the topics interpretable. These documents will only be used for topic labeling and interpretation. The clustering itself will be entirely driven by the chat-chat-matrices we pass as embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _, topic_model = apply_bertTopic(structure_log_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Visualise and Explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, structure_log_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ss_structural_embeddings, \n",
    "topic_count_structural_embeddings, \n",
    "noise_structural_embeddings) = get_evaluations(structure_log_vectors,\n",
    "                                               propabilities,\n",
    "                                               topic_model, \n",
    "                                               chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combined Structural Vectors & Message Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we conduct chat-clustering using a chats structural attributes. Structural attributes are a chats connections to other telegram entities. \n",
    "\n",
    "For our purposes, we have considered two kinds of connections:\n",
    "\n",
    "1. Forwarded (fwd) messages between chats.\n",
    "\n",
    "2. Textual references (ref) to chats or other telegram-entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Chat-Vectors\n",
    "\n",
    "To vectorize these connections, we created chat-chat-matrices based on forwards and text based references between chats in `02_feature_engineering`, which we'll load now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "fwd_log_path = os.path.join(os.getcwd(), '../features/2_log_fwd_vectors.pkl')\n",
    "fwd_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_fwd_vectors.pkl')\n",
    "ref_log_path = os.path.join(os.getcwd(), '../features/2_log_ref_vectors.pkl')\n",
    "ref_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_ref_vectors.pkl')\n",
    "\n",
    "# load the chat vectors\n",
    "fwd_log_vectors = pd.read_pickle(fwd_log_path)\n",
    "fwd_onehot_vectors = pd.read_pickle(fwd_onehot_path)\n",
    "ref_log_vectors = pd.read_pickle(ref_log_path)\n",
    "ref_onehot_vectors = pd.read_pickle(ref_onehot_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the forward-based and the reference-based chat-vectors to create our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the vectors\n",
    "structure_log_vectors = fwd_log_vectors.combine(ref_log_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "structure_onehot_vectors = fwd_onehot_vectors.combine(ref_onehot_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the original vectors\n",
    "dimension_fwd_vectors = len(fwd_log_vectors.iloc[1])\n",
    "dimension_ref_vectors = len(ref_log_vectors.iloc[1])\n",
    "assert len(structure_log_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors\n",
    "assert len(structure_onehot_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Concenate the Features\n",
    "\n",
    "To supplement the text embeddings with structural information while still using BERTopic for clustering, we'll concatenate the new feature vectors and the chat embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the message embedding based chat vectors with the log-scaled structure-based chat vectors\n",
    "combined_vectors = base_chat_vectors.combine(structure_log_vectors, lambda x, y: np.concatenate([x, y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the message text embeddings and the structure-based vectors\n",
    "assert len(combined_vectors.iloc[1]) == len(base_chat_vectors.iloc[1]) + len(structure_log_vectors.iloc[1])\n",
    "\n",
    "combined_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the Combined Chat Vectors\n",
    "\n",
    "Now, we can cluster the resulting combined vectors using BERTopic. As we're only use the Messages as textual feature, we will reuse the filtered Chat-Message-Text-Aggregations we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _, topic_model = apply_bertTopic(combined_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(topic_model, combined_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ss_msg_structural_embeddings, \n",
    " topic_count_msg_structural_embeddings,\n",
    " noise_structural_embeddings) = get_evaluations(combined_vectors,\n",
    "                                                propabilities, \n",
    "                                                topic_model, \n",
    "                                                chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the different approaches based on the evaluation data we collected for each feature and feature combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame({\n",
    "    \"Model\": [\"Base\", \"Filtered\", \"Webpreview\", \"Msg + Webpreview\", \"Structural\", \"Msg + Structural\"],\n",
    "    \"Silhouette Score\": [ss_base_embeddings, ss_filtered_embeddings, ss_webpreview_embeddings, ss_msg_webpreview_embeddings, ss_structural_embeddings, ss_msg_structural_embeddings],\n",
    "    \"Topic Count\": [topic_count_base_embeddings, topic_count_filtered_embeddings, topic_count_webpreview_embeddings, topic_count_msg_webpreview_embeddings, topic_count_structural_embeddings, topic_count_msg_structural_embeddings], \n",
    "    \"Noise Instances\": [noise_base_embeddings, noise_filtered_embeddings, noise_webpreview_embeddings, noise_count_msg_webpreview_embeddings, noise_structural_embeddings, noise_structural_embeddings]\n",
    "}).set_index(\"Model\")\n",
    "\n",
    "evaluation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram_chat_clustering_03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
