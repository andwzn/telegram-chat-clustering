{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # to avoid warning when using a tokenizer to calculate the coherence score during evaluation of a topic model\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from bertopic import BERTopic\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import silhouette_score\n",
    "#from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from bertopic import BERTopic\n",
    "from typing import Optional, Tuple, Dict, Union, List\n",
    "import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# download stopwords and tokenizers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Get the parent directory of the current notebook directory and add it to the python path to import custom modules\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from util.clustering_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Experiment\n",
    "\n",
    "In this notebook, we will cluster the chats using the chat representations created in 02_feature_engineering with BERTopic.\n",
    "\n",
    "To address the randomness inherent in topic modeling, we will run the model multiple times, analyzing the average evaluation metrics and selecting the iteration that is closest to these averages.\n",
    "\n",
    "The overall process will be consistent for each feature, with some adjustments to accommodate their specific characteristics:\n",
    "\n",
    "1. **Load the Chat Representations:** Import the chat representations.\n",
    "\n",
    "2. **Cluster Using BERTopic:** Apply BERTopic to the chat embeddings.\n",
    "\n",
    "3. **Evaluate Performance**:** Analyze the average evaluation metrics across multiple runs.\n",
    "\n",
    "4. **Visualize Clusters:** Create visualizations to represent the clustered topics.\n",
    "\n",
    "5. **Inspect Representative Texts:** Identify the most representative texts for each topic.\n",
    "\n",
    "\n",
    "Finally, we'll compare the results by:\n",
    "\n",
    "1. **Comparing Evaluation Metrics:** Analyze the evaluation metrics across different models to identify the best-performing one and asses the impact of the inclusion of different features on the clustering results.\n",
    "\n",
    "2. **Comparing Topic Assignments:** Evaluate how topic assignments differ from those of the base model to understand the effect different features have on clustering results.\n",
    "\n",
    "3. **Qualitatively asses the topics:** Use the most representative texts and visualisations to qualitativly asses the results of each model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before we start clustering the data, we have to:\n",
    "\n",
    "1. **Load the Dataset:** In  order to inpect message later on, we'll need to load the dataset we used to create the chat representations. \n",
    "\n",
    "2. **Create Chat-Text Aggregations:** Since we will use chat embeddings generated in a previous notebook rather than relying on BERTopic to create its own embeddings, we need to create custom text documents associated with each representation. These will be passed to BERTopic in order to ensure that the topics identified by BERTopic are interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(os.getcwd(), '../data/preprocessed/df_preprocessed.pkl')\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create Chat-Text-Aggregations for Topic-Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the interpretability of the topics generated by BERTopic, we need to provide it with the message texts we used to create features for each chat. \n",
    "\n",
    "To make sure that the \"chat-text-aggregations\" used for this purpose are as meaningful as possible, we will perform the following operations: \n",
    "\n",
    "- **Basic preprocessing**, including lowercasing, stop word removal, removal of punctuation and digits, and tokenization.\n",
    "\n",
    "- **Removing custom stopwords** specific to the Telegram domain, such as:\n",
    "\n",
    "    - Telegram chat handles, which are frequently used to \"sign\" each  message in broadcast chats and could distort the analysis of the most common words.\n",
    "\n",
    "    - Common social media call-to-action phrases, such as \"share,\" \"follow,\" and \"comment,\" which are often repeated irrespective of topic.\n",
    "\n",
    "- **Multilingual processing:** Since our corpus is multilingual, language-dependent preprocessing will be applied only to messages in the most frequent languages, as the other languages contribute only a marginal number of messages.\n",
    "- **TF-IDF filtering:** We will filter out words below a certain TF-IDF threshold to ensure that only distinctive terms are included in the aggregation.\n",
    "\n",
    "Afterwards, we will aggregate the messages and webpage previews for each chat into a single string. This string, along with the chat vector representations created earlier, will be passed to BERTopic as a basis for its topic description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Preprocessing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame, text_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the text data in a specified column of a DataFrame by performing several cleaning operations:\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): the DataFrame containing the text to preprocess.\n",
    "        text_column (str): the name of the column that contains the text to clean.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with additional columns for cleaned (`<text_column>_cleaned`) and tokenized \n",
    "        (`<text_column>_preprocessed`) text.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing messages...\")\n",
    "\n",
    "    # get stop words \n",
    "    stop_words_en = set(stopwords.words('english'))\n",
    "    stop_words_de = set(stopwords.words('german'))\n",
    "\n",
    "    cta_stop_words_en = {'click', 'tap', 'press', 'subscribe', 'follow', 'share', 'like', 'comment',\n",
    "                        'join', 'sign', 'visit', 'download', 'register', 'give', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "    cta_stop_words_de = {'klicken', 'tippen', 'drücken', 'abonnieren', 'folgen', 'teilen', 'mögen', 'kommentieren',\n",
    "                        'beitreten', 'anmelden', 'besuchen', 'herunterladen', 'registrieren', 'geben', 'message', 'chat', 'group', 'channel', 'bot', 'reply'}\n",
    "\n",
    "    stop_words_en = stop_words_en.union(cta_stop_words_en)\n",
    "    stop_words_de = stop_words_de.union(cta_stop_words_de)\n",
    "    print(\"Stop words loaded\")\n",
    "\n",
    "    # get frequent chat handles\n",
    "    frequent_chat_handles = df[\"referenced_chat_handles\"].explode().value_counts()\n",
    "    frequent_chat_handles = frequent_chat_handles[frequent_chat_handles > 100].index.tolist()\n",
    "    print(\"Frequent chat handles loaded\")\n",
    "\n",
    "    # create regex patterns\n",
    "    def create_pattern(words):\n",
    "        return rf'\\b(?:{\"|\".join(map(re.escape, words))})\\b'\n",
    "\n",
    "    frequent_chat_pattern = create_pattern(frequent_chat_handles)\n",
    "    stop_words_en_pattern = create_pattern(stop_words_en)\n",
    "    stop_words_de_pattern = create_pattern(stop_words_de)\n",
    "    print(\"Regex-Patterns created\")\n",
    "    \n",
    "    # remove the most frequent chat handles  #TODO: Seems not to work\n",
    "    df[f\"{text_column}_cleaned\"] = df[text_column].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"Handles removed\")\n",
    "\n",
    "    # remove URLs\n",
    "    pattern = r\"(https?:\\/\\/[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(www\\.[^\\s/$.?#].[^\\s]*[^\\s.,?!)](?![\\])]))|(t\\.me\\/[^\\s.,?!)]*)\"\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(pattern, '', regex=True).str.strip()\n",
    "    print(\"URLs removed\")\n",
    "\n",
    "    # lowercase text\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.lower()\n",
    "    print(\"Lowercase\")\n",
    "    \n",
    "    # remove punctuation\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    print(\"Punctuation removed\")\n",
    "\n",
    "    # remove the most frequent chat handles that included an @\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(frequent_chat_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"Handles with @ removed\")\n",
    "\n",
    "    # remove punctuation again\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(f\"[{re.escape(string.punctuation)}]\", ' ', regex=True).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    print(\"Punctuation removed\")\n",
    "\n",
    "    # remove digits\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].str.replace(r'\\d+', '', regex=True).str.strip()\n",
    "    print(\"Digits removed\")\n",
    "\n",
    "    # remove english stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"English\", f\"{text_column}_cleaned\"].str.replace(stop_words_en_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"English stop words removed\")\n",
    "\n",
    "    # remove german stop words\n",
    "    df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"] = \\\n",
    "        df.loc[df[\"message_text_lang\"] == \"German\", f\"{text_column}_cleaned\"].str.replace(stop_words_de_pattern, '', regex=True, flags=re.IGNORECASE).str.strip()\n",
    "    print(\"German stop words removed\")\n",
    "\n",
    "    # fill NaN with empty string\n",
    "    df[f\"{text_column}_cleaned\"] = df[f\"{text_column}_cleaned\"].fillna('')\n",
    "    \n",
    "    # tokenize text\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_cleaned\"].apply(lambda x: word_tokenize(x) if x else [])\n",
    "\n",
    "    df[f\"{text_column}_preprocessed\"] = df[f\"{text_column}_preprocessed\"].apply(lambda x: ' '.join(x))\n",
    "    print(\"Tokenized\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_msg_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_msgs_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_msg_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"message_text\")\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"message_text_preprocessed\"].to_pickle(preprocessen_msg_path)\n",
    "    print(\"Preprocessed messages saved\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading preprocessed messages...\")\n",
    "    preprocessed_msg = pd.read_pickle(preprocessen_msg_path)\n",
    "    df[\"message_text_preprocessed\"] = preprocessed_msg\n",
    "\n",
    "# display five random samples\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df[[\"message_text\", \"message_text_preprocessed\", \"referenced_chat_handles\"]].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Preprocessing to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check, if the data was already preprocessed\n",
    "preprocessen_web_path = os.path.join(os.getcwd(), '../data/preprocessed/preprocessed_web_viz.pkl')\n",
    "already_preprocessed = os.path.exists(preprocessen_web_path)\n",
    "already_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_preprocessed:\n",
    "    # apply preprocessing\n",
    "    df = preprocess(df, \"webpage_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of emoji-unicodes using data from \"https://unicode.org/Public/emoji/15.1/\"\n",
    "if not already_preprocessed:\n",
    "\n",
    "    # list the paths to the unicode-files\n",
    "    path_1 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-sequences.txt\")\n",
    "    path_2 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-test.txt\")\n",
    "    path_3 = os.path.join(os.getcwd(), \"../data/auxiliary/emoji_unicode/emoji-zwj-sequences.txt\")\n",
    "    file_paths = [path_1, path_2, path_3]\n",
    "\n",
    "    # load all emojis from the unicode-files\n",
    "    emoji_sequences = load_emoji_list(file_paths)\n",
    "\n",
    "    # create a regex pattern from the emoji sequence\n",
    "    emoji_pattern = '|'.join(re.escape(emoji) for emoji in emoji_sequences)\n",
    "    print(\"Emoji pattern created\")\n",
    "\n",
    "    def demojize_chunk(chunk, emoji_pattern):\n",
    "        # remove emojis\n",
    "        chunk[\"webpage_description_preprocessed\"] = chunk[\"webpage_description_preprocessed\"].str.replace(emoji_pattern, \" \", regex=True)\n",
    "        return chunk\n",
    "\n",
    "    n_jobs = 3  # Use three cores (seems to be fastest?)\n",
    "\n",
    "    # remove emojis in parallel for each chunk\n",
    "    chunks = np.array_split(df, n_jobs)\n",
    "    df_chunks = Parallel(n_jobs=n_jobs)(delayed(demojize_chunk)(chunk, emoji_pattern) for chunk in chunks)\n",
    "    df = pd.concat(df_chunks, ignore_index=True)\n",
    "\n",
    "    # save the preprocessed data\n",
    "    df[\"webpage_description_preprocessed\"].to_pickle(preprocessen_web_path)\n",
    "    print(\"Preprocessed messages saved\")    \n",
    "\n",
    "# simply load the preprocessed data, if it was already preprocessed\n",
    "else:\n",
    "    print(\"Loading preprocessed webpage previews...\")\n",
    "    preprocessed_web_previews = pd.read_pickle(preprocessen_web_path)\n",
    "    df[\"webpage_description_preprocessed\"] = preprocessed_web_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Apply TF-IDF-Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Message Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_msgs_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"message_text_lang\"] == \"English\"][\"message_text_preprocessed\"]\n",
    "    german_texts = df[df[\"message_text_lang\"] == \"German\"][\"message_text_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"message_text_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"message_text_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"message_text_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"message_text_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # TODO: Change column name\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"message_text_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"message_text_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"message_text_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply TF-IDF-Filtering to Webpage Previews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = os.path.join(os.getcwd(), '../data/preprocessed/tfidf_web_viz.pkl')\n",
    "already_tfidf = os.path.exists(tfidf_path)\n",
    "already_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_tfidf:\n",
    "    # isolate English and German texts and prepare them for TF-IDF vectorization\n",
    "    english_texts = df[df[\"webpage_description_lang\"] == \"English\"][\"webpage_description_preprocessed\"]\n",
    "    german_texts = df[df[\"webpage_description_lang\"] == \"German\"][\"webpage_description_preprocessed\"]\n",
    "\n",
    "    # create and fit TF-IDF vectorizers based on the isolated texts\n",
    "    tfidf_vectorizer_en = TfidfVectorizer(ngram_range=(1,1))\n",
    "    tfidf_vectorizer_de = TfidfVectorizer(ngram_range=(1,1)) \n",
    "    tfidf_vectorizer_en.fit(english_texts)\n",
    "    tfidf_vectorizer_de.fit(german_texts)\n",
    "\n",
    "    feature_names_en = tfidf_vectorizer_en.get_feature_names_out()\n",
    "    feature_names_de = tfidf_vectorizer_de.get_feature_names_out()\n",
    "\n",
    "    def apply_tf_idf_threshold_web(row, tfidf_vectorizer_en, tfidf_vectorizer_de, threshold):\n",
    "\n",
    "        if row[\"webpage_description_lang\"] == \"English\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_en\n",
    "            feature_names = feature_names_en\n",
    "        elif row[\"webpage_description_lang\"] == \"German\":\n",
    "            tfidf_vectorizer = tfidf_vectorizer_de\n",
    "            feature_names = feature_names_de\n",
    "        else:\n",
    "            return row[\"webpage_description_preprocessed\"]\n",
    "\n",
    "        tfidf_matrix = tfidf_vectorizer.transform([row[\"webpage_description_preprocessed\"]])\n",
    "        tfidf_values = tfidf_matrix.toarray().flatten()\n",
    "        \n",
    "        distinctive_words = [feature_names[i] for i in np.where(tfidf_values > threshold)[0]]\n",
    "\n",
    "        return ' '.join(distinctive_words)\n",
    "\n",
    "    # apply the threshold to the TF-IDF values\n",
    "    df[\"webpage_description_tfidf\"] = df.apply(lambda x: apply_tf_idf_threshold_web(x, tfidf_vectorizer_en, tfidf_vectorizer_de, 0.15), axis=1)\n",
    "\n",
    "    # save the preprocessed messages\n",
    "    df[\"webpage_description_tfidf\"].to_pickle(tfidf_path)\n",
    "\n",
    "else:\n",
    "    print(\"Loading tf-idf-filtered messages...\")\n",
    "    tfidf_filtered_msg = pd.read_pickle(tfidf_path)\n",
    "    df[\"webpage_description_tfidf\"] = tfidf_filtered_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Create Chat-Text-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_texts = chat_texts.astype(str)\n",
    "chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Create Webpage-Preview-Aggregations for each Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"telegram_chat_id\")\n",
    "chat_webpage_previews = grouped[\"webpage_description_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "chat_webpage_previews = chat_webpage_previews.astype(str)\n",
    "chat_webpage_previews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start of, we'll cluster the chats in our dataset using an aggregation of the embeddings of messages sent inside of them. \n",
    "\n",
    "This model will act as a baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(os.getcwd(), '../features/0_base_chat_vectors.npy')\n",
    "base_chat_vectors = np.load(base_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {base_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {base_chat_vectors.iloc[0].shape}\")\n",
    "base_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings. \n",
    "\n",
    "In addition to clustering, BERTopic will also automatically create summaries for each topic using a representational model. The method chosen here, KeyBERT-inspired, extracts keywords from all documents assigned to a topic using BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used to create the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "\n",
    "# check, if the base embeddings were already processed\n",
    "feature_name = \"base\"\n",
    "base_done = is_processed(feature_name)\n",
    "\n",
    "# load the results, if the base embeddings were already processed\n",
    "if base_done:\n",
    "    print(\"Base embeddings already processed. Loading results...\")\n",
    "    base_evaluation_metrics, base_topic_model, representative_messages, representative_webpages = load_data(feature_name)\n",
    "    \n",
    "# run the experiment, if the base embeddings were not already processed\n",
    "else:\n",
    "    print(\"Running experiment for base embeddings...\")\n",
    "    base_evaluation_metrics, base_topics, base_probabilities, base_topic_model = run_experiment(\n",
    "        chat_embeddings=base_chat_vectors, \n",
    "        chat_texts=chat_texts, \n",
    "        n=1, \n",
    "        topic_model_dir_path=os.path.join(os.getcwd(), \"../results/base_embeddings/topic_models/\"),\n",
    "        feature_name=feature_name,\n",
    "        used_embedding_model=transformer_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in base_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and explore the results of the  most average topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(base_topic_model, base_chat_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Inspect the most representative messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**\n",
    "\n",
    "We average the embeddings of all chats assigned to a topic model to find a topics center. This vector will be used as a topic representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not base_done:\n",
    "    topic_vectors = create_topic_vectors(base_topic_model, base_chat_vectors)\n",
    "    topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top n messages closest to the topic vector they were assigned to**\n",
    "\n",
    "First, we add the message-text-embeddings created in `02_feature_engineering` back to the messages in the DataFrame. \n",
    "We will then use them to compare the messages to the topic vectors created earlier to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not base_done:\n",
    "    # load the message embeddings\n",
    "    message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "    message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # create a series where each element is an message-vector\n",
    "    message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "    # check if the message embeddings have the same shape as the dataframe\n",
    "    assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "    message_embeddings_series\n",
    "\n",
    "    # add the message embeddings to the dataframe\n",
    "    df[\"message_vector\"] = message_embeddings_series\n",
    "    df[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not base_done:\n",
    "    representative_messages = get_representative_texts(df = df,\n",
    "                                                    topic_model = base_topic_model,\n",
    "                                                    topic_vectors = topic_vectors,\n",
    "                                                    chat_vectors = base_chat_vectors,\n",
    "                                                    n = 10,\n",
    "                                                    feature_name = \"base\",\n",
    "                                                    text_column = \"message_text\",\n",
    "                                                    text_embeddings_column = \"message_vector\",\n",
    "                                                    text_preprocessed_column = \"message_text_preprocessed\")\n",
    "\n",
    "    # save representative messages\n",
    "    import json\n",
    "    representative_messages_path = os.path.join(os.getcwd(), '../results/base_embeddings/representative_messages.json')\n",
    "    representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "    with open(representative_messages_path, 'w') as jsonfile:\n",
    "        json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtered Chat Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the filtered Chat Embeddings\n",
    "\n",
    "First, we load the chat representations we created in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_path = os.path.join(os.getcwd(), '../features/1_filtered_chat_vectors.npy')\n",
    "filtered_chat_vectors = np.load(filtered_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {filtered_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {filtered_chat_vectors.iloc[0].shape}\")\n",
    "filtered_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create filtered Chat-Message-Text-Aggregations\n",
    "\n",
    "Now we filter the dataset to remove all Forwarded/Original-Message-Pairs using the indices we saved in  `02_feature_engineering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_path = os.path.join(os.getcwd(), \"../features/1_implicit_ref_filtered_indices.npy\")\n",
    "filtered_rows_indices = np.load(indices_path)\n",
    "df_references_filtered = df.loc[filtered_rows_indices]\n",
    "df_references_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we removed some messages from our dataset, we need to create updated chat-text-aggregations that reflect these changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_references_filtered.groupby(\"telegram_chat_id\")\n",
    "filtered_chat_texts = grouped[\"message_text_tfidf\"].agg(lambda x: \" \".join(x))\n",
    "filtered_chat_texts = filtered_chat_texts.astype(str)\n",
    "filtered_chat_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings. Again, we will run the model multiple times and inspect the average results of the topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to reuse it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "\n",
    "# check, if the filtered embeddings were already processed\n",
    "feature_name = \"filtered\"\n",
    "filtered_done = is_processed(feature_name)\n",
    "\n",
    "# load the results, if the filtered embeddings were already processed\n",
    "if filtered_done:\n",
    "    print(\"Filtered embeddings already processed. Loading results...\")\n",
    "    filtered_evaluation_metrics, filtered_topic_model, representative_messages, representative_webpages = load_data(feature_name)\n",
    "\n",
    "# run the experiment, if the filtered embeddings were not already processed\n",
    "else:\n",
    "    print(\"Running experiment for filtered embeddings...\")\n",
    "    filtered_evaluation_metrics, filtered_topics, filtered_probabilities, filtered_topic_model = run_experiment(\n",
    "        chat_embeddings=filtered_chat_vectors, \n",
    "        chat_texts=filtered_chat_texts, \n",
    "        n=1, \n",
    "        topic_model_dir_path=os.path.join(os.getcwd(), \"../results/filtered_embeddings/topic_models/\"),\n",
    "        feature_name=\"filtered\",\n",
    "        used_embedding_model=transformer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in filtered_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(filtered_topic_model, filtered_chat_vectors, filtered_chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Inspect the most representative messages\n",
    "\n",
    "Again, we'll inspect the most representative messages for each topic by comparing them to topic vectors derived from the topic assignments returned by the topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**\n",
    "\n",
    "We average the embeddings of all chats assigned to a topic model to find a topics center. This vector will be used as a topic representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filtered_done:\n",
    "    topic_vectors = create_topic_vectors(filtered_topic_model, filtered_chat_vectors)\n",
    "    topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top n messages closest to the topic vector they were assigned to**\n",
    "\n",
    "Again, we add the embeddings created in `02_feature_engineering` back to the messages in the DataFrame and use them to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filtered_done:\n",
    "    # filter the df to only include the messages that were used in the filtered chat vectors. We can use the indices saved in the feature engineering notebook\n",
    "    filtered_rows_indices = np.load(indices_path)\n",
    "    df_filtered = df.loc[filtered_rows_indices]\n",
    "\n",
    "    # load the message embeddings\n",
    "    message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "    message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # create a series where each element is a message-vector\n",
    "    message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "    # check if the message embeddings have the same shape as the dataframe\n",
    "    assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "    message_embeddings_series\n",
    "\n",
    "    # add the message embeddings to the dataframe\n",
    "    df_filtered[\"message_vector\"] = message_embeddings_series\n",
    "    df_filtered[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not filtered_done:\n",
    "    representative_messages = get_representative_texts(df = df_filtered,\n",
    "                                                    topic_model = filtered_topic_model,\n",
    "                                                    topic_vectors = topic_vectors,\n",
    "                                                    chat_vectors = filtered_chat_vectors,\n",
    "                                                    n = 10,\n",
    "                                                    feature_name = \"filtered\",\n",
    "                                                    text_column = \"message_text\",\n",
    "                                                    text_embeddings_column = \"message_vector\",\n",
    "                                                    text_preprocessed_column = \"message_text_preprocessed\")\n",
    "\n",
    "    # save representative messages\n",
    "    import json\n",
    "    representative_messages_path = os.path.join(os.getcwd(), '../results/filtered_embeddings/representative_messages.json')\n",
    "    representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "    with open(representative_messages_path, 'w') as jsonfile:\n",
    "        json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Webpage Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Chat Representations\n",
    "\n",
    "First, we load the chat representations we created based on webpage previews in the notebook `02_feature_engineering`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpreview_path = os.path.join(os.getcwd(), '../features/3_webpreview_chat_vectors.npy')\n",
    "webpreview_chat_vectors = np.load(webpreview_path, allow_pickle=True)\n",
    "print(f\"Number of chat vectors: {webpreview_chat_vectors.shape[0]}\")\n",
    "print(f\"Vector Dimension: {webpreview_chat_vectors.iloc[0].shape}\")\n",
    "webpreview_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the embeddings\n",
    "\n",
    "Now we can use BERTopic to cluster the embeddings. \n",
    "\n",
    "Again, we'll run the experiment multiple times in order to inspect average results and models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "\n",
    "# check, if the webpreview embeddings were already processed\n",
    "feature_name = \"webpreview\"\n",
    "webpreview_done = is_processed(feature_name, True)\n",
    "\n",
    "# load the results, if the webpreview embeddings were already processed\n",
    "if webpreview_done:\n",
    "    print(\"Webpreview embeddings already processed. Loading results...\")\n",
    "    webpreview_evaluation_metrics, webpreview_topic_model, representative_messages, representative_webpreviews = load_data(feature_name, True)\n",
    "    \n",
    "# run the experiment, if the webpreview embeddings were not already processed\n",
    "else:\n",
    "    print(\"Running experiment for webpreview embeddings...\")\n",
    "    webpreview_evaluation_metrics, webpreview_topics, webpreview_probabilities, webpreview_topic_model = run_experiment(\n",
    "        chat_embeddings=webpreview_chat_vectors, \n",
    "        chat_texts=chat_webpage_previews, \n",
    "        n=1, \n",
    "        topic_model_dir_path=os.path.join(os.getcwd(), \"../results/webpreview_embeddings/topic_models/\"),\n",
    "        feature_name=\"webpreview\",\n",
    "        used_embedding_model=transformer_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in webpreview_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualize and explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(webpreview_topic_model, webpreview_chat_vectors, chat_webpage_previews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Inspect the most representative messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not webpreview_done:\n",
    "    topic_vectors = create_topic_vectors(webpreview_topic_model, webpreview_chat_vectors)\n",
    "    display(topic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top messages closest to the topic vector they were assigned to**\n",
    "\n",
    "Again, we add the embeddings created in `02_feature_engineering` back to the messages in the DataFrame and use them to compare them to the topic vectors created earlier to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the message embeddings\n",
    "if not webpreview_done:\n",
    "    message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "    message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # create a series where each element is an message-vector\n",
    "    message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "    # check if the message embeddings have the same shape as the dataframe\n",
    "    assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "    message_embeddings_series\n",
    "\n",
    "    # add the message embeddings to the dataframe\n",
    "    df[\"message_vector\"] = message_embeddings_series\n",
    "    display(df[[\"message_text\", \"message_vector\"]].head()) #TODO: Add display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the message embeddings with the topic vectors\n",
    "if not webpreview_done:\n",
    "    representative_messages = get_representative_texts(df,\n",
    "                                                    webpreview_topic_model,\n",
    "                                                    topic_vectors,\n",
    "                                                    webpreview_chat_vectors,\n",
    "                                                    10,\n",
    "                                                    \"webpreview\",\n",
    "                                                    \"message_text\",\n",
    "                                                    \"message_vector\",\n",
    "                                                    \"message_text_preprocessed\")\n",
    "\n",
    "    # save representative messages\n",
    "    import json\n",
    "    representative_messages_path = os.path.join(os.getcwd(), '../results/webpreview_embeddings/representative_messages.json')\n",
    "    representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "    with open(representative_messages_path, 'w') as jsonfile:\n",
    "        json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inspect the most representative Webpage-Previews\n",
    "\n",
    "Now, we will inspect the most representative webpage-previews. To do so, we'll compare the webpage-preview-embeddings to the topic-vectors created earlier and inspect the most similar ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webpage embeddings\n",
    "if not webpreview_done:\n",
    "    webpreview_embeddings_path = os.path.join(os.getcwd(), '../features/3_webpage_embeddings.npy')\n",
    "\n",
    "    # create a series where each element is an message-vector\n",
    "    webpreview_embeddings = np.load(webpreview_embeddings_path)\n",
    "\n",
    "    # add the message embeddings to the dataframe.\n",
    "    df[\"webpreview_vector\"] = webpreview_embeddings.tolist() \n",
    "    display(df[[\"webpage_title\", \"webpage_description\", \"webpreview_vector\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the webpage embeddings with the topic vectors and get the most similar webpages\n",
    "if not webpreview_done:\n",
    "    representative_webpreviews = get_representative_texts(df, \n",
    "                                                        webpreview_topic_model, \n",
    "                                                        topic_vectors, \n",
    "                                                        webpreview_chat_vectors, \n",
    "                                                        10, \n",
    "                                                        \"webpreview\", \n",
    "                                                        \"webpage_description\",\n",
    "                                                        \"webpreview_vector\",\n",
    "                                                        \"webpage_description_preprocessed\")\n",
    "\n",
    "    # save representative messages\n",
    "    import json\n",
    "    representative_webpreview_path = os.path.join(os.getcwd(), '../results/webpreview_embeddings/representative_webpreviews.json')\n",
    "    representative_webpreviews = {int(topic): messages for topic, messages in representative_webpreviews.items()} # convert keys to int\n",
    "    with open(representative_webpreview_path, 'w') as jsonfile:\n",
    "        json.dump(representative_webpreviews, jsonfile, indent=4)\n",
    "\n",
    "# print representative webpreviews\n",
    "for topic, text in representative_webpreviews.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, text in enumerate(text):\n",
    "        print(f\"{i+1}. {text.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined Message & Webpage-Preview Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the message- and web-preview-embeddings and inspect the changes in clustering results.\n",
    "\n",
    "#### 1. Combine Message-Text- and Webpage-Preview-Vectors\n",
    "\n",
    "First, we load the chat-vectors we created by averaging the webpage-preview- and message-vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_vectors_path = os.path.join(os.getcwd(), '../features/3_msg_webpreview_chat_vectors.npy')\n",
    "combined_vectors = np.load(combine_vectors_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create combined Chat-Text-Aggregations\n",
    "\n",
    "Next, we combine the Text-Aggregations for Webpage-Previews and Chat-Messages in order to use them  to make the topics interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame to combine the texts\n",
    "combined_text_dataframe = pd.DataFrame({\n",
    "    \"chat_texts\": chat_texts,\n",
    "    \"chat_webpage_previews\": chat_webpage_previews\n",
    "})\n",
    "\n",
    "# combine the texts\n",
    "combined_text_dataframe[\"combined_texts\"] = combined_text_dataframe[\"chat_texts\"] + \" \" + combined_text_dataframe[\"chat_webpage_previews\"]\n",
    "\n",
    "# calculate the length of the texts\n",
    "combined_text_dataframe[\"chat_texts_len\"] = combined_text_dataframe[\"chat_texts\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"chat_webpage_previews_len\"] = combined_text_dataframe[\"chat_webpage_previews\"].apply(lambda x: len(x.split()))\n",
    "combined_text_dataframe[\"combined_texts_len\"] = combined_text_dataframe[\"combined_texts\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "# check if the combined arrays are the same length as the original arrays combined\n",
    "assert combined_text_dataframe[\"combined_texts_len\"].equals(combined_text_dataframe[\"chat_texts_len\"] + combined_text_dataframe[\"chat_webpage_previews_len\"])\n",
    "\n",
    "# get the combined texts\n",
    "combined_texts = combined_text_dataframe[\"combined_texts\"]\n",
    "combined_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the Combined Embeddings\n",
    "\n",
    "Now, we can cluster the chats using the combined message- and webpage-preview embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "\n",
    "# check, if the webpreview embeddings were already processed\n",
    "feature_name = \"combined_webpreview\"\n",
    "combined_webpreview_done = is_processed(feature_name, True)\n",
    "\n",
    "# load the results, if the webpreview embeddings were already processed\n",
    "if combined_webpreview_done:\n",
    "    print(\"Combined webpreview embeddings already processed. Loading results...\")\n",
    "    combined_webpreview_evaluation_metrics, combined_webpreview_topic_model, representative_messages, representative_webpreviews = load_data(feature_name, True)\n",
    "    \n",
    "# run the experiment, if the webpreview embeddings were not already processed\n",
    "else:\n",
    "    print(\"Running experiment for combined webpreview embeddings...\")\n",
    "\n",
    "    combined_webpreview_evaluation_metrics, combined_webpreview_topics, combined_webpreview_probabilities, combined_webpreview_topic_model = run_experiment(\n",
    "        chat_embeddings=combined_vectors, \n",
    "        chat_texts=combined_texts, \n",
    "        n=1, \n",
    "        topic_model_dir_path=os.path.join(os.getcwd(), \"../results/combined_webpreview_embeddings/topic_models/\"),\n",
    "        feature_name=\"combined_webpreview\",\n",
    "        used_embedding_model=transformer_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in combined_webpreview_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Visualise and explore the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(combined_webpreview_topic_model, combined_vectors, combined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inspect the most representative messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not combined_webpreview_done:\n",
    "    topic_vectors = create_topic_vectors(combined_webpreview_topic_model, combined_vectors)\n",
    "    display(topic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. For each topic, extract the top n messages closest to the topic vector they were assigned to**\n",
    "\n",
    "Like with the features used before, we add the embeddings created in `02_feature_engineering` back to the messages in the DataFrame and use them to compare them to the topic vectors created earlier to find the most representative messages for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the message embeddings\n",
    "if not combined_webpreview_done:\n",
    "    message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "    message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "    # create a series where each element is an message-vector\n",
    "    message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "    # check if the message embeddings have the same shape as the dataframe\n",
    "    assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "    message_embeddings_series\n",
    "\n",
    "    # add the message embeddings to the dataframe\n",
    "    df[\"message_vector\"] = message_embeddings_series\n",
    "    display(df[[\"message_text\", \"message_vector\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the messages closest to the topic centers\n",
    "if not combined_webpreview_done:\n",
    "    representative_messages = get_representative_texts(df = df,\n",
    "                                                    topic_model = combined_webpreview_topic_model,\n",
    "                                                    topic_vectors = topic_vectors,\n",
    "                                                    chat_vectors = combined_vectors,\n",
    "                                                    n = 10,\n",
    "                                                    feature_name = \"combined_webpreview\",\n",
    "                                                    text_column = \"message_text\",\n",
    "                                                    text_embeddings_column = \"message_vector\",\n",
    "                                                    text_preprocessed_column = \"message_text_preprocessed\")\n",
    "\n",
    "    # save representative messages\n",
    "    import json\n",
    "    representative_messages_path = os.path.join(os.getcwd(), '../results/combined_webpreview_embeddings/representative_messages.json')\n",
    "    representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "    with open(representative_messages_path, 'w') as jsonfile:\n",
    "        json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Inspect the most representative Webpage-Previews\n",
    "\n",
    "Now, we will inspect the most representative webpage-previews using the same approach we have already used to find representative messages. We will reuse the topic vectors created while inspecting the most representative messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the webpage-preview embeddings\n",
    "if not combined_webpreview_done:\n",
    "    webpreview_embeddings_path = os.path.join(os.getcwd(), '../features/3_webpage_embeddings.npy')\n",
    "\n",
    "    # create a series where each element is an message-vector\n",
    "    webpreview_embeddings = np.load(webpreview_embeddings_path)\n",
    "\n",
    "    # add the message embeddings to the dataframe.\n",
    "    df[\"webpreview_vector\"] = webpreview_embeddings.tolist() \n",
    "    df[[\"webpage_title\", \"webpage_description\", \"webpreview_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the webpage previews closest to the topic centers\n",
    "if not combined_webpreview_done:\n",
    "    representative_webpreviews = get_representative_texts(df, \n",
    "                                                        combined_webpreview_topic_model, \n",
    "                                                        topic_vectors, \n",
    "                                                        combined_vectors, \n",
    "                                                        10, \n",
    "                                                        \"combined_webpreview\", \n",
    "                                                        \"webpage_description\",\n",
    "                                                        \"webpreview_vector\",\n",
    "                                                        \"webpage_description_preprocessed\")\n",
    "\n",
    "    # save representative webpreviews\n",
    "    import json\n",
    "    representative_webpreview_path = os.path.join(os.getcwd(), '../results/combined_webpreview_embeddings/representative_webpreviews.json')\n",
    "    representative_webpreviews = {int(topic): messages for topic, messages in representative_webpreviews.items()} # convert keys to int\n",
    "    with open(representative_webpreview_path, 'w') as jsonfile:\n",
    "        json.dump(representative_webpreviews, jsonfile, indent=4)\n",
    "\n",
    "# print representative webpreviews\n",
    "for topic, text in representative_webpreviews.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, text in enumerate(text):\n",
    "        print(f\"{i+1}. {text.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Structural Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we conduct chat-clustering using a chats structural attributes. Structural attributes are a chats connections to other telegram entities. \n",
    "\n",
    "For our purposes, we have considered two kinds of connections:\n",
    "\n",
    "1. Forwarded (fwd) messages between chats.\n",
    "\n",
    "2. Textual references (ref) to chats or other telegram-entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Chat-Vectors\n",
    "\n",
    "To vectorize these connections, we created chat-chat-matrices based on forwards and text based references between chats in `02_feature_engineering`, which we'll load now.\n",
    "\n",
    "As we used both one-hot-encoding and log scaling to normalize the matrices, we'll compare results for both approaches and continue with the one archiving better scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "fwd_log_path = os.path.join(os.getcwd(), '../features/2_log_fwd_vectors.pkl')\n",
    "fwd_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_fwd_vectors.pkl')\n",
    "ref_log_path = os.path.join(os.getcwd(), '../features/2_log_ref_vectors.pkl')\n",
    "ref_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_ref_vectors.pkl')\n",
    "\n",
    "# load the chat vectors\n",
    "fwd_log_vectors = pd.read_pickle(fwd_log_path)\n",
    "fwd_onehot_vectors = pd.read_pickle(fwd_onehot_path)\n",
    "ref_log_vectors = pd.read_pickle(ref_log_path)\n",
    "ref_onehot_vectors = pd.read_pickle(ref_onehot_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the forward-based and the reference-based chat-vectors to create our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the vectors\n",
    "structure_log_vectors = fwd_log_vectors.combine(ref_log_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "structure_onehot_vectors = fwd_onehot_vectors.combine(ref_onehot_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the original vectors\n",
    "dimension_fwd_vectors = len(fwd_log_vectors.iloc[1])\n",
    "dimension_ref_vectors = len(ref_log_vectors.iloc[1])\n",
    "assert len(structure_log_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors\n",
    "assert len(structure_onehot_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster the Structural Vectors\n",
    "\n",
    "**Get average models and evaluation results**\n",
    "\n",
    "Eventhough they are no traditional text-embeddings we will pass the structural vectors to BERTopic for clustering. This is possible, as BERTopic can accept any kind of custom numerical vector representation of a document instead of generating them from text.\n",
    "\n",
    "We will then use the message-text-aggregations created earlier to make the topics found by BERTopic interpretable. These documents will only be used for topic labeling and interpretation. The clustering itself will be entirely driven by the chat-chat-matrices we pass as embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "    \n",
    "# set the parameters for the HDBSCAN model\n",
    "# alpha = 1.0\n",
    "# min_cluster_size = 7\n",
    "# min_samples = 5\n",
    "# hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "#                         min_samples=min_samples,\n",
    "#                         alpha=alpha,\n",
    "#                         prediction_data=True)\n",
    "\n",
    "# run the experiment for the structural vectors normalized using the log transformation\n",
    "log_structural_evaluation_metrics, log_structural_topics, log_structural_probabilities, log_structural_topic_model = run_experiment(\n",
    "    chat_embeddings=structure_log_vectors, \n",
    "    chat_texts=chat_texts, \n",
    "    n=1, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/log_structural_embeddings/topic_models/\"),\n",
    "    feature_name=\"log_structural\",\n",
    "    used_embedding_model=transformer_model\n",
    ")\n",
    "\n",
    "# run the experiment for the structural vectors normalized using one hot encoding\n",
    "onehot_structural_evaluation_metrics, onehot_structural_topics, onehot_structural_probabilities, onehot_structural_topic_model = run_experiment(\n",
    "    chat_embeddings=structure_onehot_vectors, \n",
    "    chat_texts=chat_texts, \n",
    "    n=1, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/onehot_structural_embeddings/topic_models/\"),\n",
    "    feature_name=\"onehot_structural\",\n",
    "    used_embedding_model=transformer_model\n",
    ")\n",
    "\n",
    "# compare the evaluation metrics\n",
    "print(\"\\n#### Log Structural Evaluation Metrics: ####\")\n",
    "for key, value in log_structural_evaluation_metrics.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "print(\"\\n#### Onehot Structural Evaluation Metrics: ####\")\n",
    "for key, value in onehot_structural_evaluation_metrics.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Models:**\n",
    "\n",
    "Next, we compare the average evaluation results of the models created using the two normalization techniques. To determine which model has more favorable evaluation metrics, we will apply the following heuristics:\n",
    "\n",
    "- **Coherence Score**: Lower values indicate better topic coherence.\n",
    "- **Silhouette Score**: Higher values suggest better-defined and more distinct clusters.\n",
    "- **Davies-Bouldin Score**: Lower values reflect more compact and well-separated clusters.\n",
    "- **Topic Count**: This metric will not be considered, as the number of topics alone does not reliably indicate model quality.\n",
    "- **Noise Count**: Lower values are preferable, as they imply fewer data points are classified as noise.\n",
    "\n",
    "Based on these criteria, we will select the model with the more desirable evaluation results for further analysis and inspection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Remove\n",
    "log_structural_evaluation_metrics[\"avg_coherence_scores\"] = 0\n",
    "log_structural_evaluation_metrics[\"avg_silhouette_scores\"] = 0\n",
    "log_structural_evaluation_metrics[\"avg_davies_bouldin_scores\"] = 0\n",
    "log_structural_evaluation_metrics[\"average_noise_counts\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structural_topics, structural_propabilities, structural_topic_model, structural_evaluation_metrics, structural_vectors  = compare_averages(\n",
    "    metrics_model_1 = log_structural_evaluation_metrics,\n",
    "    topics_model_1 = log_structural_topics,\n",
    "    propabilities_model_1 = log_structural_probabilities,\n",
    "    model_1 = log_structural_topic_model,\n",
    "    vectors_1 = structure_log_vectors,\n",
    "    metrics_model_2 = onehot_structural_evaluation_metrics,\n",
    "    topics_model_2 = onehot_structural_topics, \n",
    "    propabilities_model_2 = onehot_structural_probabilities,\n",
    "    model_2 = onehot_structural_topic_model,\n",
    "    vectors_2 = structure_onehot_vectors\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in structural_evaluation_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualise and Explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(structural_topic_model, structural_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Inspect the most representative messages\n",
    "\n",
    "In this approach, chat vectors were generated using a chat-to-chat matrix, rather than by averaging message vectors. As a result, we cannot directly compare message vectors to the topic center to identify the most representative messages.\n",
    "\n",
    "However, we can still work with the chat vectors. \n",
    "Accordingly, we'll use the following appraoch to find the most representative messages:\n",
    "\n",
    "1. Create topic vectors by taking the mean of the chat vectors of each topic.\n",
    "2. Identify the chat whose vector is closest to the topic center.\n",
    "3. Compute the mean of all message embeddings within this chat. (We can reuse the base chat representations created in `02_feature_engineering`, as they are simply averages of all message vectors of a chat.)\n",
    "4. Retrieve the messages from this chat that are closest to this mean, as they are likely to be the most representative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = create_topic_vectors(structural_topic_model, structural_vectors)\n",
    "topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Find the most representative chat for each topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame containing the structural vectors, the topics assigned by the topic models and the associated topic vectors\n",
    "vector_assignment_df = pd.DataFrame(structural_vectors, columns=[\"structural_chat_vector\"])\n",
    "vector_assignment_df[\"topic_assignment\"] = structural_topic_model.topics_\n",
    "vector_assignment_df[\"topic_vectors\"] = vector_assignment_df[\"topic_assignment\"].map(topic_vectors)\n",
    "\n",
    "# drop the rows where the topic assignment is -1 (i.e. the \"Other\" topic)\n",
    "vector_assignment_df = vector_assignment_df[vector_assignment_df[\"topic_assignment\"] != -1]\n",
    "\n",
    "def cosine_similarity_row(row):\n",
    "    \n",
    "    # reshape the vectors to be 2D arrays (input format of cosine_similarity function)\n",
    "    chat_vector = np.array(row['structural_chat_vector']).reshape(1, -1)\n",
    "    topic_vector = np.array(row['topic_vectors']).reshape(1, -1)\n",
    "    \n",
    "    similarity = cosine_similarity(chat_vector, topic_vector)\n",
    "    \n",
    "    return similarity[0][0]\n",
    "\n",
    "# calculate the cosine similarity between the chat vectors and their topic vectors\n",
    "vector_assignment_df[\"similarity\"] = vector_assignment_df.apply(lambda row: cosine_similarity_row(row), axis=1)\n",
    "\n",
    "# get the chat vectors and the chat index of the chat vector most similar to its topic vector for each topic\n",
    "most_similar_index = vector_assignment_df.groupby(\"topic_assignment\")[\"similarity\"].idxmax()\n",
    "most_similar_chat = vector_assignment_df.loc[most_similar_index]\n",
    "\n",
    "most_similar_chats = (\n",
    "    most_similar_chat\n",
    "    .reset_index()  \n",
    "    .rename(columns={\"index\": \"chat_id\"}) \n",
    "    .set_index(\"topic_assignment\") \n",
    ")[[\"chat_id\",\"structural_chat_vector\"]]\n",
    "\n",
    "most_similar_chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the chat embeddings created using averaged message embeddings to the dataframe\n",
    "most_similar_chats[\"base_chat_vectors\"] = most_similar_chats[\"chat_id\"].map(base_chat_vectors)\n",
    "\n",
    "# get the average chat vector for each topic\n",
    "avg_chat_vectors = most_similar_chats[\"base_chat_vectors\"]\n",
    "avg_chat_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. For each topic, extract the most representative messages of the chat closest to the topic vector**\n",
    "\n",
    "As in previous steps, we'll add the message text embeddings from `02_feature_engineering` back to the DataFrame of messages.\n",
    "\n",
    "For each topic, we'll then extract the most representative messages (i.e. the ones closest to the mean of all message embeddings of this chat) from the chat closest to the topic center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the message embeddings\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# create a series where each element is an message-vector\n",
    "message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "# check if the message embeddings have the same shape as the dataframe\n",
    "assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "message_embeddings_series\n",
    "\n",
    "# add the message embeddings to the dataframe\n",
    "df[\"message_vector\"] = message_embeddings_series\n",
    "df[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_messages = get_representative_texts(\n",
    "    df = df,\n",
    "    topic_model = structural_topic_model,\n",
    "    topic_vectors = avg_chat_vectors, \n",
    "    chat_vectors = base_chat_vectors, # comparison between the \"average chat vector\" of a topic and the base chat vectors will always return the chat closest to the topic vector\n",
    "    n = 10,\n",
    "    feature_name = \"structural\",\n",
    "    text_column = \"message_text\",\n",
    "    text_embeddings_column = \"message_vector\",\n",
    "    text_preprocessed_column = \"message_text_preprocessed\"\n",
    "    )\n",
    "\n",
    "# save representative messages\n",
    "import json\n",
    "representative_messages_dir = os.path.join(os.getcwd(), '../results/structural_embeddings/')\n",
    "representative_messages_path = os.path.join(os.getcwd(), '../results/structural_embeddings/representative_messages.json')\n",
    "os.makedirs(representative_messages_dir, exist_ok=True)\n",
    "representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "with open(representative_messages_path, 'w') as jsonfile:\n",
    "    json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combined Message Embeddings & Structural Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will cluster the chats based on a combination of their structural attributes and their averaged message embeddings. We will use the same approach as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the Chat-Vectors\n",
    "\n",
    "First, we load the structural vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "fwd_log_path = os.path.join(os.getcwd(), '../features/2_log_fwd_vectors.pkl')\n",
    "fwd_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_fwd_vectors.pkl')\n",
    "ref_log_path = os.path.join(os.getcwd(), '../features/2_log_ref_vectors.pkl')\n",
    "ref_onehot_path = os.path.join(os.getcwd(), '../features/2_onehot_ref_vectors.pkl')\n",
    "\n",
    "# load the chat vectors\n",
    "fwd_log_vectors = pd.read_pickle(fwd_log_path)\n",
    "fwd_onehot_vectors = pd.read_pickle(fwd_onehot_path)\n",
    "ref_log_vectors = pd.read_pickle(ref_log_path)\n",
    "ref_onehot_vectors = pd.read_pickle(ref_onehot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the forward-based and the reference-based chat-vectors to create a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the vectors normalized using the log transformation\n",
    "structure_log_vectors = fwd_log_vectors.combine(ref_log_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "\n",
    "# combine the vectors normalized using one hot encoding\n",
    "structure_onehot_vectors = fwd_onehot_vectors.combine(ref_onehot_vectors, lambda x, y: np.concatenate([x,y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the original vectors\n",
    "dimension_fwd_vectors = len(fwd_log_vectors.iloc[1])\n",
    "dimension_ref_vectors = len(ref_log_vectors.iloc[1])\n",
    "assert len(structure_log_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors\n",
    "assert len(structure_onehot_vectors.iloc[1]) == dimension_fwd_vectors + dimension_ref_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Concenate the Features\n",
    "\n",
    "To supplement the text embeddings with structural information, we'll concatenate the new feature vectors and the chat embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the message embedding based chat vectors with the log-scaled structure-based chat vectors\n",
    "combined_vectors_log = base_chat_vectors.combine(structure_log_vectors, lambda x, y: np.concatenate([x, y]))\n",
    "\n",
    "# combine the message embedding based chat vectors with the one-hot encoded structure-based chat vectors\n",
    "combined_vectors_onehot = base_chat_vectors.combine(structure_onehot_vectors, lambda x, y: np.concatenate([x, y]))\n",
    "\n",
    "# check if the combined vectors have the expected length of a sum of the message text embeddings and the structure-based vectors\n",
    "assert len(combined_vectors_log.iloc[1]) == len(base_chat_vectors.iloc[1]) + len(structure_log_vectors.iloc[1])\n",
    "assert len(combined_vectors_onehot.iloc[1]) == len(base_chat_vectors.iloc[1]) + len(structure_onehot_vectors.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster the Combined Chat Vectors\n",
    "\n",
    "Now, we can cluster the resulting combined vectors using BERTopic. \n",
    "\n",
    "We'll cluster the chats multiple times, using each normalization method and continue with the model that yields the more favorable evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model we used for the embeddings, in order to use it for the representational model\n",
    "current_path = os.getcwd()\n",
    "model_dir = os.path.join(current_path, \"../data/models/\")\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    print(\"Model not found. Downloading...\")\n",
    "    transformer_model = SentenceTransformer(model_name)\n",
    "    transformer_model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "else:\n",
    "    print(f\"Model already downloaded. Loading...\")\n",
    "    transformer_model = SentenceTransformer(model_path)\n",
    "    \n",
    "# set the parameters for the HDBSCAN model\n",
    "alpha = 1.0\n",
    "min_cluster_size = 7\n",
    "min_samples = 5\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        alpha=alpha,\n",
    "                        prediction_data=True)\n",
    "\n",
    "# run the experiment for the combined vectors normalized containing structural vectors created using the log transformation\n",
    "log_combined_structural_evaluation_metrics, log_combined_structural_topics, log_combined_structural_probabilities, log_combined_structural_topic_model = run_experiment(\n",
    "    chat_embeddings=combined_vectors_log, \n",
    "    chat_texts=chat_texts, \n",
    "    n=1, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/log_combined_structural_embeddings/topic_models/\"),\n",
    "    feature_name=\"log_combined_structural\",\n",
    "    used_embedding_model=transformer_model\n",
    ")\n",
    "\n",
    "# run the experiment for the combined vectors containing structural vectors created using one hot encoding\n",
    "onehot_combined_structural_evaluation_metrics, onehot_combined_structural_topics, onehot_combined_structural_probabilities, onehot_combined_structural_topic_model = run_experiment(\n",
    "    chat_embeddings=structure_onehot_vectors, \n",
    "    chat_texts=chat_texts, \n",
    "    n=1, \n",
    "    topic_model_dir_path=os.path.join(os.getcwd(), \"../results/onehot_combined_structural_embeddings/topic_models/\"),\n",
    "    feature_name=\"onehot_combined_structural\",\n",
    "    used_embedding_model=transformer_model\n",
    ")\n",
    "\n",
    "# compare the evaluation metrics\n",
    "print(\"\\n#### Log Combined Structural Evaluation Metrics: ####\")\n",
    "for key, value in log_combined_structural_evaluation_metrics.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "    \n",
    "print(\"\\n#### Onehot Combined Structural Evaluation Metrics: ####\")\n",
    "for key, value in onehot_combined_structural_evaluation_metrics.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Models:**\n",
    "\n",
    "Like we did with the models created using only structural vectors, we'll compare the average evaluation results of the models created using the two normalization techniques. To determine which model has more favorable evaluation metrics, we will apply the heuristics outlined earlier in this notebook and continue to work with the more favourable model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_structural_topics, combined_structural_propabilities, combined_structural_topic_model, combined_structural_metrics, combined_structural_vectors  = compare_averages(\n",
    "    metrics_model_1 = log_combined_structural_evaluation_metrics,\n",
    "    topics_model_1 = log_combined_structural_topics,\n",
    "    propabilities_model_1 = log_combined_structural_probabilities,\n",
    "    model_1 = log_combined_structural_topic_model,\n",
    "    vectors_1 = combined_vectors_log,\n",
    "    metrics_model_2 = onehot_combined_structural_evaluation_metrics,\n",
    "    topics_model_2 = onehot_combined_structural_topics, \n",
    "    propabilities_model_2 = onehot_combined_structural_probabilities,\n",
    "    model_2 = onehot_combined_structural_topic_model,\n",
    "    vectors_2 = combined_vectors_onehot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Inspect the average evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in combined_structural_metrics.items():\n",
    "    print(f\"{key.replace(\"_\", \" \")[4:].title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Visualise and explore the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_topic_visualisations(combined_structural_topic_model, combined_structural_vectors, chat_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inspect the most representative messages\n",
    "\n",
    "Since the chat vectors were generated by combining chat-cooccurrence vectors and message text embeddings, we can't directly compare individual message vectors to the topic center in order to find the most representative messages.\n",
    "\n",
    "To address this, we'll add its chat's cooccurrence vector to each message vectors. This will allow us to compare the resulting message vectors to the topic centers, which we'll create by averaging all chat vectors assigned to a given topic.\n",
    "\n",
    "\n",
    "**1. Create Topic Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = create_topic_vectors(combined_structural_topic_model, combined_structural_vectors)\n",
    "topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Add the structural vector of each chat to its messages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Avoid repeating code\n",
    "\n",
    "# load the message embeddings\n",
    "message_embeddings_path = os.path.join(os.getcwd(), '../features/0_message_embeddings.npy')\n",
    "message_embeddings = np.load(message_embeddings_path, allow_pickle=True)\n",
    "\n",
    "# create a series where each element is an message-vector\n",
    "message_embeddings_series = pd.Series([embedding for embedding in message_embeddings])\n",
    "\n",
    "# check if the message embeddings have the same shape as the dataframe\n",
    "assert message_embeddings_series.shape[0] == len(message_embeddings)\n",
    "message_embeddings_series\n",
    "\n",
    "# add the message embeddings to the dataframe\n",
    "df[\"message_vector\"] = message_embeddings_series\n",
    "df[[\"message_text\", \"message_vector\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate the structural part of the combined vectors to be appended to the message embeddings\n",
    "structural_part = combined_structural_vectors.apply(lambda x: x[384:])\n",
    "\n",
    "# get the representative messages\n",
    "representative_messages =  get_representative_texts(df = df,\n",
    "                                                    topic_model = combined_structural_topic_model,\n",
    "                                                    topic_vectors = topic_vectors, \n",
    "                                                    chat_vectors = combined_structural_vectors, \n",
    "                                                    n = 10,\n",
    "                                                    feature_name = \"combined_structural\",\n",
    "                                                    text_column = \"message_text\",\n",
    "                                                    text_embeddings_column = \"message_vector\",\n",
    "                                                    text_preprocessed_column = \"message_text_preprocessed\",\n",
    "                                                    add_structural_info = True,\n",
    "                                                    structural_embedding_chat_map = structural_part)\n",
    "\n",
    "# save representative messages\n",
    "import json\n",
    "representative_messages_dir = os.path.join(os.getcwd(), '../results/combined_structural_embeddings/')\n",
    "representative_messages_path = os.path.join(os.getcwd(), '../results/combined_structural_embeddings/representative_messages.json')\n",
    "os.makedirs(representative_messages_dir, exist_ok=True)\n",
    "representative_messages = {int(topic): messages for topic, messages in representative_messages.items()} # convert keys to int\n",
    "with open(representative_messages_path, 'w') as jsonfile:\n",
    "    json.dump(representative_messages, jsonfile, indent=4)\n",
    "\n",
    "# print representative messages\n",
    "for topic, messages in representative_messages.items():\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"{i+1}. {message.strip()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the different approaches based on the evaluation data we collected for each feature and feature combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame({\n",
    "    \"Features\": [\"Msg\", \"Filtered\", \"Webpreview\", \"Msg + Webpreview\", \"Structural\", \"Msg + Structural\"],\n",
    "    \"Silhouette Score\": [ss_base_embeddings, ss_filtered_embeddings, ss_webpreview_embeddings, ss_msg_webpreview_embeddings, ss_structural_embeddings, ss_msg_structural_embeddings],\n",
    "    \"Topic Count\": [topic_count_base_embeddings, topic_count_filtered_embeddings, topic_count_webpreview_embeddings, topic_count_msg_webpreview_embeddings, topic_count_structural_embeddings, topic_count_msg_structural_embeddings], \n",
    "    \"Noise Instances\": [noise_base_embeddings, noise_filtered_embeddings, noise_webpreview_embeddings, noise_count_msg_webpreview_embeddings, noise_structural_embeddings, noise_structural_embeddings]\n",
    "}).set_index(\"Features\")\n",
    "evaluation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "telegram_chat_clustering_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
